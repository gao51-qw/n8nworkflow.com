{
  "id": 4043,
  "slug": "4043",
  "title": "Adaptive RAG with Google Gemini & Qdrant: context-aware query answering",
  "description": "**Description**\nThis workflow automatically classifies user queries and retrieves the most relevant information based on the query type. üåü It uses adaptive strategies like;\n   Factual, Analytical, Opinion, and Contextual to deliver more precise and meaningful responses by leveraging n8n's flexibility. Integrated with Qdrant vector store and Google Gemini, it processes each query faster and more effectively. üöÄ\n\n**How It Works?**\nQuery Reception: A user query is triggered (e.g., through a chatbot interface). üí¨\n\n*Classification*: The query is classified into one of four categories:\n\n*Factual*: Queries seeking verifiable information.\n\n*Analytical*: Queries that require in-depth analysis or explanation.\n\n*Opinion*: Queries looking for different perspectives or subjective viewpoints.\n\n*Contextual*: Queries specific to the user or certain contextual conditions.\n\n*Adaptive Strategy Application*: Based on classification, the query is restructured using the relevant strategy for better results.\n\nResponse Generation**: The most relevant documents and context are used to generate a tailored response. üéØ\n\n**Set Up Steps**\n\nEstimated Time: ‚è≥ 10-15 minutes\nPrerequisites: You need an n8n account and a Qdrant vector store connection.\nSteps:\n\nImport the n8n workflow: Load the workflow into your n8n instance.\n\nConnect Google Gemini and Qdrant: Link these tools for query processing and data retrieval.\n\nConnect the Trigger Interface: Integrate with a chatbot or API to trigger the workflow.\n\nCustomize: Adjust settings based on the query types you want to handle and the output format. üîß\n\n**For more detailed instructions, please check the sticky notes inside the workflow. üìå**",
  "featuredImage": "/data/workflows/4043/4043.webp",
  "author": {
    "id": 101,
    "slug": "nisacayir",
    "name": "Nisa",
    "avatar": ""
  },
  "categories": [
    "Internal Wiki",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 4388,
  "downloads": 438,
  "createdAt": "2025-05-14T15:10:47.350Z",
  "updatedAt": "2026-01-16T08:32:44.593Z",
  "publishedAt": "2025-05-14T15:10:47.350Z",
  "nodes": 40,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/4043",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Adaptive RAG with Google Gemini & Qdrant: context-aware query answering",
    "workflowName": "Adaptive RAG with Google Gemini & Qdrant: context-aware query answering",
    "description": "**Description**\nThis workflow automatically classifies user queries and retrieves the most relevant information based on the query type. üåü It uses adaptive strategies like;\n   Factual, Analytical, Opinion, and Contextual to deliver more precise and meaningful responses by leveraging n8n's flexibility. Integrated with Qdrant vector store and Google Gemini, it processes each query faster and more effectively. üöÄ\n\n**How It Works?**\nQuery Reception: A user query is triggered (e.g., through a chatbot interface). üí¨\n\n*Classification*: The query is classified into one of four categories:\n\n*Factual*: Queries seeking verifiable information.\n\n*Analytical*: Queries that require in-depth analysis or explanation.\n\n*Opinion*: Queries looking for different perspectives or subjective viewpoints.\n\n*Contextual*: Queries specific to the user or certain contextual conditions.\n\n*Adaptive Strategy Application*: Based on classification, the query is restructured using the relevant strategy for better results.\n\nResponse Generation**: The most relevant documents and context are used to generate a tailored response. üéØ\n\n**Set Up Steps**\n\nEstimated Time: ‚è≥ 10-15 minutes\nPrerequisites: You need an n8n account and a Qdrant vector store connection.\nSteps:\n\nImport the n8n workflow: Load the workflow into your n8n instance.\n\nConnect Google Gemini and Qdrant: Link these tools for query processing and data retrieval.\n\nConnect the Trigger Interface: Integrate with a chatbot or API to trigger the workflow.\n\nCustomize: Adjust settings based on the query types you want to handle and the output format. üîß\n\n**For more detailed instructions, please check the sticky notes inside the workflow. üìå**",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Query Classification",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.8"
    },
    {
      "name": "Switch",
      "type": "n8n-nodes-base.switch",
      "role": "switch",
      "configDescription": "Version 3.2"
    },
    {
      "name": "Factual Strategy - Focus on Precision",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.7"
    },
    {
      "name": "Analytical Strategy - Comprehensive Coverage",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.7"
    },
    {
      "name": "Opinion Strategy - Diverse Perspectives",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.7"
    },
    {
      "name": "Contextual Strategy - User Context Integration",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.7"
    },
    {
      "name": "Chat",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Factual Prompt and Output",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Contextual Prompt and Output",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Opinion Prompt and Output",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Analytical Prompt and Output",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Gemini Classification",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Gemini Factual",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Gemini Analytical",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Chat Buffer Memory Analytical",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Chat Buffer Memory Factual",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Gemini Opinion",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Chat Buffer Memory Opinion",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Gemini Contextual",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Chat Buffer Memory Contextual",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Embeddings",
      "type": "@n8n/n8n-nodes-langchain.embeddingsGoogleGemini",
      "role": "embeddingsGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Concatenate Context",
      "type": "n8n-nodes-base.summarize",
      "role": "summarize",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Retrieve Documents from Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreQdrant",
      "role": "vectorStoreQdrant",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Set Prompt and Output",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Gemini Answer",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Answer",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.8"
    },
    {
      "name": "Chat Buffer Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Respond to Webhook",
      "type": "n8n-nodes-base.respondToWebhook",
      "role": "respondToWebhook",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "When Executed by Another Workflow",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "role": "executeWorkflowTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Combined Fields",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Sticky Note7",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note9",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}