{
  "id": 7170,
  "slug": "7170",
  "title": "Document Q&A with RAG: Query PDF content using Weaviate and OpenAI",
  "description": "# RAG over a PDF with Weaviate\nThis workflow allows you to upload a PDF file and ask questions about it using the Question and Answer Chain and the Weaviate Vector Store nodes.\n\n## Who it's for\nThis workflow is the simplest possible implementation of RAG with Weaviate in n8n. It's intended to act as an extendable template for RAG over your own documents.\n\n## Prerequisites\n1.  **An existing Weaviate cluster.** You can view instructions for setting up a **local cluster** with Docker [here](https://weaviate.io/developers/weaviate/installation/docker-compose#starter-docker-compose-file) or a **Weaviate Cloud** cluster [here](https://weaviate.io/developers/wcs/quickstart).\n2.  **API keys** to generate embeddings and power chat models. We use [OpenAI](https://openai.com/), but feel free to switch out the models as you like.\n3.  **Self-hosted n8n instance.** See this [video](https://www.youtube.com/watch?v=kq5bmrjPPAY&t=108s) for how to get set up in just three minutes.\n\n## How it works\n### Part 1: Manually upload data\nIn this example, we manually upload a 100+ page article from arXiv called [\"A Survey of Large Language Models\"](https://arxiv.org/pdf/2303.18223). But you can replace this with your own more advanced data pipeline, if you wish.\n### Part 2: Embed and load data into Weaviate collection\nHere, we generate embeddings for the full-text of the article and store them in Weaviate.\n### Part 3: Perform RAG over PDF file with Weaviate\nIn this part of the workflow, you can enter your query by running the Chat Node and get a RAG response grounded in context via the Question and Answer Chain node.\n\n\n## How to run the workflow\n1. Go through the prerequisites, creating a Weaviate cluster (can be local or cloud), downloading self-hosted n8n, and adding your API keys and other credentials.\n2. Select the embedding and chat models you'd like to use.\n3. Upload a PDF file you want to ask questions about.\n4. Execute the rest of the workflow.\n",
  "featuredImage": "/data/workflows/7170/7170.webp",
  "author": {
    "id": 101,
    "slug": "maryn",
    "name": "Mary Newhauser",
    "avatar": ""
  },
  "categories": [
    "Document Extraction",
    "Multimodal AI"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 2011,
  "downloads": 201,
  "createdAt": "2025-08-08T11:00:43.813Z",
  "updatedAt": "2026-01-16T08:48:49.519Z",
  "publishedAt": "2025-08-08T11:00:43.813Z",
  "nodes": 17,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/7170",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Document Q&A with RAG: Query PDF content using Weaviate and OpenAI",
    "workflowName": "Document Q&A with RAG: Query PDF content using Weaviate and OpenAI",
    "description": "# RAG over a PDF with Weaviate\nThis workflow allows you to upload a PDF file and ask questions about it using the Question and Answer Chain and the Weaviate Vector Store nodes.\n\n## Who it's for\nThis workflow is the simplest possible implementation of RAG with Weaviate in n8n. It's intended to act as an extendable template for RAG over your own documents.\n\n## Prerequisites\n1.  **An existing Weaviate cluster.** You can view instructions for setting up a **local cluster** with Docker [here](https://weaviate.io/developers/weaviate/installation/docker-compose#starter-docker-compose-file) or a **Weaviate Cloud** cluster [here](https://weaviate.io/developers/wcs/quickstart).\n2.  **API keys** to generate embeddings and power chat models. We use [OpenAI](https://openai.com/), but feel free to switch out the models as you like.\n3.  **Self-hosted n8n instance.** See this [video](https://www.youtube.com/watch?v=kq5bmrjPPAY&t=108s) for how to get set up in just three minutes.\n\n## How it works\n### Part 1: Manually upload data\nIn this example, we manually upload a 100+ page article from arXiv called [\"A Survey of Large Language Models\"](https://arxiv.org/pdf/2303.18223). But you can replace this with your own more advanced data pipeline, if you wish.\n### Part 2: Embed and load data into Weaviate collection\nHere, we generate embeddings for the full-text of the article and store them in Weaviate.\n### Part 3: Perform RAG over PDF file with Weaviate\nIn this part of the workflow, you can enter your query by running the Chat Node and get a RAG response grounded in context via the Question and Answer Chain node.\n\n\n## How to run the workflow\n1. Go through the prerequisites, creating a Weaviate cluster (can be local or cloud), downloading self-hosted n8n, and adding your API keys and other credentials.\n2. Select the embedding and chat models you'd like to use.\n3. Upload a PDF file you want to ask questions about.\n4. Execute the rest of the workflow.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Weaviate Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreWeaviate",
      "role": "vectorStoreWeaviate",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings OpenAI",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Recursive Character Text Splitter1",
      "type": "@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
      "role": "textSplitterRecursiveCharacterTextSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "Extract from File",
      "type": "n8n-nodes-base.extractFromFile",
      "role": "extractFromFile",
      "configDescription": "Version 1"
    },
    {
      "name": "Edit Fields",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Weaviate Vector Store1",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreWeaviate",
      "role": "vectorStoreWeaviate",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Question and Answer Chain",
      "type": "@n8n/n8n-nodes-langchain.chainRetrievalQa",
      "role": "chainRetrievalQa",
      "configDescription": "Version 1.6"
    },
    {
      "name": "Vector Store Retriever",
      "type": "@n8n/n8n-nodes-langchain.retrieverVectorStore",
      "role": "retrieverVectorStore",
      "configDescription": "Version 1"
    },
    {
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Embeddings OpenAI1",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Upload PDF",
      "type": "n8n-nodes-base.formTrigger",
      "role": "formTrigger",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}