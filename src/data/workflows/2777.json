{
  "id": 2777,
  "slug": "2777",
  "title": "üêãDeepSeek V3 chat & R1 reasoning quick start",
  "description": "This n8n workflow demonstrates multiple ways to harness DeepSeek's AI models in your automation pipeline! üåü\n\n## Core Features\n\n**Multiple Integration Methods** üîå\n- Local deployment using Ollama for DeepSeek-R1\n- Direct API integration with DeepSeek Chat V3\n- Conversational agent with memory buffer\n- HTTP request implementation with both raw and JSON formats\n\n**Model Options** üß†\n- DeepSeek Chat V3 for general conversation\n- DeepSeek-R1 for advanced reasoning\n- Memory-enabled agent for persistent context\n\n## Quick Setup üõ†Ô∏è\n\n**API Configuration**\n- Base URL: https://api.deepseek.com\n- Get your API key from platform.deepseek.com/api_keys\n\n**Local Setup** üíª\n- Install Ollama for local deployment\n- Set up DeepSeek-R1 via Ollama\n- Configure local credentials in n8n\n\n## Implementation Details üîß\n\n**Conversational Agent**\n- Window Buffer Memory for context\n- Customizable system messages\n- Built-in error handling with retries\n\n**API Endpoints** üåê\n- Chat completions for V3 and R1 models\n- OpenAI API format compatibles\n",
  "featuredImage": "/data/workflows/2777/2777.webp",
  "author": {
    "id": 101,
    "slug": "joe",
    "name": "Joseph LePage",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "AI Chatbot"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 39257,
  "downloads": 3925,
  "createdAt": "2025-01-22T23:27:28.003Z",
  "updatedAt": "2026-01-16T08:26:24.998Z",
  "publishedAt": "2025-01-22T23:27:28.003Z",
  "nodes": 15,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/2777",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "üêãDeepSeek V3 chat & R1 reasoning quick start",
    "workflowName": "üêãDeepSeek V3 chat & R1 reasoning quick start",
    "description": "This n8n workflow demonstrates multiple ways to harness DeepSeek's AI models in your automation pipeline! üåü\n\n## Core Features\n\n**Multiple Integration Methods** üîå\n- Local deployment using Ollama for DeepSeek-R1\n- Direct API integration with DeepSeek Chat V3\n- Conversational agent with memory buffer\n- HTTP request implementation with both raw and JSON formats\n\n**Model Options** üß†\n- DeepSeek Chat V3 for general conversation\n- DeepSeek-R1 for advanced reasoning\n- Memory-enabled agent for persistent context\n\n## Quick Setup üõ†Ô∏è\n\n**API Configuration**\n- Base URL: https://api.deepseek.com\n- Get your API key from platform.deepseek.com/api_keys\n\n**Local Setup** üíª\n- Install Ollama for local deployment\n- Set up DeepSeek-R1 via Ollama\n- Configure local credentials in n8n\n\n## Implementation Details üîß\n\n**Conversational Agent**\n- Window Buffer Memory for context\n- Customizable system messages\n- Built-in error handling with retries\n\n**API Endpoints** üåê\n- Chat completions for V3 and R1 models\n- OpenAI API format compatibles",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.7"
    },
    {
      "name": "DeepSeek",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Window Buffer Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Basic LLM Chain2",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "role": "chainLlm",
      "configDescription": "Version 1.5"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Ollama DeepSeek",
      "type": "@n8n/n8n-nodes-langchain.lmChatOllama",
      "role": "lmChatOllama",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "DeepSeek JSON Body",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "DeepSeek Raw Body",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}