{
  "id": 9775,
  "slug": "9775",
  "title": "Dual-path customer support system with Google Sheets, vectors & Gemini",
  "description": "\n#### This n8n workflow template implements a dual-path architecture for AI customer support, based on the principles outlined in the research paper \"[A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation](https://arxiv.org/abs/2510.01671)\" (Sato et al.).\n\n\nThe system, named **LENOHA** (Low Energy, No Hallucination, Leave No One Behind Architecture), uses a high-precision classifier to differentiate between high-stakes queries and casual conversation. Queries matching a known FAQ are answered with a pre-approved, verbatim response, structurally eliminating hallucination risk. All other queries are routed to a standard generative LLM for conversational flexibility.\n\nThis template provides a **practical ++blueprint++** for building safer, more reliable, and cost-efficient AI agents, particularly in **regulated** or **high-stakes domains** where factual accuracy is critical.\n\n### What This Template Does (Step-by-Step)\n- Loads an expert-curated FAQ from Google Sheets and creates a searchable vector store from the questions during a one-time setup flow.\n- Receives incoming user queries in real-time via a chat trigger.\n- Classifies user intent by converting the query to an embedding and searching the vector store for the most semantically similar FAQ question.\n- Routes the query down one of two paths based on a configurable similarity score threshold.\n- Responds with a verbatim, pre-approved answer if a match is found (safe path), or generates a conversational reply via an LLM if no match is found (casual path).\n\n\n\n### Important Note for Production Use\nThis template uses an in-memory Simple Vector Store for demonstration purposes. For a production application, this should be replaced with a persistent vector database (e.g., Pinecone, Chroma, Weaviate, Supabase) to store your embeddings permanently.\n\n### Required Integrations:\n- Google Sheets (for the FAQ knowledge base)\n- Hugging Face API (for creating embeddings)\n- An LLM provider (e.g., OpenAI, Anthropic, Mistral)\n- (Recommended) A persistent Vector Store integration.\n\n### Best For:\nüè¶ Organizations in regulated industries (finance, healthcare) requiring high accuracy.\nüí∞ Applications where reducing LLM operational costs is a priority.\n‚öôÔ∏è Technical support agents that must provide precise, unchanging information.\nüîí Systems where auditability and deterministic responses for known issues are required.\n\n### Key Benefits:\n‚úÖ Structurally eliminates hallucination risk for known topics.\n‚úÖ Reduces reliance on expensive generative models for common queries.\n‚úÖ Ensures deterministic, accurate, and consistent answers for your FAQ.\n‚úÖ Provides high-speed classification via vector search.\n‚úÖ Implements a research-backed architecture for building safer AI systems.",
  "featuredImage": "/data/workflows/9775/9775.webp",
  "author": {
    "id": 101,
    "slug": "maximosipovs",
    "name": "Maxim Osipovs",
    "avatar": ""
  },
  "categories": [
    "Support Chatbot",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 185,
  "downloads": 18,
  "createdAt": "2025-10-16T11:42:08.400Z",
  "updatedAt": "2026-01-16T09:01:52.352Z",
  "publishedAt": "2025-10-16T11:42:08.400Z",
  "nodes": 20,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/9775",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Dual-path customer support system with Google Sheets, vectors & Gemini",
    "workflowName": "Dual-path customer support system with Google Sheets, vectors & Gemini",
    "description": "#### This n8n workflow template implements a dual-path architecture for AI customer support, based on the principles outlined in the research paper \"[A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation](https://arxiv.org/abs/2510.01671)\" (Sato et al.).\n\n\nThe system, named **LENOHA** (Low Energy, No Hallucination, Leave No One Behind Architecture), uses a high-precision classifier to differentiate between high-stakes queries and casual conversation. Queries matching a known FAQ are answered with a pre-approved, verbatim response, structurally eliminating hallucination risk. All other queries are routed to a standard generative LLM for conversational flexibility.\n\nThis template provides a **practical ++blueprint++** for building safer, more reliable, and cost-efficient AI agents, particularly in **regulated** or **high-stakes domains** where factual accuracy is critical.\n\n### What This Template Does (Step-by-Step)\n- Loads an expert-curated FAQ from Google Sheets and creates a searchable vector store from the questions during a one-time setup flow.\n- Receives incoming user queries in real-time via a chat trigger.\n- Classifies user intent by converting the query to an embedding and searching the vector store for the most semantically similar FAQ question.\n- Routes the query down one of two paths based on a configurable similarity score threshold.\n- Responds with a verbatim, pre-approved answer if a match is found (safe path), or generates a conversational reply via an LLM if no match is found (casual path).\n\n\n\n### Important Note for Production Use\nThis template uses an in-memory Simple Vector Store for demonstration purposes. For a production application, this should be replaced with a persistent vector database (e.g., Pinecone, Chroma, Weaviate, Supabase) to store your embeddings permanently.\n\n### Required Integrations:\n- Google Sheets (for the FAQ knowledge base)\n- Hugging Face API (for creating embeddings)\n- An LLM provider (e.g., OpenAI, Anthropic, Mistral)\n- (Recommended) A persistent Vector Store integration.\n\n### Best For:\nüè¶ Organizations in regulated industries (finance, healthcare) requiring high accuracy.\nüí∞ Applications where reducing LLM operational costs is a priority.\n‚öôÔ∏è Technical support agents that must provide precise, unchanging information.\nüîí Systems where auditability and deterministic responses for known issues are required.\n\n### Key Benefits:\n‚úÖ Structurally eliminates hallucination risk for known topics.\n‚úÖ Reduces reliance on expensive generative models for common queries.\n‚úÖ Ensures deterministic, accurate, and consistent answers for your FAQ.\n‚úÖ Provides high-speed classification via vector search.\n‚úÖ Implements a research-backed architecture for building safer AI systems.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When clicking ‚ÄòExecute workflow‚Äô",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings HuggingFace Inference",
      "type": "@n8n/n8n-nodes-langchain.embeddingsHuggingFaceInference",
      "role": "embeddingsHuggingFaceInference",
      "configDescription": "Version 1"
    },
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1.1"
    },
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Respond to Chat",
      "type": "@n8n/n8n-nodes-langchain.chat",
      "role": "chat",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings HuggingFace Inference2",
      "type": "@n8n/n8n-nodes-langchain.embeddingsHuggingFaceInference",
      "role": "embeddingsHuggingFaceInference",
      "configDescription": "Version 1"
    },
    {
      "name": "Respond to Chat1",
      "type": "@n8n/n8n-nodes-langchain.chat",
      "role": "chat",
      "configDescription": "Version 1"
    },
    {
      "name": "Knowledge Database",
      "type": "n8n-nodes-base.googleSheets",
      "role": "googleSheets",
      "configDescription": "Version 4.7"
    },
    {
      "name": "Extract Questions",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Generate & Store Embeddings",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreInMemory",
      "role": "vectorStoreInMemory",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Retrieve & Score Embeddings",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreInMemory",
      "role": "vectorStoreInMemory",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Determine Question Type",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Get Respective Answers",
      "type": "n8n-nodes-base.googleSheets",
      "role": "googleSheets",
      "configDescription": "Version 4.7"
    },
    {
      "name": "Forward Chat Message",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.2"
    },
    {
      "name": "Chat Model",
      "type": "@n8n/n8n-nodes-langchain.googleGemini",
      "role": "googleGemini",
      "configDescription": "Version 1"
    }
  ]
}