{
  "id": 5741,
  "slug": "5741",
  "title": "Generate cinematic videos from text prompts with GPT-4o, Fal.AI Seedance & Audio",
  "description": "### Who’s it for?\n\nThis workflow is built for:\n- **AI storytellers**, **content creators**, **YouTubers**, and **short-form video marketers**\n- Anyone looking to transform text prompts into **cinematic AI-generated videos** fully automatically\n- **Educators**, **trainers**, or **agencies** creating story-based visual content at scale\n\n---\n\n###  What It Does\n\nThis n8n workflow allows you to automatically turn a **simple text prompt** into a **multi-scene cinematic video**, using the powerful **Fal.AI Seedance V1.0** model (developed by **ByteDance** — the creators of TikTok).\n\nIt combines the creativity of **GPT-4o**, the motion synthesis of **Seedance**, and the automation power of **n8n** to generate AI videos with ambient sound and publish-ready format.\n\n---\n\n### How It Works\n\n1. Accepts a prompt from **Google Sheets** (configurable fields like duration, aspect ratio, resolution, scene count)\n2. Uses **OpenAI GPT-4o** to write a vivid cinematic **narrative**\n3. Splits the story into **n separate scenes**\n4. For each scene:\n   - GPT generates a structured cinematic description (characters, camera, movement, sound)\n   - The **Seedance V1.0 model (via Fal.AI API)** renders a 5s animated video\n   - Optional: Adds ambient **audio via Fal’s MM-Audio model**\n5. Finally:\n   - Merges all scene videos using **Fal’s FFmpeg API**\n   - Optionally **uploads to YouTube automatically**\n\n---\n\n###  Why This Is Special\n\n- **Fal.AI Seedance V1.0** is a highly advanced motion video model developed by ByteDance, capable of generating expressive, stylized 5–6 second cinematic clips from text.\n- This workflow supports full looping, scene count validation, and wait-polling for long render jobs.\n- The entire story, breakdown, and scene design are AI-generated — no manual effort needed.\n- Output is export-ready: MP4 with sound, ideal for YouTube Shorts, Reels, or TikTok.\n\n---\n\n### Requirements\n\n- n8n (Self-hosted recommended)\n- API Keys:\n  -  `Fal.AI` (https://fal.ai)\n  -  `OpenAI` (GPT-4o or 3.5)\n  -  `Google Sheets` [Example Google Sheet](https://docs.google.com/spreadsheets/d/1FuDdvkzq5TZ3Evs92BxUxD4qOK0EDLAzB-SayKwpAdw)\n\n---\n\n### How to Set It Up\n\n1. Clone the template into your n8n instance\n2. Configure credentials:\n   - Fal.AI Header Token\n   - OpenAI API Key\n   - Google Sheets OAuth2\n   - (Optional) YouTube API OAuth\n3. Prepare a Google Sheet with these columns:\n   - `story` (short prompt)\n   - `number_of_scene`\n   - `duration` (per clip)\n   - `aspect_ratio`, `resolution`, `model`\n4. Run manually or trigger on Sheet update.\n\n---\n\n### How to Customize\n\n- Modify the storytelling tone in GPT prompts (e.g., switch to fantasy, horror, sci-fi)\n- Change Seedance model params like style or seed\n- Add subtitles or branding overlays to final video\n- Integrate LINE, Notion, or Telegram for auto-sharing\n\n---\n\n### Example Output\n\n**Prompt**: *“A rabbit flies to the moon on a dragonfly and eats watermelon together”*  \n→ Result: 3 scenes, each 5s, cinematic camera pans, soft ambient audio, auto-uploaded to YouTube\n[Result](https://youtu.be/_PKvi0Sfs84)\n",
  "featuredImage": "/data/workflows/5741/5741.webp",
  "author": {
    "id": 101,
    "slug": "jaruphatj",
    "name": "Jaruphat J.",
    "avatar": ""
  },
  "categories": [
    "Content Creation",
    "Multimodal AI"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 18134,
  "downloads": 1813,
  "createdAt": "2025-07-07T09:21:49.644Z",
  "updatedAt": "2026-01-16T08:41:12.126Z",
  "publishedAt": "2025-07-07T09:21:49.644Z",
  "nodes": 38,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/5741",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Generate cinematic videos from text prompts with GPT-4o, Fal.AI Seedance & Audio",
    "workflowName": "Generate cinematic videos from text prompts with GPT-4o, Fal.AI Seedance & Audio",
    "description": "### Who’s it for?\n\nThis workflow is built for:\n- **AI storytellers**, **content creators**, **YouTubers**, and **short-form video marketers**\n- Anyone looking to transform text prompts into **cinematic AI-generated videos** fully automatically\n- **Educators**, **trainers**, or **agencies** creating story-based visual content at scale\n\n---\n\n###  What It Does\n\nThis n8n workflow allows you to automatically turn a **simple text prompt** into a **multi-scene cinematic video**, using the powerful **Fal.AI Seedance V1.0** model (developed by **ByteDance** — the creators of TikTok).\n\nIt combines the creativity of **GPT-4o**, the motion synthesis of **Seedance**, and the automation power of **n8n** to generate AI videos with ambient sound and publish-ready format.\n\n---\n\n### How It Works\n\n1. Accepts a prompt from **Google Sheets** (configurable fields like duration, aspect ratio, resolution, scene count)\n2. Uses **OpenAI GPT-4o** to write a vivid cinematic **narrative**\n3. Splits the story into **n separate scenes**\n4. For each scene:\n   - GPT generates a structured cinematic description (characters, camera, movement, sound)\n   - The **Seedance V1.0 model (via Fal.AI API)** renders a 5s animated video\n   - Optional: Adds ambient **audio via Fal’s MM-Audio model**\n5. Finally:\n   - Merges all scene videos using **Fal’s FFmpeg API**\n   - Optionally **uploads to YouTube automatically**\n\n---\n\n###  Why This Is Special\n\n- **Fal.AI Seedance V1.0** is a highly advanced motion video model developed by ByteDance, capable of generating expressive, stylized 5–6 second cinematic clips from text.\n- This workflow supports full looping, scene count validation, and wait-polling for long render jobs.\n- The entire story, breakdown, and scene design are AI-generated — no manual effort needed.\n- Output is export-ready: MP4 with sound, ideal for YouTube Shorts, Reels, or TikTok.\n\n---\n\n### Requirements\n\n- n8n (Self-hosted recommended)\n- API Keys:\n  -  `Fal.AI` (https://fal.ai)\n  -  `OpenAI` (GPT-4o or 3.5)\n  -  `Google Sheets` [Example Google Sheet](https://docs.google.com/spreadsheets/d/1FuDdvkzq5TZ3Evs92BxUxD4qOK0EDLAzB-SayKwpAdw)\n\n---\n\n### How to Set It Up\n\n1. Clone the template into your n8n instance\n2. Configure credentials:\n   - Fal.AI Header Token\n   - OpenAI API Key\n   - Google Sheets OAuth2\n   - (Optional) YouTube API OAuth\n3. Prepare a Google Sheet with these columns:\n   - `story` (short prompt)\n   - `number_of_scene`\n   - `duration` (per clip)\n   - `aspect_ratio`, `resolution`, `model`\n4. Run manually or trigger on Sheet update.\n\n---\n\n### How to Customize\n\n- Modify the storytelling tone in GPT prompts (e.g., switch to fantasy, horror, sci-fi)\n- Change Seedance model params like style or seed\n- Add subtitles or branding overlays to final video\n- Integrate LINE, Notion, or Telegram for auto-sharing\n\n---\n\n### Example Output\n\n**Prompt**: *“A rabbit flies to the moon on a dragonfly and eats watermelon together”*  \n→ Result: 3 scenes, each 5s, cinematic camera pans, soft ambient audio, auto-uploaded to YouTube\n[Result](https://youtu.be/_PKvi0Sfs84)",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Structured Output Parser",
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "role": "outputParserStructured",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Structured Output Parser1",
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "role": "outputParserStructured",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Split Out",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "OpenAI Chat Model1",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Structured Output Parser2",
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "role": "outputParserStructured",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Wait for the video",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Get the video status",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Video status",
      "type": "n8n-nodes-base.switch",
      "role": "switch",
      "configDescription": "Version 3.2"
    },
    {
      "name": "Get the  video",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Loop Over Items",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "Get audio status",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Get video with audio",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Start adding audio to the video",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Wait for adding the audio",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Audio status",
      "type": "n8n-nodes-base.switch",
      "role": "switch",
      "configDescription": "Version 3.2"
    },
    {
      "name": "Loop Over Items1",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "Start merging videos",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Wait for the merge to complete",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Merge videos status",
      "type": "n8n-nodes-base.switch",
      "role": "switch",
      "configDescription": "Version 3.2"
    },
    {
      "name": "Get merge videos status",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Get merged video",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Aggregate videos with audio",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "OpenAI Chat Model2",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "When clicking ‘Execute workflow’",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Get Data",
      "type": "n8n-nodes-base.googleSheets",
      "role": "googleSheets",
      "configDescription": "Version 4.6"
    },
    {
      "name": "Verify number of scene",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Generate Full Narrative from Prompt",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 2"
    },
    {
      "name": "Break Narrative into {{n}} Scenes",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 2"
    },
    {
      "name": "Describe Each Scene for Video",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 2"
    },
    {
      "name": "Call Fal.ai API (Seedance)",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Scene count",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "YouTube",
      "type": "n8n-nodes-base.youTube",
      "role": "youTube",
      "configDescription": "Version 1"
    },
    {
      "name": "Get the  video1",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    }
  ]
}