{
  "id": 3527,
  "slug": "3527",
  "title": "Compare sequential, agent-based, and parallel LLM processing with Claude 3.7",
  "description": "This workflow demonstrates three distinct approaches to chaining LLM operations using Claude 3.7 Sonnet. Connect to any section to experience the differences in implementation, performance, and capabilities.\n\n## What you'll find:\n\n### 1️⃣ Naive Sequential Chaining\nThe simplest but least efficient approach - connecting LLM nodes in a direct sequence. Easy to set up for beginners but becomes unwieldy and slow as your chain grows.\n\n### 2️⃣ Agent-Based Processing with Memory\nProcess a list of instructions through a single AI Agent that maintains conversation history. This structured approach provides better context management while keeping your workflow organized.\n\n### 3️⃣ Parallel Processing for Maximum Speed\nSplit your prompts and process them simultaneously for much faster results. Ideal when you need to run multiple independent tasks without shared context.\n\n## Setup Instructions:\n\n1. **API Credentials**: Configure your Anthropic API key in the credentials manager. This workflow uses Claude 3.7 Sonnet, but you can modify the model in each Anthropic Chat Model node, or pick an entirely different LLM.\n\n2. **For Cloud Users**: If using the parallel processing method (section 3), replace `{{ $env.WEBHOOK_URL }}` in the \"LLM steps - parallel\" HTTP Request node with your n8n instance URL.\n\n3. **Test Data**: The workflow fetches content from the n8n blog by default. You can modify this part to use a different content or a data source.\n\n4. **Customization**: Each section contains a set of example prompts. Modify the \"Initial prompts\" nodes to change the questions asked to the LLM.\n\nCompare these methods to understand the trade-offs between simplicity, speed, and context management in your AI workflows!\n\n---\n\nFollow me on [LinkedIn](https://www.linkedin.com/in/parsadanyan/) for more tips on AI automation and n8n workflows!",
  "featuredImage": "/data/workflows/3527/3527.webp",
  "author": {
    "id": 101,
    "slug": "eduard",
    "name": "Eduard",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "Multimodal AI"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 1816,
  "downloads": 181,
  "createdAt": "2025-04-12T15:33:29.105Z",
  "updatedAt": "2026-01-16T08:30:10.124Z",
  "publishedAt": "2025-04-12T15:33:29.105Z",
  "nodes": 38,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/3527",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Compare sequential, agent-based, and parallel LLM processing with Claude 3.7",
    "workflowName": "Compare sequential, agent-based, and parallel LLM processing with Claude 3.7",
    "description": "This workflow demonstrates three distinct approaches to chaining LLM operations using Claude 3.7 Sonnet. Connect to any section to experience the differences in implementation, performance, and capabilities.\n\n## What you'll find:\n\n### 1️⃣ Naive Sequential Chaining\nThe simplest but least efficient approach - connecting LLM nodes in a direct sequence. Easy to set up for beginners but becomes unwieldy and slow as your chain grows.\n\n### 2️⃣ Agent-Based Processing with Memory\nProcess a list of instructions through a single AI Agent that maintains conversation history. This structured approach provides better context management while keeping your workflow organized.\n\n### 3️⃣ Parallel Processing for Maximum Speed\nSplit your prompts and process them simultaneously for much faster results. Ideal when you need to run multiple independent tasks without shared context.\n\n## Setup Instructions:\n\n1. **API Credentials**: Configure your Anthropic API key in the credentials manager. This workflow uses Claude 3.7 Sonnet, but you can modify the model in each Anthropic Chat Model node, or pick an entirely different LLM.\n\n2. **For Cloud Users**: If using the parallel processing method (section 3), replace `{{ $env.WEBHOOK_URL }}` in the \"LLM steps - parallel\" HTTP Request node with your n8n instance URL.\n\n3. **Test Data**: The workflow fetches content from the n8n blog by default. You can modify this part to use a different content or a data source.\n\n4. **Customization**: Each section contains a set of example prompts. Modify the \"Initial prompts\" nodes to change the questions asked to the LLM.\n\nCompare these methods to understand the trade-offs between simplicity, speed, and context management in your AI workflows!\n\n---\n\nFollow me on [LinkedIn](https://www.linkedin.com/in/parsadanyan/) for more tips on AI automation and n8n workflows!",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When clicking ‘Test workflow’",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "HTTP Request",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Markdown",
      "type": "n8n-nodes-base.markdown",
      "role": "markdown",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Anthropic Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
      "role": "lmChatAnthropic",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Anthropic Chat Model1",
      "type": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
      "role": "lmChatAnthropic",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Anthropic Chat Model2",
      "type": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
      "role": "lmChatAnthropic",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Anthropic Chat Model3",
      "type": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
      "role": "lmChatAnthropic",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Merge",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.1"
    },
    {
      "name": "Simple Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Clean memory",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "role": "memoryManager",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Initial prompts",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Split Out",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Reshape",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Anthropic Chat Model4",
      "type": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
      "role": "lmChatAnthropic",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Merge2",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Basic LLM Chain4",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "role": "chainLlm",
      "configDescription": "Version 1.5"
    },
    {
      "name": "Split Out1",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Anthropic Chat Model5",
      "type": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
      "role": "lmChatAnthropic",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "role": "webhook",
      "configDescription": "Version 2"
    },
    {
      "name": "CONNECT ME",
      "type": "n8n-nodes-base.noOp",
      "role": "noOp",
      "configDescription": "Version 1"
    },
    {
      "name": "CONNECT ME1",
      "type": "n8n-nodes-base.noOp",
      "role": "noOp",
      "configDescription": "Version 1"
    },
    {
      "name": "CONNECT ME2",
      "type": "n8n-nodes-base.noOp",
      "role": "noOp",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Initial prompts1",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "LLM Chain - Step 1",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "role": "chainLlm",
      "configDescription": "Version 1.6"
    },
    {
      "name": "LLM Chain - Step 2",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "role": "chainLlm",
      "configDescription": "Version 1.6"
    },
    {
      "name": "LLM Chain - Step 3",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "role": "chainLlm",
      "configDescription": "Version 1.6"
    },
    {
      "name": "LLM Chain - Step 4",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "role": "chainLlm",
      "configDescription": "Version 1.6"
    },
    {
      "name": "All LLM steps here - sequentially",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.8"
    },
    {
      "name": "LLM steps - parallel",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Merge output with initial prompts",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.1"
    },
    {
      "name": "Merge output with initial prompts1",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.1"
    }
  ]
}