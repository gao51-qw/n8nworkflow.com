{
  "id": 3967,
  "slug": "3967",
  "title": "Build an on-premises AI Kaggle competition assistant with Qdrant RAG and Ollama",
  "description": "# LLM/RAG Kaggle Development Assistant\n\nAn on-premises, domain-specific AI assistant for Kaggle (tested on binary disaster-tweet classification), combining LLM, an n8n workflow engine, and Qdrant-backed Retrieval-Augmented Generation (RAG).\nDeploy via containerized starter kit.\nNeeds high end GPU support or patience.\nInitial chat should contain guidelines on what to to produce and the challenge guidelines.\n---\n##  Features\n\n- **Coding Assistance**  \n  • \"Real\"-time Python code recommendations, debugging help, and data-science best practices  \n  • Multi-turn conversational context\n- **Workflow Automation**  \n  • n8n orchestration for LLM calls, document ingestion, and external API integrations  \n- **Retrieval-Augmented Generation (RAG)**  \n  • Qdrant vector-database for competition-specific document lookup  \n  • On-demand retrieval of Kaggle competition guidelines, tutorials, and notebooks after convertion to HTML and ingestion into RAG\n- **entirly On-Premises for Privacy**  \n  • Locally hosted LLM (via Ollama) – no external code or data transfer  \n\nALIENTELLIGENCE/contentsummarizer:latest for summarizing\nqwen3:8b for chat and coding\nmxbai-embed-large:latest for embedding\n  \n  • GPU acceleration required\n\nBased on:\nhttps://n8n.io/workflows/2339 breakdown documents into study notes using templating mistralai and qdrant/",
  "featuredImage": "/data/workflows/3967/3967.webp",
  "author": {
    "id": 101,
    "slug": "jac2325057",
    "name": "JHH",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 4393,
  "downloads": 439,
  "createdAt": "2025-05-10T05:03:59.199Z",
  "updatedAt": "2026-01-16T08:32:24.055Z",
  "publishedAt": "2025-05-10T05:03:59.199Z",
  "nodes": 23,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/3967",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Build an on-premises AI Kaggle competition assistant with Qdrant RAG and Ollama",
    "workflowName": "Build an on-premises AI Kaggle competition assistant with Qdrant RAG and Ollama",
    "description": "# LLM/RAG Kaggle Development Assistant\n\nAn on-premises, domain-specific AI assistant for Kaggle (tested on binary disaster-tweet classification), combining LLM, an n8n workflow engine, and Qdrant-backed Retrieval-Augmented Generation (RAG).\nDeploy via containerized starter kit.\nNeeds high end GPU support or patience.\nInitial chat should contain guidelines on what to to produce and the challenge guidelines.\n---\n##  Features\n\n- **Coding Assistance**  \n  • \"Real\"-time Python code recommendations, debugging help, and data-science best practices  \n  • Multi-turn conversational context\n- **Workflow Automation**  \n  • n8n orchestration for LLM calls, document ingestion, and external API integrations  \n- **Retrieval-Augmented Generation (RAG)**  \n  • Qdrant vector-database for competition-specific document lookup  \n  • On-demand retrieval of Kaggle competition guidelines, tutorials, and notebooks after convertion to HTML and ingestion into RAG\n- **entirly On-Premises for Privacy**  \n  • Locally hosted LLM (via Ollama) – no external code or data transfer  \n\nALIENTELLIGENCE/contentsummarizer:latest for summarizing\nqwen3:8b for chat and coding\nmxbai-embed-large:latest for embedding\n  \n  • GPU acceleration required\n\nBased on:\nhttps://n8n.io/workflows/2339 breakdown documents into study notes using templating mistralai and qdrant/",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Local File Trigger",
      "type": "n8n-nodes-base.localFileTrigger",
      "role": "localFileTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1"
    },
    {
      "name": "Recursive Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
      "role": "textSplitterRecursiveCharacterTextSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "Settings",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.3"
    },
    {
      "name": "Merge",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 2.1"
    },
    {
      "name": "Get FileType",
      "type": "n8n-nodes-base.switch",
      "role": "switch",
      "configDescription": "Version 3"
    },
    {
      "name": "Import File",
      "type": "n8n-nodes-base.readWriteFile",
      "role": "readWriteFile",
      "configDescription": "Version 1"
    },
    {
      "name": "Extract from TEXT",
      "type": "n8n-nodes-base.extractFromFile",
      "role": "extractFromFile",
      "configDescription": "Version 1"
    },
    {
      "name": "Summarization Chain",
      "type": "@n8n/n8n-nodes-langchain.chainSummarization",
      "role": "chainSummarization",
      "configDescription": "Version 2"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Qdrant Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreQdrant",
      "role": "vectorStoreQdrant",
      "configDescription": "Version 1"
    },
    {
      "name": "Markdown",
      "type": "n8n-nodes-base.markdown",
      "role": "markdown",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings Ollama",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOllama",
      "role": "embeddingsOllama",
      "configDescription": "Version 1"
    },
    {
      "name": "Ollama Summarizer",
      "type": "@n8n/n8n-nodes-langchain.lmOllama",
      "role": "lmOllama",
      "configDescription": "Version 1"
    },
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.7"
    },
    {
      "name": "Vector Store Tool",
      "type": "@n8n/n8n-nodes-langchain.toolVectorStore",
      "role": "toolVectorStore",
      "configDescription": "Version 1"
    },
    {
      "name": "Window Buffer Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Qdrant Vector Store2",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreQdrant",
      "role": "vectorStoreQdrant",
      "configDescription": "Version 1"
    },
    {
      "name": "Ollama Chat Model3",
      "type": "@n8n/n8n-nodes-langchain.lmChatOllama",
      "role": "lmChatOllama",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings Ollama2",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOllama",
      "role": "embeddingsOllama",
      "configDescription": "Version 1"
    },
    {
      "name": "Ollama Chat Model4",
      "type": "@n8n/n8n-nodes-langchain.lmChatOllama",
      "role": "lmChatOllama",
      "configDescription": "Version 1"
    }
  ]
}