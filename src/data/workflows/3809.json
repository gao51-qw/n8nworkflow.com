{
  "id": 3809,
  "slug": "3809",
  "title": "Generate SEO content audit reports with DataForSEO and Google Search Console",
  "description": "## Introduction\nThe Content SEO Audit Workflow is a powerful automated solution that generates comprehensive SEO audit reports for websites. \n\nBy combining the crawling capabilities of DataForSEO with the search performance metrics from Google Search Console, this workflow delivers actionable insights into content quality, technical SEO issues, and performance optimization opportunities. \n\nThe workflow crawls up to 1,000 pages of a website, analyzes various SEO factors including metadata, content quality, internal linking, and search performance, and then generates a professional, branded HTML report that can be shared directly with clients. \n\nThe entire process is automated, transforming what would typically be hours of manual analysis into a streamlined workflow that produces consistent, thorough results. \n\nThis workflow bridges the gap between technical SEO auditing and practical, client-ready deliverables, making it an invaluable tool for SEO professionals and digital marketing agencies.\n\n## Who is this for?\nThis workflow is designed for SEO consultants, digital marketing agencies, and content strategists who need to perform comprehensive content audits for clients or their own websites. It's particularly valuable for professionals who:\n\n1. Regularly conduct SEO audits as part of their service offerings\n2. Need to provide branded, professional reports to clients\n3. Want to automate the time-consuming process of content analysis\n4. Require data-driven insights to inform content strategy decisions\n\nUsers should have basic familiarity with SEO concepts and metrics, as well as a basic understanding of how to set up API credentials in n8n. \n\nWhile no coding knowledge is required to run the workflow, users should be comfortable with configuring workflow parameters and following setup instructions.\n\n## What problem is this workflow solving?\nContent audits are essential for SEO strategy but are traditionally labor-intensive and time-consuming. This workflow addresses several key challenges:\n\n1. **Manual Data Collection**: Gathering data from multiple sources (crawlers, Google Search Console, etc.) typically requires hours of work. This workflow automates the entire data collection process.\n\n2. **Inconsistent Analysis**: Manual audits can suffer from inconsistency in methodology. This workflow applies the same comprehensive analysis criteria to every page, ensuring thorough and consistent results.\n\n3. **Report Generation**: Creating professional, client-ready reports often requires additional design work after the analysis is complete. This workflow generates a fully branded HTML report automatically.\n\n4. **Data Integration**: Correlating technical SEO issues with actual search performance metrics is difficult when working with separate tools. This workflow seamlessly integrates crawl data with Google Search Console metrics.\n\n5. **Scale Limitations**: Manual audits become increasingly difficult with larger websites. This workflow can efficiently process up to 1,000 pages without additional effort.\n\n## What this workflow does\n### Overview\nThe Content SEO Audit Workflow crawls a specified website, analyzes its content for various SEO issues, retrieves performance data from Google Search Console, and generates a comprehensive HTML report. \n\nThe workflow identifies issues in five key categories: status issues (404 errors, redirects), content quality (thin content, readability), metadata SEO (title/description issues), internal linking (orphan pages, excessive click depth), and performance (underperforming content). \n\nThe final report includes executive summaries, detailed issue breakdowns, and actionable recommendations, all branded with your company's colors and logo.\n\n### Process\n1. **Initial Configuration**: The workflow begins by setting parameters including the target domain, crawl limits, company information, and branding colors.\n\n2. **Website Crawling**: The workflow creates a crawl task in DataForSEO and periodically checks its status until completion.\n\n3. **Data Collection**: Once crawling is complete, the workflow:\n   - Retrieves the raw audit data from DataForSEO\n   - Extracts all URLs with status code 200 (successful pages)\n   - Queries Google Search Console API for each URL to get clicks and impressions data\n   - Identifies 404 and 301 pages and retrieves their source links\n\n4. **Data Analysis**: The workflow analyzes the collected data to identify issues including:\n   - Technical issues: 404 errors, redirects, canonicalization problems\n   - Content issues: thin content, outdated content, readability problems\n   - SEO metadata issues: missing/duplicate titles and descriptions, H1 problems\n   - Internal linking issues: orphan pages, excessive click depth, low internal links\n   - Performance issues: underperforming pages based on GSC data\n\n5. **Report Generation**: Finally, the workflow:\n   - Calculates a health score based on the severity and quantity of issues\n   - Generates prioritized recommendations\n   - Creates a comprehensive HTML report with interactive tables and visualizations\n   - Customizes the report with your company's branding\n   - Provides the report as a downloadable HTML file\n\n## Setup\nTo set up this workflow, follow these steps:\n\n1. **Import the workflow**: Download the JSON file and import it into your n8n instance.\n\n2. **Configure DataForSEO credentials**:\n   - Create a DataForSEO account at https://app.dataforseo.com/api-access (they offer a free $1 credit for testing)\n   - Add a new \"Basic Auth\" credential in n8n following the [HTTP Request Authentication guide](https://docs.n8n.io/integrations/builtin/credentials/httprequest/)\n   - Assign this credential to the \"Create Task\", \"Check Task Status\", \"Get Raw Audit Data\", and \"Get Source URLs Data\" nodes\n\n3. **Configure Google Search Console credentials**:\n   - Add a new \"Google OAuth2 API\" credential following the [Google OAuth guide](https://docs.n8n.io/integrations/builtin/credentials/google/oauth-generic/)\n   - Ensure your Google account has access to the Google Search Console property you want to analyze\n   - Assign this credential to the \"Query GSC API\" node\n\n4. **Update the \"Set Fields\" node** with:\n   - dfs_domain: The website domain you want to audit\n   - dfs_max_crawl_pages: Maximum number of pages to crawl (default: 1000)\n   - dfs_enable_javascript: Whether to enable JavaScript rendering (default: false)\n   - company_name: Your company name for the report branding\n   - company_website: Your company website URL\n   - company_logo_url: URL to your company logo\n   - brand_primary_color: Your primary brand color (hex code)\n   - brand_secondary_color: Your secondary brand color (hex code)\n   - gsc_property_type: Set to \"domain\" or \"url\" depending on your Google Search Console property type\n\n5. **Run the workflow**: Click \"Start\" and wait for it to complete (approximately 20 minutes for 500 pages).\n\n6. **Download the report**: Once complete, download the HTML file from the \"Download Report\" node.\n\n## How to customize this workflow to your needs\nThis workflow can be adapted in several ways to better suit your specific requirements:\n\n1. **Adjust crawl parameters**: Modify the \"Set Fields\" node to change:\n   - The maximum number of pages to crawl (dfs_max_crawl_pages). This workflow supports up to 1000 pages.\n   - Whether to enable JavaScript rendering for JavaScript-heavy sites (dfs_enable_javascript)\n\n2. **Customize issue detection thresholds**: In the \"Build Report Structure\" code node, you can modify:\n   - Word count thresholds for thin content detection (currently 1500 words)\n   - Click depth thresholds (currently flags pages deeper than 4 clicks)\n   - Title and description length parameters (currently 40-60 chars for titles, 70-155 for descriptions)\n   - Readability score thresholds (currently flags Flesch-Kincaid scores below 55)\n\n3. **Modify the report design**: In the \"Generate HTML Report\" code node, you can:\n   - Adjust the HTML/CSS to change the report layout and styling\n   - Add or remove sections from the report\n   - Change the recommendations logic\n   - Modify the health score calculation algorithm\n\n4. **Add additional data sources**: You could extend the workflow by:\n   - Adding Pagespeed Insights data for performance metrics\n   - Incorporating backlink data from other APIs\n   - Adding keyword ranking data from rank tracking APIs\n\n5. **Implement automated delivery**: Add nodes after the \"Download Report\" to:\n   - Send the report directly to clients via email\n   - Upload it to cloud storage\n   - Create a PDF version of the report",
  "featuredImage": "/data/workflows/3809/3809.webp",
  "author": {
    "id": 101,
    "slug": "customworkflowsai",
    "name": "Custom Workflows AI",
    "avatar": ""
  },
  "categories": [
    "Market Research"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 4578,
  "downloads": 457,
  "createdAt": "2025-04-30T19:12:11.896Z",
  "updatedAt": "2026-01-16T08:31:35.443Z",
  "publishedAt": "2025-04-30T19:12:11.896Z",
  "nodes": 21,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/3809",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Generate SEO content audit reports with DataForSEO and Google Search Console",
    "workflowName": "Generate SEO content audit reports with DataForSEO and Google Search Console",
    "description": "## Introduction\nThe Content SEO Audit Workflow is a powerful automated solution that generates comprehensive SEO audit reports for websites. \n\nBy combining the crawling capabilities of DataForSEO with the search performance metrics from Google Search Console, this workflow delivers actionable insights into content quality, technical SEO issues, and performance optimization opportunities. \n\nThe workflow crawls up to 1,000 pages of a website, analyzes various SEO factors including metadata, content quality, internal linking, and search performance, and then generates a professional, branded HTML report that can be shared directly with clients. \n\nThe entire process is automated, transforming what would typically be hours of manual analysis into a streamlined workflow that produces consistent, thorough results. \n\nThis workflow bridges the gap between technical SEO auditing and practical, client-ready deliverables, making it an invaluable tool for SEO professionals and digital marketing agencies.\n\n## Who is this for?\nThis workflow is designed for SEO consultants, digital marketing agencies, and content strategists who need to perform comprehensive content audits for clients or their own websites. It's particularly valuable for professionals who:\n\n1. Regularly conduct SEO audits as part of their service offerings\n2. Need to provide branded, professional reports to clients\n3. Want to automate the time-consuming process of content analysis\n4. Require data-driven insights to inform content strategy decisions\n\nUsers should have basic familiarity with SEO concepts and metrics, as well as a basic understanding of how to set up API credentials in n8n. \n\nWhile no coding knowledge is required to run the workflow, users should be comfortable with configuring workflow parameters and following setup instructions.\n\n## What problem is this workflow solving?\nContent audits are essential for SEO strategy but are traditionally labor-intensive and time-consuming. This workflow addresses several key challenges:\n\n1. **Manual Data Collection**: Gathering data from multiple sources (crawlers, Google Search Console, etc.) typically requires hours of work. This workflow automates the entire data collection process.\n\n2. **Inconsistent Analysis**: Manual audits can suffer from inconsistency in methodology. This workflow applies the same comprehensive analysis criteria to every page, ensuring thorough and consistent results.\n\n3. **Report Generation**: Creating professional, client-ready reports often requires additional design work after the analysis is complete. This workflow generates a fully branded HTML report automatically.\n\n4. **Data Integration**: Correlating technical SEO issues with actual search performance metrics is difficult when working with separate tools. This workflow seamlessly integrates crawl data with Google Search Console metrics.\n\n5. **Scale Limitations**: Manual audits become increasingly difficult with larger websites. This workflow can efficiently process up to 1,000 pages without additional effort.\n\n## What this workflow does\n### Overview\nThe Content SEO Audit Workflow crawls a specified website, analyzes its content for various SEO issues, retrieves performance data from Google Search Console, and generates a comprehensive HTML report. \n\nThe workflow identifies issues in five key categories: status issues (404 errors, redirects), content quality (thin content, readability), metadata SEO (title/description issues), internal linking (orphan pages, excessive click depth), and performance (underperforming content). \n\nThe final report includes executive summaries, detailed issue breakdowns, and actionable recommendations, all branded with your company's colors and logo.\n\n### Process\n1. **Initial Configuration**: The workflow begins by setting parameters including the target domain, crawl limits, company information, and branding colors.\n\n2. **Website Crawling**: The workflow creates a crawl task in DataForSEO and periodically checks its status until completion.\n\n3. **Data Collection**: Once crawling is complete, the workflow:\n   - Retrieves the raw audit data from DataForSEO\n   - Extracts all URLs with status code 200 (successful pages)\n   - Queries Google Search Console API for each URL to get clicks and impressions data\n   - Identifies 404 and 301 pages and retrieves their source links\n\n4. **Data Analysis**: The workflow analyzes the collected data to identify issues including:\n   - Technical issues: 404 errors, redirects, canonicalization problems\n   - Content issues: thin content, outdated content, readability problems\n   - SEO metadata issues: missing/duplicate titles and descriptions, H1 problems\n   - Internal linking issues: orphan pages, excessive click depth, low internal links\n   - Performance issues: underperforming pages based on GSC data\n\n5. **Report Generation**: Finally, the workflow:\n   - Calculates a health score based on the severity and quantity of issues\n   - Generates prioritized recommendations\n   - Creates a comprehensive HTML report with interactive tables and visualizations\n   - Customizes the report with your company's branding\n   - Provides the report as a downloadable HTML file\n\n## Setup\nTo set up this workflow, follow these steps:\n\n1. **Import the workflow**: Download the JSON file and import it into your n8n instance.\n\n2. **Configure DataForSEO credentials**:\n   - Create a DataForSEO account at https://app.dataforseo.com/api-access (they offer a free $1 credit for testing)\n   - Add a new \"Basic Auth\" credential in n8n following the [HTTP Request Authentication guide](https://docs.n8n.io/integrations/builtin/credentials/httprequest/)\n   - Assign this credential to the \"Create Task\", \"Check Task Status\", \"Get Raw Audit Data\", and \"Get Source URLs Data\" nodes\n\n3. **Configure Google Search Console credentials**:\n   - Add a new \"Google OAuth2 API\" credential following the [Google OAuth guide](https://docs.n8n.io/integrations/builtin/credentials/google/oauth-generic/)\n   - Ensure your Google account has access to the Google Search Console property you want to analyze\n   - Assign this credential to the \"Query GSC API\" node\n\n4. **Update the \"Set Fields\" node** with:\n   - dfs_domain: The website domain you want to audit\n   - dfs_max_crawl_pages: Maximum number of pages to crawl (default: 1000)\n   - dfs_enable_javascript: Whether to enable JavaScript rendering (default: false)\n   - company_name: Your company name for the report branding\n   - company_website: Your company website URL\n   - company_logo_url: URL to your company logo\n   - brand_primary_color: Your primary brand color (hex code)\n   - brand_secondary_color: Your secondary brand color (hex code)\n   - gsc_property_type: Set to \"domain\" or \"url\" depending on your Google Search Console property type\n\n5. **Run the workflow**: Click \"Start\" and wait for it to complete (approximately 20 minutes for 500 pages).\n\n6. **Download the report**: Once complete, download the HTML file from the \"Download Report\" node.\n\n## How to customize this workflow to your needs\nThis workflow can be adapted in several ways to better suit your specific requirements:\n\n1. **Adjust crawl parameters**: Modify the \"Set Fields\" node to change:\n   - The maximum number of pages to crawl (dfs_max_crawl_pages). This workflow supports up to 1000 pages.\n   - Whether to enable JavaScript rendering for JavaScript-heavy sites (dfs_enable_javascript)\n\n2. **Customize issue detection thresholds**: In the \"Build Report Structure\" code node, you can modify:\n   - Word count thresholds for thin content detection (currently 1500 words)\n   - Click depth thresholds (currently flags pages deeper than 4 clicks)\n   - Title and description length parameters (currently 40-60 chars for titles, 70-155 for descriptions)\n   - Readability score thresholds (currently flags Flesch-Kincaid scores below 55)\n\n3. **Modify the report design**: In the \"Generate HTML Report\" code node, you can:\n   - Adjust the HTML/CSS to change the report layout and styling\n   - Add or remove sections from the report\n   - Change the recommendations logic\n   - Modify the health score calculation algorithm\n\n4. **Add additional data sources**: You could extend the workflow by:\n   - Adding Pagespeed Insights data for performance metrics\n   - Incorporating backlink data from other APIs\n   - Adding keyword ranking data from rank tracking APIs\n\n5. **Implement automated delivery**: Add nodes after the \"Download Report\" to:\n   - Send the report directly to clients via email\n   - Upload it to cloud storage\n   - Create a PDF version of the report",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Set Fields",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "When clicking ‘Start’",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Check Task Status",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Create Task",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "If",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Wait",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Get RAW Audit Data",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Extract URLs",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Loop Over Items",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "Query GSC API",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Wait1",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Map GSC Data to URL",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Merge GSC Data with RAW Data",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Build Report Structure",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Generate HTML Report",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Extract 404 & 301",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Loop Over Items1",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "Map URLs Data",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Get Source URLs Data",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Download Report",
      "type": "n8n-nodes-base.convertToFile",
      "role": "convertToFile",
      "configDescription": "Version 1.1"
    }
  ]
}