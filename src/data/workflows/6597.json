{
  "id": 6597,
  "slug": "6597",
  "title": "Monetize your private LLM models with x402 & Ollama",
  "description": "*This workflow contains community nodes that are only compatible with the self-hosted version of n8n.*\n\n![x402ollama.png](fileId:1960)\n\n# Monetize Your Private LLM Models with x402 and Ollama\n\nSelf-hosting custom LLMs is becoming more popular and easier with turn-key inferencing tools like [Ollama](https://ollama.com/). With Ollama you can host your own proprietary models for customers in a private cloud or on your own hardware. \n\nBut monetizing custom-trained, propietary models is still a challenge, requiring integrations with payment processors like Stripe, which don't support micropayments for on-demand API consumption. \n\nWith this free workflow you can quickly monetize your proprietary LLM models with the [x402](https://x402.org) payment scheme in n8n with [1Shot API](https://1shotapi.com). \n\n## Setup Steps:\n\n1. Authenticate the 1Shot API node against your 1Shot API business account. \n2. Point the 1Shot API simulate and execute nodes at the x402-compatible payment token you'd like to receive as payment.\n3. Configure the Ollama n8n node in the workflow (with optional authentication) to forward to your Ollama API endpoint and let users query it through an n8n webhook endpoint while paying you directly in your preferred stablecoin (like [USDC](https://www.circle.com/usdc)). \n\nThrough x402, users and AI agents can pay per-inference, whith no overhead wasted on centralized payment processors. \n\n## Walkthrough Tutorial\n\nCheck out the [YouTube tutorial](https://youtu.be/YjdFS9VDOMo) for this workflow so see the full end-to-end process.",
  "featuredImage": "/data/workflows/6597/6597.webp",
  "author": {
    "id": 101,
    "slug": "oneshotapi",
    "name": "1Shot API",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "AI Chatbot"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 542,
  "downloads": 54,
  "createdAt": "2025-07-28T20:27:16.317Z",
  "updatedAt": "2026-01-16T08:45:53.441Z",
  "publishedAt": "2025-07-28T20:27:16.317Z",
  "nodes": 18,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/6597",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Monetize your private LLM models with x402 & Ollama",
    "workflowName": "Monetize your private LLM models with x402 & Ollama",
    "description": "*This workflow contains community nodes that are only compatible with the self-hosted version of n8n.*\n\n![x402ollama.png](fileId:1960)\n\n# Monetize Your Private LLM Models with x402 and Ollama\n\nSelf-hosting custom LLMs is becoming more popular and easier with turn-key inferencing tools like [Ollama](https://ollama.com/). With Ollama you can host your own proprietary models for customers in a private cloud or on your own hardware. \n\nBut monetizing custom-trained, propietary models is still a challenge, requiring integrations with payment processors like Stripe, which don't support micropayments for on-demand API consumption. \n\nWith this free workflow you can quickly monetize your proprietary LLM models with the [x402](https://x402.org) payment scheme in n8n with [1Shot API](https://1shotapi.com). \n\n## Setup Steps:\n\n1. Authenticate the 1Shot API node against your 1Shot API business account. \n2. Point the 1Shot API simulate and execute nodes at the x402-compatible payment token you'd like to receive as payment.\n3. Configure the Ollama n8n node in the workflow (with optional authentication) to forward to your Ollama API endpoint and let users query it through an n8n webhook endpoint while paying you directly in your preferred stablecoin (like [USDC](https://www.circle.com/usdc)). \n\nThrough x402, users and AI agents can pay per-inference, whith no overhead wasted on centralized payment processors. \n\n## Walkthrough Tutorial\n\nCheck out the [YouTube tutorial](https://youtu.be/YjdFS9VDOMo) for this workflow so see the full end-to-end process.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "role": "webhook",
      "configDescription": "Version 2"
    },
    {
      "name": "Check for presence of X-HEADER",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Decode & Validate X-Payment",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Simulate Payment",
      "type": "n8n-nodes-1shot.oneShot",
      "role": "oneShot",
      "configDescription": "Version 1"
    },
    {
      "name": "On Successful Payment Simulation",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "1Shot API Submit & Wait",
      "type": "n8n-nodes-1shot.oneShotSynch",
      "role": "oneShotSynch",
      "configDescription": "Version 1"
    },
    {
      "name": "Ensure Well Formatted Payment Payload",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Response: Missing or Invalid Payment Headers",
      "type": "n8n-nodes-base.respondToWebhook",
      "role": "respondToWebhook",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Response: Payment Invalid",
      "type": "n8n-nodes-base.respondToWebhook",
      "role": "respondToWebhook",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Response: 200 - Payment Successful",
      "type": "n8n-nodes-base.respondToWebhook",
      "role": "respondToWebhook",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Private Model Inference",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "role": "chainLlm",
      "configDescription": "Version 1.7"
    },
    {
      "name": "Ollama Engine",
      "type": "@n8n/n8n-nodes-langchain.lmOllama",
      "role": "lmOllama",
      "configDescription": "Version 1"
    }
  ]
}