{
  "id": 5028,
  "slug": "5028",
  "title": "Basic RAG chat",
  "description": "This workflow demonstrates a simple Retrieval-Augmented Generation (RAG) pipeline in n8n, split into two main sections:\n\nüîπ Part 1: Load Data into Vector Store\nReads files from disk (or Google Drive).\n\nSplits content into manageable chunks using a recursive text splitter.\n\nGenerates embeddings using the Cohere Embedding API.\n\nStores the vectors into an In-Memory Vector Store (for simplicity; can be replaced with Pinecone, Qdrant, etc.).\n\nüîπ Part 2: Chat with the Vector Store\nTakes user input from a chat UI or trigger node.\n\nEmbeds the query using the same Cohere embedding model.\n\nRetrieves similar chunks from the vector store via similarity search.\n\nUses Groq-hosted LLM to generate a final answer based on the context.\n\nüõ†Ô∏è Technologies Used:\nüì¶ Cohere Embedding API\n\n‚ö° Groq LLM for fast inference\n\nüß† n8n for orchestrating and visualizing the flow\n\nüß≤ In-Memory Vector Store (for prototyping)\n\nüß™ Usage:\nUpload or point to your source documents.\n\nEmbed them and populate the vector store.\n\nAsk questions through the chat trigger node.\n\nReceive context-aware responses based on retrieved content.\n\n",
  "featuredImage": "/data/workflows/5028/5028.webp",
  "author": {
    "id": 101,
    "slug": "justin",
    "name": "JustinLee",
    "avatar": ""
  },
  "categories": [
    "Internal Wiki",
    "AI RAG"
  ],
  "complexityLevel": "intermediate",
  "price": 0,
  "visitors": 4861,
  "downloads": 486,
  "createdAt": "2025-06-18T16:55:03.909Z",
  "updatedAt": "2026-01-16T08:37:16.425Z",
  "publishedAt": "2025-06-18T16:55:03.909Z",
  "nodes": 14,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/5028",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Basic RAG chat",
    "workflowName": "Basic RAG chat",
    "description": "This workflow demonstrates a simple Retrieval-Augmented Generation (RAG) pipeline in n8n, split into two main sections:\n\nüîπ Part 1: Load Data into Vector Store\nReads files from disk (or Google Drive).\n\nSplits content into manageable chunks using a recursive text splitter.\n\nGenerates embeddings using the Cohere Embedding API.\n\nStores the vectors into an In-Memory Vector Store (for simplicity; can be replaced with Pinecone, Qdrant, etc.).\n\nüîπ Part 2: Chat with the Vector Store\nTakes user input from a chat UI or trigger node.\n\nEmbeds the query using the same Cohere embedding model.\n\nRetrieves similar chunks from the vector store via similarity search.\n\nUses Groq-hosted LLM to generate a final answer based on the context.\n\nüõ†Ô∏è Technologies Used:\nüì¶ Cohere Embedding API\n\n‚ö° Groq LLM for fast inference\n\nüß† n8n for orchestrating and visualizing the flow\n\nüß≤ In-Memory Vector Store (for prototyping)\n\nüß™ Usage:\nUpload or point to your source documents.\n\nEmbed them and populate the vector store.\n\nAsk questions through the chat trigger node.\n\nReceive context-aware responses based on retrieved content.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Recursive Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
      "role": "textSplitterRecursiveCharacterTextSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1"
    },
    {
      "name": "Question and Answer Chain",
      "type": "@n8n/n8n-nodes-langchain.chainRetrievalQa",
      "role": "chainRetrievalQa",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Vector Store Retriever",
      "type": "@n8n/n8n-nodes-langchain.retrieverVectorStore",
      "role": "retrieverVectorStore",
      "configDescription": "Version 1"
    },
    {
      "name": "When clicking 'Test Workflow' button",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "When clicking 'Chat' button below",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Read/Write Files from Disk",
      "type": "n8n-nodes-base.readWriteFile",
      "role": "readWriteFile",
      "configDescription": "Version 1"
    },
    {
      "name": "In-Memory Vector Store1",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreInMemory",
      "role": "vectorStoreInMemory",
      "configDescription": "Version 1"
    },
    {
      "name": "In-Memory Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreInMemory",
      "role": "vectorStoreInMemory",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings Cohere",
      "type": "@n8n/n8n-nodes-langchain.embeddingsCohere",
      "role": "embeddingsCohere",
      "configDescription": "Version 1"
    },
    {
      "name": "Groq Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatGroq",
      "role": "lmChatGroq",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings Cohere1",
      "type": "@n8n/n8n-nodes-langchain.embeddingsCohere",
      "role": "embeddingsCohere",
      "configDescription": "Version 1"
    }
  ]
}