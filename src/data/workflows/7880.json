{
  "id": 7880,
  "slug": "7880",
  "title": "Process Thai documents with TyphoonOCR & AI to Google Sheets (multi-page PDF)",
  "description": "#### ⚠️ Note: This template requires a community node and works only on self-hosted n8n installations. It uses the Typhoon OCR Python package, `pdfseparate` from poppler-utils, and custom command execution. Make sure to install all required dependencies locally.\n\n---\n\n## Who is this for?\n\nThis template is designed for developers, back-office teams, and automation builders (especially in Thailand or Thai-speaking environments) who need to process **multi-file, multi-page Thai PDFs** and automatically export structured results to Google Sheets.\n\nIt is ideal for:\n\n- Government and enterprise document processing\n- Thai-language invoices, memos, and official letters\n- AI-powered automation pipelines that require Thai OCR\n\n---\n\n## What problem does this solve?\n\nTyphoon OCR is one of the most accurate OCR tools for Thai text, but integrating it into an end-to-end workflow usually requires manual scripting and handling multi-page PDFs. This template solves that by:\n\n- Splitting PDFs into individual pages\n- Running Typhoon OCR on each page\n- Aggregating text back into a single file\n- Using AI to extract structured fields\n- Automatically saving structured data into Google Sheets\n\n---\n\n## What this workflow does\n![wf22.gif](fileId:2196)\n\n- **Trigger:** Manual execution or any n8n trigger node\n- **Load Files:** Read PDFs from a local `doc/multipage` folder\n- **Split PDF Pages:** Use `pdfinfo` and `pdfseparate` to break PDFs into pages\n- **Typhoon OCR:** Run OCR on each page via Execute Command\n- **Aggregate:** Combine per-page OCR text\n- **LLM Extraction:** Use AI (e.g., GPT-4, OpenRouter) to extract fields into JSON\n- **Parse JSON:** Convert structured JSON into a tabular format\n- **Google Sheets:** Append one row per file into a Google Sheet\n- **Cleanup:** Delete temp split pages and move processed PDFs into a Completed folder\n\n---\n\n## Setup\n\n1. **Install Requirements**\n\n   - Python 3.10+\n   - `typhoon-ocr`: `pip install typhoon-ocr`\n   - poppler-utils: provides `pdfinfo`, `pdfseparate`\n   - qpdf: backup page counting\n\n2. **Create folders**\n\n   - `/doc/multipage` for incoming files\n   - `/doc/tmp` for split pages\n   - `/doc/multipage/Completed` for processed files\n\n3. **Google Sheet**\n\n   - Create a Google Sheet with column headers like:\n     ```\n     book_id | date | subject | to | attach | detail | signed_by | signed_by2 | contact_phone | contact_email | contact_fax | download_url\n     ```\n\n4. **API Keys**\n\n   - Export your `TYPHOON_OCR_API_KEY` and `OPENAI_API_KEY` (or use credentials in n8n)\n\n---\n\n## How to customize this workflow\n\n- Replace the LLM provider in the “Structure Text to JSON with LLM” node (supports OpenRouter, OpenAI, etc.)\n- Adjust the JSON schema and parsing logic to match your documents\n- Update Google Sheets mapping to fit your desired fields\n- Add trigger nodes (Dropbox, Google Drive, Webhook) to automate file ingestion\n\n---\n\n## About Typhoon OCR\n\nTyphoon is a multilingual LLM and NLP toolkit optimized for Thai. It includes `typhoon-ocr`, a Python OCR package designed for Thai-centric documents. It is open-source, highly accurate, and works well in automation pipelines. Perfect for government paperwork, PDF reports, and multi-language documents in Southeast Asia.\n\n---\n\n## Deployment Option\n\nYou can also deploy this workflow easily using the Docker image provided in my GitHub repository: [https://github.com/Jaruphat/n8n-ffmpeg-typhoon-ollama](https://github.com/Jaruphat/n8n-ffmpeg-typhoon-ollama)\n\nThis Docker setup already includes n8n, ffmpeg, Typhoon OCR, and Ollama combined together, so you can run the whole environment without installing each dependency manually.\n\n",
  "featuredImage": "/data/workflows/7880/7880.webp",
  "author": {
    "id": 101,
    "slug": "jaruphatj",
    "name": "Jaruphat J.",
    "avatar": ""
  },
  "categories": [
    "AI Summarization",
    "Multimodal AI"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 456,
  "downloads": 45,
  "createdAt": "2025-08-26T06:28:58.606Z",
  "updatedAt": "2026-01-16T08:52:48.993Z",
  "publishedAt": "2025-08-26T06:28:58.606Z",
  "nodes": 15,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/7880",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Process Thai documents with TyphoonOCR & AI to Google Sheets (multi-page PDF)",
    "workflowName": "Process Thai documents with TyphoonOCR & AI to Google Sheets (multi-page PDF)",
    "description": "#### ⚠️ Note: This template requires a community node and works only on self-hosted n8n installations. It uses the Typhoon OCR Python package, `pdfseparate` from poppler-utils, and custom command execution. Make sure to install all required dependencies locally.\n\n---\n\n## Who is this for?\n\nThis template is designed for developers, back-office teams, and automation builders (especially in Thailand or Thai-speaking environments) who need to process **multi-file, multi-page Thai PDFs** and automatically export structured results to Google Sheets.\n\nIt is ideal for:\n\n- Government and enterprise document processing\n- Thai-language invoices, memos, and official letters\n- AI-powered automation pipelines that require Thai OCR\n\n---\n\n## What problem does this solve?\n\nTyphoon OCR is one of the most accurate OCR tools for Thai text, but integrating it into an end-to-end workflow usually requires manual scripting and handling multi-page PDFs. This template solves that by:\n\n- Splitting PDFs into individual pages\n- Running Typhoon OCR on each page\n- Aggregating text back into a single file\n- Using AI to extract structured fields\n- Automatically saving structured data into Google Sheets\n\n---\n\n## What this workflow does\n![wf22.gif](fileId:2196)\n\n- **Trigger:** Manual execution or any n8n trigger node\n- **Load Files:** Read PDFs from a local `doc/multipage` folder\n- **Split PDF Pages:** Use `pdfinfo` and `pdfseparate` to break PDFs into pages\n- **Typhoon OCR:** Run OCR on each page via Execute Command\n- **Aggregate:** Combine per-page OCR text\n- **LLM Extraction:** Use AI (e.g., GPT-4, OpenRouter) to extract fields into JSON\n- **Parse JSON:** Convert structured JSON into a tabular format\n- **Google Sheets:** Append one row per file into a Google Sheet\n- **Cleanup:** Delete temp split pages and move processed PDFs into a Completed folder\n\n---\n\n## Setup\n\n1. **Install Requirements**\n\n   - Python 3.10+\n   - `typhoon-ocr`: `pip install typhoon-ocr`\n   - poppler-utils: provides `pdfinfo`, `pdfseparate`\n   - qpdf: backup page counting\n\n2. **Create folders**\n\n   - `/doc/multipage` for incoming files\n   - `/doc/tmp` for split pages\n   - `/doc/multipage/Completed` for processed files\n\n3. **Google Sheet**\n\n   - Create a Google Sheet with column headers like:\n     ```\n     book_id | date | subject | to | attach | detail | signed_by | signed_by2 | contact_phone | contact_email | contact_fax | download_url\n     ```\n\n4. **API Keys**\n\n   - Export your `TYPHOON_OCR_API_KEY` and `OPENAI_API_KEY` (or use credentials in n8n)\n\n---\n\n## How to customize this workflow\n\n- Replace the LLM provider in the “Structure Text to JSON with LLM” node (supports OpenRouter, OpenAI, etc.)\n- Adjust the JSON schema and parsing logic to match your documents\n- Update Google Sheets mapping to fit your desired fields\n- Add trigger nodes (Dropbox, Google Drive, Webhook) to automate file ingestion\n\n---\n\n## About Typhoon OCR\n\nTyphoon is a multilingual LLM and NLP toolkit optimized for Thai. It includes `typhoon-ocr`, a Python OCR package designed for Thai-centric documents. It is open-source, highly accurate, and works well in automation pipelines. Perfect for government paperwork, PDF reports, and multi-language documents in Southeast Asia.\n\n---\n\n## Deployment Option\n\nYou can also deploy this workflow easily using the Docker image provided in my GitHub repository: [https://github.com/Jaruphat/n8n-ffmpeg-typhoon-ollama](https://github.com/Jaruphat/n8n-ffmpeg-typhoon-ollama)\n\nThis Docker setup already includes n8n, ffmpeg, Typhoon OCR, and Ollama combined together, so you can run the whole environment without installing each dependency manually.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When clicking ‘Test workflow’",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Extract Text with Typhoon OCR",
      "type": "n8n-nodes-base.executeCommand",
      "role": "executeCommand",
      "configDescription": "Version 1"
    },
    {
      "name": "Load PDFs from doc Folder",
      "type": "n8n-nodes-base.readWriteFile",
      "role": "readWriteFile",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Read/Write Files from Disk",
      "type": "n8n-nodes-base.readWriteFile",
      "role": "readWriteFile",
      "configDescription": "Version 1"
    },
    {
      "name": "Set_Input_Path",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Split PDF page",
      "type": "n8n-nodes-base.executeCommand",
      "role": "executeCommand",
      "configDescription": "Version 1"
    },
    {
      "name": "Read Splite PDF Page",
      "type": "n8n-nodes-base.readWriteFile",
      "role": "readWriteFile",
      "configDescription": "Version 1"
    },
    {
      "name": "Aggregate",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "Loop Over Items",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "Parse JSON to Sheet Format",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Structure Text to JSON with LLM",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "role": "chainLlm",
      "configDescription": "Version 1.6"
    },
    {
      "name": "OpenRouter Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "role": "lmChatOpenRouter",
      "configDescription": "Version 1"
    },
    {
      "name": "Clear tmp files",
      "type": "n8n-nodes-base.executeCommand",
      "role": "executeCommand",
      "configDescription": "Version 1"
    },
    {
      "name": "Save to Google Sheet",
      "type": "n8n-nodes-base.googleSheets",
      "role": "googleSheets",
      "configDescription": "Version 4.5"
    }
  ]
}