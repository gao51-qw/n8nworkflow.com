{
  "id": 7171,
  "slug": "7171",
  "title": "Build a GLPI knowledge base RAG pipeline with Google Gemini and PostgreSQL",
  "description": "## Description\nThis workflow automates the creation of a Retrieval-Augmented Generation (RAG) pipeline using content from the GLPI Knowledge Base. It retrieves and processes FAQ articles directly via the GLPI API, cleans and vectorizes the content using pgvector in PostgreSQL, and prepares the data for use by LLM-powered AI agents.\n\n\n\n## What Problem Does This Solve?\nManually building a RAG pipeline from a GLPI knowledge base requires integrating multiple tools, cleaning data, and managing embeddings—tasks that are often complex and repetitive. This subworkflow simplifies the entire process by automating data retrieval, transformation, and vector storage, allowing you to focus on building intelligent support agents or chatbots powered by your internal documentation.\n\n## Features\nConnects to GLPI via API to fetch FAQ articles\n\nCleans and normalizes content for better embedding quality\n\nGenerates vector embeddings using Google Gemini (or another model)\n\nStores embeddings in a PostgreSQL database with pgvector\n\nFully modular: easily integrate with any RAG-ready LLM pipeline\n\n## Prerequisites\nBefore using this subworkflow, make sure you have:\n\nA GLPI instance installed on a Linux server with API access enabled\n\nA PostgreSQL database with the pgvector extension installed\n\nAn OpenAI API key (or alternative embedding provider)\n\nn8n instance (self-hosted or cloud)\n\n## Suggested Usage\nThis subworkflow is intended to be part of a larger AI pipeline. Attach it to a scheduled workflow (e.g. daily sync) or use it in response to updates in your GLPI base. Ideal for internal support bots, IT documentation assistants, and help desk AI agents that rely on up-to-date knowledge.",
  "featuredImage": "/data/workflows/7171/7171.webp",
  "author": {
    "id": 101,
    "slug": "thiagovazzoler",
    "name": "Thiago Vazzoler Loureiro",
    "avatar": ""
  },
  "categories": [
    "Internal Wiki",
    "Multimodal AI"
  ],
  "complexityLevel": "intermediate",
  "price": 0,
  "visitors": 244,
  "downloads": 24,
  "createdAt": "2025-08-08T12:12:26.153Z",
  "updatedAt": "2026-01-16T08:48:49.739Z",
  "publishedAt": "2025-08-08T12:12:26.153Z",
  "nodes": 9,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/7171",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Build a GLPI knowledge base RAG pipeline with Google Gemini and PostgreSQL",
    "workflowName": "Build a GLPI knowledge base RAG pipeline with Google Gemini and PostgreSQL",
    "description": "## Description\nThis workflow automates the creation of a Retrieval-Augmented Generation (RAG) pipeline using content from the GLPI Knowledge Base. It retrieves and processes FAQ articles directly via the GLPI API, cleans and vectorizes the content using pgvector in PostgreSQL, and prepares the data for use by LLM-powered AI agents.\n\n\n\n## What Problem Does This Solve?\nManually building a RAG pipeline from a GLPI knowledge base requires integrating multiple tools, cleaning data, and managing embeddings—tasks that are often complex and repetitive. This subworkflow simplifies the entire process by automating data retrieval, transformation, and vector storage, allowing you to focus on building intelligent support agents or chatbots powered by your internal documentation.\n\n## Features\nConnects to GLPI via API to fetch FAQ articles\n\nCleans and normalizes content for better embedding quality\n\nGenerates vector embeddings using Google Gemini (or another model)\n\nStores embeddings in a PostgreSQL database with pgvector\n\nFully modular: easily integrate with any RAG-ready LLM pipeline\n\n## Prerequisites\nBefore using this subworkflow, make sure you have:\n\nA GLPI instance installed on a Linux server with API access enabled\n\nA PostgreSQL database with the pgvector extension installed\n\nAn OpenAI API key (or alternative embedding provider)\n\nn8n instance (self-hosted or cloud)\n\n## Suggested Usage\nThis subworkflow is intended to be part of a larger AI pipeline. Attach it to a scheduled workflow (e.g. daily sync) or use it in response to updates in your GLPI base. Ideal for internal support bots, IT documentation assistants, and help desk AI agents that rely on up-to-date knowledge.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.8"
    },
    {
      "name": "Google Gemini Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Embeddings Google Gemini1",
      "type": "@n8n/n8n-nodes-langchain.embeddingsGoogleGemini",
      "role": "embeddingsGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings Google Gemini3",
      "type": "@n8n/n8n-nodes-langchain.embeddingsGoogleGemini",
      "role": "embeddingsGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "CONHECIMENTO_TI_GLPI",
      "type": "@n8n/n8n-nodes-langchain.vectorStorePGVector",
      "role": "vectorStorePGVector",
      "configDescription": "Version 1.1"
    },
    {
      "name": "CONFLUENCE_TI_CONFLUENCE_SGU_GPL",
      "type": "@n8n/n8n-nodes-langchain.vectorStorePGVector",
      "role": "vectorStorePGVector",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Simple Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    }
  ]
}