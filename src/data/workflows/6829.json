{
  "id": 6829,
  "slug": "6829",
  "title": "Build persistent chat memory with GPT-4o-mini and Qdrant vector database",
  "description": "## üß† Long-Term Memory System for AI Agents with Vector Database\n\nTransform your AI assistants into intelligent agents with persistent memory capabilities. This production-ready workflow implements a sophisticated long-term memory system using vector databases, enabling AI agents to remember conversations, user preferences, and contextual information across unlimited sessions.\n\n### üéØ What This Template Does\n\nThis workflow creates an AI assistant that never forgets. Unlike traditional chatbots that lose context after each session, this implementation uses vector database technology to store and retrieve conversation history semantically, providing truly persistent memory for your AI agents.\n\n### üîë Key Features\n\n- **Persistent Context Storage**: Automatically stores all conversations in a vector database for permanent retrieval\n- **Semantic Memory Search**: Uses advanced embedding models to find relevant past interactions based on meaning, not just keywords\n- **Intelligent Reranking**: Employs Cohere's reranking model to ensure the most relevant memories are used for context\n- **Structured Data Management**: Formats and stores conversations with metadata for optimal retrieval\n- **Scalable Architecture**: Handles unlimited conversations and users with consistent performance\n- **No Context Window Limitations**: Effectively bypasses LLM token limits through intelligent retrieval\n\n### üí° Use Cases\n\n- **Customer Support Bots**: Remember customer history, preferences, and previous issues\n- **Personal AI Assistants**: Maintain user preferences and conversation continuity over months or years\n- **Knowledge Management Systems**: Build accumulated knowledge bases from user interactions\n- **Educational Tutors**: Track student progress and adapt teaching based on history\n- **Enterprise Chatbots**: Maintain context across departments and long-term projects\n\n### üõ†Ô∏è How It Works\n\n1. **User Input**: Receives messages through n8n's chat interface\n2. **Memory Retrieval**: Searches vector database for relevant past conversations\n3. **Context Integration**: AI agent uses retrieved memories to generate contextual responses\n4. **Response Generation**: Creates informed responses based on historical context\n5. **Memory Storage**: Stores new conversation data for future retrieval\n\n### üìã Requirements\n\n- **OpenAI API Key**: For embeddings and chat completions\n- **Qdrant Instance**: Cloud or self-hosted vector database\n- **Cohere API Key**: Optional, for enhanced retrieval accuracy\n- **n8n Instance**: Version 1.0+ with LangChain nodes\n\n### üöÄ Quick Setup\n\n1. Import this workflow into your n8n instance\n2. Configure credentials for OpenAI, Qdrant, and Cohere\n3. Create a Qdrant collection named 'ltm' with 1024 dimensions\n4. Activate the workflow and start chatting!\n\n### üìä Performance Metrics\n\n- **Response Time**: 2-3 seconds average\n- **Memory Recall Accuracy**: 95%+\n- **Token Usage**: 50-70% reduction compared to full context inclusion\n- **Scalability**: Tested with 100k+ stored conversations\n\n### üí∞ Cost Optimization\n\n- Uses GPT-4o-mini for optimal cost/performance balance\n- Implements efficient chunking strategies to minimize embedding costs\n- Reranking can be disabled to save on Cohere API costs\n- Average cost: ~$0.01 per conversation\n\n### üìñ Learn More\n\nFor a detailed explanation of the architecture and implementation details, check out the comprehensive guide: [Long-Term Memory for LLMs using Vector Store - A Practical Approach with n8n and Qdrant](https://dev.to/einarcesar/long-term-memory-for-llms-using-vector-store-a-practical-approach-with-n8n-and-qdrant-2ha7)\n\n### ü§ù Support\n\n- **Documentation**: Full setup guide in the article above\n- **Community**: Share your experiences and get help in n8n community forums\n- **Issues**: Report bugs or request features on the workflow page\n\n---\n\n**Tags**: #AI #LangChain #VectorDatabase #LongTermMemory #RAG #OpenAI #Qdrant #ChatBot #MemorySystem #ArtificialIntelligence",
  "featuredImage": "/data/workflows/6829/6829.webp",
  "author": {
    "id": 101,
    "slug": "einarcesar",
    "name": "Einar C√©sar Santos",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 1467,
  "downloads": 146,
  "createdAt": "2025-08-02T00:28:33.640Z",
  "updatedAt": "2026-01-16T08:47:14.327Z",
  "publishedAt": "2025-08-02T00:28:33.640Z",
  "nodes": 25,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/6829",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Build persistent chat memory with GPT-4o-mini and Qdrant vector database",
    "workflowName": "Build persistent chat memory with GPT-4o-mini and Qdrant vector database",
    "description": "## üß† Long-Term Memory System for AI Agents with Vector Database\n\nTransform your AI assistants into intelligent agents with persistent memory capabilities. This production-ready workflow implements a sophisticated long-term memory system using vector databases, enabling AI agents to remember conversations, user preferences, and contextual information across unlimited sessions.\n\n### üéØ What This Template Does\n\nThis workflow creates an AI assistant that never forgets. Unlike traditional chatbots that lose context after each session, this implementation uses vector database technology to store and retrieve conversation history semantically, providing truly persistent memory for your AI agents.\n\n### üîë Key Features\n\n- **Persistent Context Storage**: Automatically stores all conversations in a vector database for permanent retrieval\n- **Semantic Memory Search**: Uses advanced embedding models to find relevant past interactions based on meaning, not just keywords\n- **Intelligent Reranking**: Employs Cohere's reranking model to ensure the most relevant memories are used for context\n- **Structured Data Management**: Formats and stores conversations with metadata for optimal retrieval\n- **Scalable Architecture**: Handles unlimited conversations and users with consistent performance\n- **No Context Window Limitations**: Effectively bypasses LLM token limits through intelligent retrieval\n\n### üí° Use Cases\n\n- **Customer Support Bots**: Remember customer history, preferences, and previous issues\n- **Personal AI Assistants**: Maintain user preferences and conversation continuity over months or years\n- **Knowledge Management Systems**: Build accumulated knowledge bases from user interactions\n- **Educational Tutors**: Track student progress and adapt teaching based on history\n- **Enterprise Chatbots**: Maintain context across departments and long-term projects\n\n### üõ†Ô∏è How It Works\n\n1. **User Input**: Receives messages through n8n's chat interface\n2. **Memory Retrieval**: Searches vector database for relevant past conversations\n3. **Context Integration**: AI agent uses retrieved memories to generate contextual responses\n4. **Response Generation**: Creates informed responses based on historical context\n5. **Memory Storage**: Stores new conversation data for future retrieval\n\n### üìã Requirements\n\n- **OpenAI API Key**: For embeddings and chat completions\n- **Qdrant Instance**: Cloud or self-hosted vector database\n- **Cohere API Key**: Optional, for enhanced retrieval accuracy\n- **n8n Instance**: Version 1.0+ with LangChain nodes\n\n### üöÄ Quick Setup\n\n1. Import this workflow into your n8n instance\n2. Configure credentials for OpenAI, Qdrant, and Cohere\n3. Create a Qdrant collection named 'ltm' with 1024 dimensions\n4. Activate the workflow and start chatting!\n\n### üìä Performance Metrics\n\n- **Response Time**: 2-3 seconds average\n- **Memory Recall Accuracy**: 95%+\n- **Token Usage**: 50-70% reduction compared to full context inclusion\n- **Scalability**: Tested with 100k+ stored conversations\n\n### üí∞ Cost Optimization\n\n- Uses GPT-4o-mini for optimal cost/performance balance\n- Implements efficient chunking strategies to minimize embedding costs\n- Reranking can be disabled to save on Cohere API costs\n- Average cost: ~$0.01 per conversation\n\n### üìñ Learn More\n\nFor a detailed explanation of the architecture and implementation details, check out the comprehensive guide: [Long-Term Memory for LLMs using Vector Store - A Practical Approach with n8n and Qdrant](https://dev.to/einarcesar/long-term-memory-for-llms-using-vector-store-a-practical-approach-with-n8n-and-qdrant-2ha7)\n\n### ü§ù Support\n\n- **Documentation**: Full setup guide in the article above\n- **Community**: Share your experiences and get help in n8n community forums\n- **Issues**: Report bugs or request features on the workflow page\n\n---\n\n**Tags**: #AI #LangChain #VectorDatabase #LongTermMemory #RAG #OpenAI #Qdrant #ChatBot #MemorySystem #ArtificialIntelligence",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Embeddings OpenAI",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note 2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Recursive Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
      "role": "textSplitterRecursiveCharacterTextSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note 3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Sticky Note 4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings for Retrieval",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sticky Note 5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Reranker Cohere",
      "type": "@n8n/n8n-nodes-langchain.rerankerCohere",
      "role": "rerankerCohere",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note 6",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "RAG_MEMORY",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreQdrant",
      "role": "vectorStoreQdrant",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sticky Note 7",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Structured Output Parser",
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "role": "outputParserStructured",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Sticky Note 8",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Format Response",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Sticky Note 9",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 2"
    },
    {
      "name": "Sticky Note 10",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Store Conversation",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreQdrant",
      "role": "vectorStoreQdrant",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sticky Note 11",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "GPT-4o-mini (Main)",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Workflow Overview",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}