{
  "id": 9594,
  "slug": "9594",
  "title": "Web crawler: Convert websites to AI-ready markdown in Google Sheets",
  "description": "Transform any website into a structured knowledge repository with this intelligent crawler that extracts hyperlinks from the homepage, intelligently filters images and content pages, and aggregates full Markdown-formatted content‚Äîperfect for fueling AI agents or building comprehensive company dossiers without manual effort.\n\n## üìã What This Template Does\nThis advanced workflow acts as a lightweight web crawler: it scrapes the homepage to discover all internal links (mimicking a sitemap extraction), deduplicates and validates them, separates image assets from textual pages, then fetches and converts non-image page content to clean Markdown. Results are seamlessly appended to Google Sheets for easy analysis, export, or integration into vector databases.\n- Automatically discovers and processes subpage links from the homepage\n- Filters out duplicates and non-HTTP links for efficient crawling\n- Converts scraped content to Markdown for AI-ready formatting\n- Categorizes and stores images, links, and full content in a single sheet row per site\n\n## üîß Prerequisites\n- Google account with Sheets access for data storage\n- n8n instance (cloud or self-hosted)\n- Basic understanding of URLs and web links\n\n## üîë Required Credentials\n\n### Google Sheets OAuth2 API Setup\n1. Go to console.cloud.google.com ‚Üí APIs & Services ‚Üí Credentials\n2. Click \"Create Credentials\" ‚Üí Select \"OAuth client ID\" ‚Üí Choose \"Web application\"\n3. Add authorized redirect URIs: https://your-n8n-instance.com/rest/oauth2-credential/callback (replace with your n8n URL)\n4. Download the client ID and secret, then add to n8n as \"Google Sheets OAuth2 API\" credential type\n5. During setup, grant access to Google Sheets scopes (e.g., spreadsheets) and test the connection by listing a sheet\n\n## ‚öôÔ∏è Configuration Steps\n1. Import the workflow JSON into your n8n instance\n2. In the \"Set Website\" node, update the `website_url` value to your target site (e.g., https://example.com)\n3. Assign your Google Sheets credential to the three \"Add ... to Sheet\" nodes\n4. Update the `documentId` and `sheetName` in those nodes to your target spreadsheet ID and sheet name/ID\n5. Ensure your sheet has columns: \"Website\", \"Links\", \"Scraped Content\", \"Images\"\n6. Activate the workflow and trigger manually to test scraping\n\n## üéØ Use Cases\n- Knowledge base creation: Crawl a company's site to aggregate all content into Sheets, then export to Notion or a vector DB for internal wikis\n- AI agent training: Extract structured Markdown from industry sites to fine-tune LLMs on domain-specific data like legal docs or tech blogs\n- Competitor intelligence: Build dossiers by crawling rival websites, separating assets and text for SEO audits or market analysis\n- Content archiving: Preserve dynamic sites (e.g., news portals) as static knowledge dumps for compliance or historical research\n\n## ‚ö†Ô∏è Troubleshooting\n- No links extracted: Verify the homepage has <a> tags; test with a simple site like example.com and check HTTP response in executions\n- Sheet update fails: Confirm column names match exactly (case-sensitive) and credential has edit permissions; try a new blank sheet\n- Content truncated: Google Sheets limits cells to ~50k chars‚Äîadjust the `.slice(0, 50000)` in \"Add Scraped Content to Sheet\" or split into multiple rows\n- Rate limiting errors: Add a \"Wait\" node after \"Scrape Links\" with 1-2s delay if the site blocks rapid requests",
  "featuredImage": "/data/workflows/9594/9594.webp",
  "author": {
    "id": 101,
    "slug": "daniel-automates",
    "name": "Daniel Nkencho",
    "avatar": ""
  },
  "categories": [
    "Document Extraction",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 985,
  "downloads": 98,
  "createdAt": "2025-10-13T23:04:49.201Z",
  "updatedAt": "2026-01-16T09:01:06.285Z",
  "publishedAt": "2025-10-13T23:04:49.201Z",
  "nodes": 22,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/9594",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Web crawler: Convert websites to AI-ready markdown in Google Sheets",
    "workflowName": "Web crawler: Convert websites to AI-ready markdown in Google Sheets",
    "description": "Transform any website into a structured knowledge repository with this intelligent crawler that extracts hyperlinks from the homepage, intelligently filters images and content pages, and aggregates full Markdown-formatted content‚Äîperfect for fueling AI agents or building comprehensive company dossiers without manual effort.\n\n## üìã What This Template Does\nThis advanced workflow acts as a lightweight web crawler: it scrapes the homepage to discover all internal links (mimicking a sitemap extraction), deduplicates and validates them, separates image assets from textual pages, then fetches and converts non-image page content to clean Markdown. Results are seamlessly appended to Google Sheets for easy analysis, export, or integration into vector databases.\n- Automatically discovers and processes subpage links from the homepage\n- Filters out duplicates and non-HTTP links for efficient crawling\n- Converts scraped content to Markdown for AI-ready formatting\n- Categorizes and stores images, links, and full content in a single sheet row per site\n\n## üîß Prerequisites\n- Google account with Sheets access for data storage\n- n8n instance (cloud or self-hosted)\n- Basic understanding of URLs and web links\n\n## üîë Required Credentials\n\n### Google Sheets OAuth2 API Setup\n1. Go to console.cloud.google.com ‚Üí APIs & Services ‚Üí Credentials\n2. Click \"Create Credentials\" ‚Üí Select \"OAuth client ID\" ‚Üí Choose \"Web application\"\n3. Add authorized redirect URIs: https://your-n8n-instance.com/rest/oauth2-credential/callback (replace with your n8n URL)\n4. Download the client ID and secret, then add to n8n as \"Google Sheets OAuth2 API\" credential type\n5. During setup, grant access to Google Sheets scopes (e.g., spreadsheets) and test the connection by listing a sheet\n\n## ‚öôÔ∏è Configuration Steps\n1. Import the workflow JSON into your n8n instance\n2. In the \"Set Website\" node, update the `website_url` value to your target site (e.g., https://example.com)\n3. Assign your Google Sheets credential to the three \"Add ... to Sheet\" nodes\n4. Update the `documentId` and `sheetName` in those nodes to your target spreadsheet ID and sheet name/ID\n5. Ensure your sheet has columns: \"Website\", \"Links\", \"Scraped Content\", \"Images\"\n6. Activate the workflow and trigger manually to test scraping\n\n## üéØ Use Cases\n- Knowledge base creation: Crawl a company's site to aggregate all content into Sheets, then export to Notion or a vector DB for internal wikis\n- AI agent training: Extract structured Markdown from industry sites to fine-tune LLMs on domain-specific data like legal docs or tech blogs\n- Competitor intelligence: Build dossiers by crawling rival websites, separating assets and text for SEO audits or market analysis\n- Content archiving: Preserve dynamic sites (e.g., news portals) as static knowledge dumps for compliance or historical research\n\n## ‚ö†Ô∏è Troubleshooting\n- No links extracted: Verify the homepage has <a> tags; test with a simple site like example.com and check HTTP response in executions\n- Sheet update fails: Confirm column names match exactly (case-sensitive) and credential has edit permissions; try a new blank sheet\n- Content truncated: Google Sheets limits cells to ~50k chars‚Äîadjust the `.slice(0, 50000)` in \"Add Scraped Content to Sheet\" or split into multiple rows\n- Rate limiting errors: Add a \"Wait\" node after \"Scrape Links\" with 1-2s delay if the site blocks rapid requests",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Overview Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Note: Trigger and Setup",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Set Website",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Manual Trigger",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Scrape Homepage",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Note: Homepage Scraping",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Extract Links from HTML",
      "type": "n8n-nodes-base.html",
      "role": "html",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Split Links",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Remove Duplicate Links",
      "type": "n8n-nodes-base.removeDuplicates",
      "role": "removeDuplicates",
      "configDescription": "Version 2"
    },
    {
      "name": "Filter Real Hyperlinks",
      "type": "n8n-nodes-base.filter",
      "role": "filter",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Note: Link Processing",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Separate Images and Links",
      "type": "n8n-nodes-base.switch",
      "role": "switch",
      "configDescription": "Version 3.2"
    },
    {
      "name": "Aggregate Images",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "Aggregate Links",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "Scrape Content Links",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Note: Content Scraping",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Convert to Markdown",
      "type": "n8n-nodes-base.markdown",
      "role": "markdown",
      "configDescription": "Version 1"
    },
    {
      "name": "Aggregate Scraped Content",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "Add Images to Sheet",
      "type": "n8n-nodes-base.googleSheets",
      "role": "googleSheets",
      "configDescription": "Version 4.7"
    },
    {
      "name": "Add Links to Sheet",
      "type": "n8n-nodes-base.googleSheets",
      "role": "googleSheets",
      "configDescription": "Version 4.7"
    },
    {
      "name": "Add Scraped Content to Sheet",
      "type": "n8n-nodes-base.googleSheets",
      "role": "googleSheets",
      "configDescription": "Version 4.7"
    },
    {
      "name": "Note: Sheet Integration",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}