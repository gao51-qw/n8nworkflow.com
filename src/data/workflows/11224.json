{
  "id": 11224,
  "slug": "11224",
  "title": "Build a multi-site content aggregator with Google Sheets & custom extraction logic",
  "description": "## An intelligent web scraping workflow that automatically routes URLs to site-specific extraction logic, normalizes data across multiple sources, and filters content by freshness to build a unified article feed.\n\n### **What Makes This Different:**\n- **Intelligent Source Routing** - Uses a Switch node to route URLs to specialized extractors based on source identifier, enabling custom CSS selectors per publisher for maximum accuracy\n- **Universal Fallback Parser** - Advanced regex-based extractor handles unknown sources automatically, extracting title, description, author, date, and images from meta tags and HTML patterns\n- **Freshness Filtering** - Built-in 45-day freshness threshold filters outdated content before saving, with configurable date validation logic\n- **Tier-Based Classification** - Automatically categorizes articles into Tier 1 (0-7 days), Tier 2 (8-14 days), Tier 3 (15-30 days), or Archive based on publication date\n- **Rate Limiting & Error Handling** - Built-in 3-second delays between requests prevents server overload, with comprehensive error handling that continues processing even if individual URLs fail\n- **Status Tracking** - Updates source spreadsheet with processing status, enabling easy monitoring and retry logic for failed extractions\n\n### **Key Benefits of Multi-Source Content Aggregation:**\n- **Scalable Architecture** - Easily add new sources by adding a Switch rule and extraction node, no code changes needed for most sites\n- **Data Normalization** - Standardizes extracted data across all sources into a consistent format (title, description, author, date, image, canonical URL)\n- **Automated Processing** - Schedule-based execution (every 4 hours) or manual triggers keep your feed updated without manual intervention\n- **Quality Control** - Freshness filtering ensures only recent, relevant content enters your feed, reducing noise from outdated articles\n- **Flexible Input** - Reads from Google Sheets, making it easy to add URLs in bulk or integrate with other systems\n- **Comprehensive Metadata** - Captures full article metadata including canonical URLs, publication dates, author information, and featured images\n\n---\n\n## Who's it for\n\nThis template is designed for **content aggregators, news monitoring services, content marketers, SEO professionals, researchers, and anyone who needs to collect and normalize articles from multiple websites**. It's perfect for organizations that need to **monitor competitor content**, **aggregate industry news**, **build content databases**, **track publication trends**, or **create unified article feeds** without **manually scraping each site or writing custom scrapers for every source**.\n\n## How it works / What it does\n\nThis workflow creates a **unified article aggregation system** that **reads URLs from Google Sheets, routes them to site-specific extractors, normalizes the data, filters by freshness, and saves results to a feed**. The system:\n\n1. **Reads Pending URLs** - Fetches URLs with source identifiers from Google Sheets, filtering for entries with \"Pending\" status\n2. **Processes with Rate Limiting** - Loops through URLs one at a time with a 3-second delay between requests to respect server resources\n3. **Fetches HTML Content** - Downloads page HTML with proper browser headers (User-Agent, Accept, Accept-Language) to avoid blocking\n4. **Routes by Source** - Switch node directs URLs to specialized extractors (Site A, B, C, D) or universal fallback parser based on Source field\n5. **Extracts Article Data** - Site-specific HTML nodes use custom CSS selectors, while fallback uses regex patterns to extract title, description, author, date, image, and canonical URL\n6. **Normalizes Data** - Standardizes all extracted fields into consistent format, handling missing values and trimming whitespace\n7. **Filters by Freshness** - Validates publication dates and filters out articles older than 45 days (configurable threshold)\n8. **Calculates Tier & Status** - Assigns tier classification and freshness status based on article age\n9. **Saves to Feed** - Appends normalized articles to Article Feed sheet with all metadata\n10. **Updates Status** - Marks processed URLs as complete in source sheet for tracking\n\n**Key Innovation: Source-Based Routing** - Unlike generic scrapers that use one-size-fits-all extraction, this workflow uses intelligent routing to apply site-specific CSS selectors. This dramatically improves extraction accuracy while maintaining a universal fallback for unknown sources, making it both precise and extensible.\n\n## How to set up\n\n### 1. Prepare Google Sheets\n- Create a Google Sheet with two tabs: **\"URLs to Process\"** and **\"Article Feed\"**\n- In **\"URLs to Process\"** sheet, create columns: `URL`, `Source`, `Status`\n- Add sample data: URLs in `URL` column, source identifiers (e.g., \"Site A\", \"Site B\") in `Source` column, and \"Pending\" in `Status` column\n- In **\"Article Feed\"** sheet, the workflow will automatically create columns: `Title`, `Description`, `Author`, `datePublished`, `imageUrl`, `canonicalUrl`, `source`, `sourceUrl`, `tier`, `freshnessStatus`, `extractedAt`\n- Verify your Google Sheets credentials are set up in n8n (OAuth2 recommended)\n\n### 2. Configure Google Sheets Nodes\n- Open the **\"Read Pending URLs\"** node and select your spreadsheet from the document dropdown\n- Set sheet name to **\"URLs to Process\"**\n- Configure the **\"Save to Article Feed\"** node: select same spreadsheet, set sheet name to **\"Article Feed\"**, operation should be **\"Append or Update\"**\n- Configure the **\"Update URL Status\"** node: same spreadsheet, **\"URLs to Process\"** sheet, operation **\"Update\"**\n- Test connection by running the \"Read Pending URLs\" node manually to verify it can access your sheet\n\n### 3. Customize Source Routing\n- Open the **\"Source Router\"** (Switch node) to see current routing rules for Site A, B, C, D, and fallback\n- To add a new source: Click \"Add Rule\", set condition: `{{ $('Loop Over URLs').item.json.Source }}` equals your source name\n- Create a new HTML extraction node for your source with appropriate CSS selectors\n- Connect the new extractor to the **\"Normalize Extracted Data\"** node\n- Update the Switch node to route to your new extractor\n- Example CSS selectors for common sites:\n  ```javascript\n  // WordPress sites\n  title: \"h1.entry-title, .post-title\"\n  author: \".author-name, .byline a\"\n  date: \"time.entry-date, time[datetime]\"\n  \n  // Modern CMS\n  title: \"h1.article__title, article h1\"\n  author: \".article__byline a, a[rel='author']\"\n  date: \"time[datetime], meta[property='article:published_time']\"\n  ```\n\n### 4. Configure Freshness Threshold\n- Open the **\"Freshness Filter (45 days)\"** IF node\n- The current threshold is 45 days (configurable in the condition expression)\n- To change threshold: Modify the expression `cutoffDate.setDate(cutoffDate.getDate() - 45)` to your desired number of days\n- The filter marks articles as \"Fresh\" (within threshold) or routes to \"Outdated\" handler\n- Test with sample URLs to verify date parsing works correctly for your sources\n\n### 5. Set Up Scheduling & Test\n- The workflow includes both **Manual Trigger** (for testing) and **Schedule Trigger** (runs every 4 hours)\n- To customize schedule: Open \"Schedule (Every 4 Hours)\" node and adjust interval\n- For initial testing: Use Manual Trigger, add 2-3 test URLs to your sheet with Status=\"Pending\"\n- Verify execution: Check that URLs are fetched, routed correctly, extracted, and saved to Article Feed\n- Monitor the **\"Completion Summary\"** node output to see processing statistics\n- Check execution logs for any errors in HTML extraction or date parsing\n- Common issues: Missing CSS selectors (update extractor), date format mismatches (adjust date parsing), or rate limiting (increase wait time if needed)\n\n## Requirements\n\n- **Google Sheets Account** - Active Google account with OAuth2 credentials configured in n8n for reading and writing spreadsheet data\n- **Source Spreadsheet** - Google Sheet with \"URLs to Process\" and \"Article Feed\" tabs, properly formatted with required columns\n- **n8n Instance** - Self-hosted or cloud n8n instance with access to external websites (HTTP Request node needs internet connectivity)\n- **Source Knowledge** - Understanding of target website HTML structure to configure CSS selectors for site-specific extractors (or use fallback parser for unknown sources)\n",
  "featuredImage": "/data/workflows/11224/11224.webp",
  "author": {
    "id": 101,
    "slug": "omerfayyaz",
    "name": "Omer Fayyaz",
    "avatar": ""
  },
  "categories": [
    "Market Research"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 40,
  "downloads": 4,
  "createdAt": "2025-11-25T16:27:24.095Z",
  "updatedAt": "2026-01-16T09:08:00.099Z",
  "publishedAt": "2025-11-25T16:27:24.095Z",
  "nodes": 27,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/11224",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Build a multi-site content aggregator with Google Sheets & custom extraction logic",
    "workflowName": "Build a multi-site content aggregator with Google Sheets & custom extraction logic",
    "description": "## An intelligent web scraping workflow that automatically routes URLs to site-specific extraction logic, normalizes data across multiple sources, and filters content by freshness to build a unified article feed.\n\n### **What Makes This Different:**\n- **Intelligent Source Routing** - Uses a Switch node to route URLs to specialized extractors based on source identifier, enabling custom CSS selectors per publisher for maximum accuracy\n- **Universal Fallback Parser** - Advanced regex-based extractor handles unknown sources automatically, extracting title, description, author, date, and images from meta tags and HTML patterns\n- **Freshness Filtering** - Built-in 45-day freshness threshold filters outdated content before saving, with configurable date validation logic\n- **Tier-Based Classification** - Automatically categorizes articles into Tier 1 (0-7 days), Tier 2 (8-14 days), Tier 3 (15-30 days), or Archive based on publication date\n- **Rate Limiting & Error Handling** - Built-in 3-second delays between requests prevents server overload, with comprehensive error handling that continues processing even if individual URLs fail\n- **Status Tracking** - Updates source spreadsheet with processing status, enabling easy monitoring and retry logic for failed extractions\n\n### **Key Benefits of Multi-Source Content Aggregation:**\n- **Scalable Architecture** - Easily add new sources by adding a Switch rule and extraction node, no code changes needed for most sites\n- **Data Normalization** - Standardizes extracted data across all sources into a consistent format (title, description, author, date, image, canonical URL)\n- **Automated Processing** - Schedule-based execution (every 4 hours) or manual triggers keep your feed updated without manual intervention\n- **Quality Control** - Freshness filtering ensures only recent, relevant content enters your feed, reducing noise from outdated articles\n- **Flexible Input** - Reads from Google Sheets, making it easy to add URLs in bulk or integrate with other systems\n- **Comprehensive Metadata** - Captures full article metadata including canonical URLs, publication dates, author information, and featured images\n\n---\n\n## Who's it for\n\nThis template is designed for **content aggregators, news monitoring services, content marketers, SEO professionals, researchers, and anyone who needs to collect and normalize articles from multiple websites**. It's perfect for organizations that need to **monitor competitor content**, **aggregate industry news**, **build content databases**, **track publication trends**, or **create unified article feeds** without **manually scraping each site or writing custom scrapers for every source**.\n\n## How it works / What it does\n\nThis workflow creates a **unified article aggregation system** that **reads URLs from Google Sheets, routes them to site-specific extractors, normalizes the data, filters by freshness, and saves results to a feed**. The system:\n\n1. **Reads Pending URLs** - Fetches URLs with source identifiers from Google Sheets, filtering for entries with \"Pending\" status\n2. **Processes with Rate Limiting** - Loops through URLs one at a time with a 3-second delay between requests to respect server resources\n3. **Fetches HTML Content** - Downloads page HTML with proper browser headers (User-Agent, Accept, Accept-Language) to avoid blocking\n4. **Routes by Source** - Switch node directs URLs to specialized extractors (Site A, B, C, D) or universal fallback parser based on Source field\n5. **Extracts Article Data** - Site-specific HTML nodes use custom CSS selectors, while fallback uses regex patterns to extract title, description, author, date, image, and canonical URL\n6. **Normalizes Data** - Standardizes all extracted fields into consistent format, handling missing values and trimming whitespace\n7. **Filters by Freshness** - Validates publication dates and filters out articles older than 45 days (configurable threshold)\n8. **Calculates Tier & Status** - Assigns tier classification and freshness status based on article age\n9. **Saves to Feed** - Appends normalized articles to Article Feed sheet with all metadata\n10. **Updates Status** - Marks processed URLs as complete in source sheet for tracking\n\n**Key Innovation: Source-Based Routing** - Unlike generic scrapers that use one-size-fits-all extraction, this workflow uses intelligent routing to apply site-specific CSS selectors. This dramatically improves extraction accuracy while maintaining a universal fallback for unknown sources, making it both precise and extensible.\n\n## How to set up\n\n### 1. Prepare Google Sheets\n- Create a Google Sheet with two tabs: **\"URLs to Process\"** and **\"Article Feed\"**\n- In **\"URLs to Process\"** sheet, create columns: `URL`, `Source`, `Status`\n- Add sample data: URLs in `URL` column, source identifiers (e.g., \"Site A\", \"Site B\") in `Source` column, and \"Pending\" in `Status` column\n- In **\"Article Feed\"** sheet, the workflow will automatically create columns: `Title`, `Description`, `Author`, `datePublished`, `imageUrl`, `canonicalUrl`, `source`, `sourceUrl`, `tier`, `freshnessStatus`, `extractedAt`\n- Verify your Google Sheets credentials are set up in n8n (OAuth2 recommended)\n\n### 2. Configure Google Sheets Nodes\n- Open the **\"Read Pending URLs\"** node and select your spreadsheet from the document dropdown\n- Set sheet name to **\"URLs to Process\"**\n- Configure the **\"Save to Article Feed\"** node: select same spreadsheet, set sheet name to **\"Article Feed\"**, operation should be **\"Append or Update\"**\n- Configure the **\"Update URL Status\"** node: same spreadsheet, **\"URLs to Process\"** sheet, operation **\"Update\"**\n- Test connection by running the \"Read Pending URLs\" node manually to verify it can access your sheet\n\n### 3. Customize Source Routing\n- Open the **\"Source Router\"** (Switch node) to see current routing rules for Site A, B, C, D, and fallback\n- To add a new source: Click \"Add Rule\", set condition: `{{ $('Loop Over URLs').item.json.Source }}` equals your source name\n- Create a new HTML extraction node for your source with appropriate CSS selectors\n- Connect the new extractor to the **\"Normalize Extracted Data\"** node\n- Update the Switch node to route to your new extractor\n- Example CSS selectors for common sites:\n  ```javascript\n  // WordPress sites\n  title: \"h1.entry-title, .post-title\"\n  author: \".author-name, .byline a\"\n  date: \"time.entry-date, time[datetime]\"\n  \n  // Modern CMS\n  title: \"h1.article__title, article h1\"\n  author: \".article__byline a, a[rel='author']\"\n  date: \"time[datetime], meta[property='article:published_time']\"\n  ```\n\n### 4. Configure Freshness Threshold\n- Open the **\"Freshness Filter (45 days)\"** IF node\n- The current threshold is 45 days (configurable in the condition expression)\n- To change threshold: Modify the expression `cutoffDate.setDate(cutoffDate.getDate() - 45)` to your desired number of days\n- The filter marks articles as \"Fresh\" (within threshold) or routes to \"Outdated\" handler\n- Test with sample URLs to verify date parsing works correctly for your sources\n\n### 5. Set Up Scheduling & Test\n- The workflow includes both **Manual Trigger** (for testing) and **Schedule Trigger** (runs every 4 hours)\n- To customize schedule: Open \"Schedule (Every 4 Hours)\" node and adjust interval\n- For initial testing: Use Manual Trigger, add 2-3 test URLs to your sheet with Status=\"Pending\"\n- Verify execution: Check that URLs are fetched, routed correctly, extracted, and saved to Article Feed\n- Monitor the **\"Completion Summary\"** node output to see processing statistics\n- Check execution logs for any errors in HTML extraction or date parsing\n- Common issues: Missing CSS selectors (update extractor), date format mismatches (adjust date parsing), or rate limiting (increase wait time if needed)\n\n## Requirements\n\n- **Google Sheets Account** - Active Google account with OAuth2 credentials configured in n8n for reading and writing spreadsheet data\n- **Source Spreadsheet** - Google Sheet with \"URLs to Process\" and \"Article Feed\" tabs, properly formatted with required columns\n- **n8n Instance** - Self-hosted or cloud n8n instance with access to external websites (HTTP Request node needs internet connectivity)\n- **Source Knowledge** - Understanding of target website HTML structure to configure CSS selectors for site-specific extractors (or use fallback parser for unknown sources)",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Sticky Note - Introduction",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note - Input",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note - Router",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note - Extractors",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note - Freshness",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note - Output",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Manual Trigger",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Schedule (Every 4 Hours)",
      "type": "n8n-nodes-base.scheduleTrigger",
      "role": "scheduleTrigger",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Read Pending URLs",
      "type": "n8n-nodes-base.googleSheets",
      "role": "googleSheets",
      "configDescription": "Version 4.7"
    },
    {
      "name": "Loop Over URLs",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "Rate Limit (3s)",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Fetch HTML",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Source Router",
      "type": "n8n-nodes-base.switch",
      "role": "switch",
      "configDescription": "Version 3.2"
    },
    {
      "name": "Extract: Fallback (Universal)",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Normalize Extracted Data",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Freshness Filter (45 days)",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Calculate Tier & Status",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Mark as Outdated",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Save to Article Feed",
      "type": "n8n-nodes-base.googleSheets",
      "role": "googleSheets",
      "configDescription": "Version 4.7"
    },
    {
      "name": "Update URL Status",
      "type": "n8n-nodes-base.googleSheets",
      "role": "googleSheets",
      "configDescription": "Version 4.7"
    },
    {
      "name": "Completion Summary",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Extract: Site B",
      "type": "n8n-nodes-base.html",
      "role": "html",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Extract: Site C",
      "type": "n8n-nodes-base.html",
      "role": "html",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Extract: Site D",
      "type": "n8n-nodes-base.html",
      "role": "html",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sticky Note - Input1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Extract: Site A",
      "type": "n8n-nodes-base.html",
      "role": "html",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sticky Note - Freshness1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}