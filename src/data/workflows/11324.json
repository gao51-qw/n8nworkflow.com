{
  "id": 11324,
  "slug": "11324",
  "title": "Telegram support bot with OpenAI, Pinecone and human expert escalation",
  "description": "**Template name**  \nSmart AI Support Assistant for Telegram\n\n**Short description**  \nSmart AI Support Assistant for Telegram automatically answers repeated questions in your group using a Q&A knowledge base in Pinecone and forwards new or unclear questions to a human expert.\n\n**Long description (Description поле)**  \n\n## How it works\n\n1. **Question detection** listens to messages in a Telegram group and checks whether each new message is a real question or an expert reply.  \n2. **Knowledge base search** looks for an existing answer in the Pinecone vector store for valid questions from the group.  \n3. **Auto‑reply from cache** sends the saved answer straight back to the group when a good match is found, without involving the expert.  \n4. **Escalation to expert** creates a ticket and forwards unanswered questions to the expert in a private chat with the same bot.  \n5. **Expert learning loop** saves the expert’s reply to Pinecone so that similar questions are answered automatically in the future.\n\n## Setup steps\n\n- Connect **Telegram Trigger** to a single Telegram bot that is added as an admin to the group/supergroup and receives all user messages.  \n- Use **the same bot** for the expert: the expert’s private chat with this bot is where tickets and questions are delivered.  \n- Set up **Pinecone**: create an index, note the environment and index name, and add your Pinecone API key to n8n credentials.  \n- Add your **AI model API key** (for example, OpenAI) and select the model used for embeddings and answer rewriting.  \n- Configure any environment variables or n8n credentials for project IDs and spaces/namespaces used in Pinecone.  \n- Test the full flow: send a question in the group, confirm that a ticket reaches the expert in a private chat, reply once, and check that the next similar question is answered automatically from the cache.\n",
  "featuredImage": "/data/workflows/11324/11324.webp",
  "author": {
    "id": 101,
    "slug": "chernyaevi",
    "name": "Igor Chernyaev",
    "avatar": ""
  },
  "categories": [
    "Support Chatbot",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 101,
  "downloads": 10,
  "createdAt": "2025-11-28T09:11:56.205Z",
  "updatedAt": "2026-01-16T09:08:21.802Z",
  "publishedAt": "2025-11-28T09:11:56.205Z",
  "nodes": 44,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/11324",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Telegram support bot with OpenAI, Pinecone and human expert escalation",
    "workflowName": "Telegram support bot with OpenAI, Pinecone and human expert escalation",
    "description": "**Template name**  \nSmart AI Support Assistant for Telegram\n\n**Short description**  \nSmart AI Support Assistant for Telegram automatically answers repeated questions in your group using a Q&A knowledge base in Pinecone and forwards new or unclear questions to a human expert.\n\n**Long description (Description поле)**  \n\n## How it works\n\n1. **Question detection** listens to messages in a Telegram group and checks whether each new message is a real question or an expert reply.  \n2. **Knowledge base search** looks for an existing answer in the Pinecone vector store for valid questions from the group.  \n3. **Auto‑reply from cache** sends the saved answer straight back to the group when a good match is found, without involving the expert.  \n4. **Escalation to expert** creates a ticket and forwards unanswered questions to the expert in a private chat with the same bot.  \n5. **Expert learning loop** saves the expert’s reply to Pinecone so that similar questions are answered automatically in the future.\n\n## Setup steps\n\n- Connect **Telegram Trigger** to a single Telegram bot that is added as an admin to the group/supergroup and receives all user messages.  \n- Use **the same bot** for the expert: the expert’s private chat with this bot is where tickets and questions are delivered.  \n- Set up **Pinecone**: create an index, note the environment and index name, and add your Pinecone API key to n8n credentials.  \n- Add your **AI model API key** (for example, OpenAI) and select the model used for embeddings and answer rewriting.  \n- Configure any environment variables or n8n credentials for project IDs and spaces/namespaces used in Pinecone.  \n- Test the full flow: send a question in the group, confirm that a ticket reaches the expert in a private chat, reply once, and check that the next similar question is answered automatically from the cache.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Telegram Trigger",
      "type": "n8n-nodes-base.telegramTrigger",
      "role": "telegramTrigger",
      "configDescription": "Version 1.2"
    },
    {
      "name": "QuestionOrReply_Switch",
      "type": "n8n-nodes-base.switch",
      "role": "switch",
      "configDescription": "Version 3.3"
    },
    {
      "name": "QuestionFilter_Code",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "ValidQuestion_IF",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "CreateTicket_Code",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "SendToExpert_Message",
      "type": "n8n-nodes-base.telegram",
      "role": "telegram",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Reformulate_Message",
      "type": "n8n-nodes-base.telegram",
      "role": "telegram",
      "configDescription": "Version 1.2"
    },
    {
      "name": "ExpertReply_Message",
      "type": "n8n-nodes-base.telegram",
      "role": "telegram",
      "configDescription": "Version 1.2"
    },
    {
      "name": "IsExpert_Code",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "IsExpert_IF",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "PrepareReplyFlag_Code",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Pinecone Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStorePinecone",
      "role": "vectorStorePinecone",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Embeddings OpenAI",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Recursive Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
      "role": "textSplitterRecursiveCharacterTextSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "Pinecone_ParseResult_Code",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Set_QA_for_Pinecone",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Embeddings OpenAI1",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "EditFields_AddUserID_Metadata",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "AddUserID_Metadata_Reply",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Pinecone_Add_Answer",
      "type": "@n8n/n8n-nodes-langchain.vectorStorePinecone",
      "role": "vectorStorePinecone",
      "configDescription": "Version 1.3"
    },
    {
      "name": "GetFirstResult",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Check If Answer Exists",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Send Auto Answer",
      "type": "n8n-nodes-base.telegram",
      "role": "telegram",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Get First Answered Document",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Set Original Question",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Restore Original Question",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Filter Similar Answers",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "CheckCacheHit",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "CacheHitIF",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "SendCachedAnswer",
      "type": "n8n-nodes-base.telegram",
      "role": "telegram",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Extract_Original_Question",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Default Data Loader1",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1.1"
    },
    {
      "name": "WrapForPinecone",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Recursive Character Text Splitter1",
      "type": "@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
      "role": "textSplitterRecursiveCharacterTextSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "Extract Group ID",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Edit Fields",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Embeddings OpenAI3",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Search Answer In Pinecone",
      "type": "@n8n/n8n-nodes-langchain.vectorStorePinecone",
      "role": "vectorStorePinecone",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}