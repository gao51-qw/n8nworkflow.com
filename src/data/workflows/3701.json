{
  "id": 3701,
  "slug": "3701",
  "title": "Scrape books from URL with Dumpling AI, clean HTML, save to Sheets, email as CSV",
  "description": "\n### üë• Who is this for?\n\nThis workflow is ideal for virtual assistants, researchers, developers, automation specialists, and data analysts who need to regularly extract and organize structured product information (like books) from a website. It‚Äôs especially useful for those working with catalog-based websites who want to automate extraction and delivery of clean, sorted data.\n\n---\n\n### üß© What problem is this solving?\n\nManually copying product listings like book titles and prices from a website into a spreadsheet is slow and repetitive. This automation solves that problem by scraping content using Dumpling AI, extracting the right data using CSS selectors, and formatting it into a clean CSV file that is sent to your email‚Äîall triggered automatically when a new URL is added to Google Sheets.\n\n---\n\n### ‚öôÔ∏è What this workflow does\n\nThis template automates an entire content scraping and delivery process:\n\n- Watches a Google Sheet for new URLs\n- Scrapes the HTML content of the given webpage using Dumpling AI\n- Uses CSS selectors in the HTML node to extract each book from the page\n- Splits the HTML array into individual items\n- Extracts the book title and price from each HTML block\n- Sorts the books in descending order based on price\n- Converts the sorted data to a CSV file\n- Sends the CSV via email using Gmail\n\n---\n\n### üõ†Ô∏è Setup\n\n1. **Google Sheets**\n   - Create a sheet titled something like `URLs`\n   - Add your product listing URLs (e.g., [http://books.toscrape.com](http://books.toscrape.com))\n   - Connect the Google Sheets trigger node to your sheet\n   - Ensure you have proper credentials connected\n\n2. **Dumpling AI**\n   - Create an account at [Dumpling AI](https://app.dumplingai.com])   - Generate your API key\n   \n   - Set the HTTP Method to `POST` and pass the URL dynamically from the Google Sheet\n   - Use `Header Auth` to include your API key in the request header\n   - Make sure `\"cleaned\": \"True\"` is included in the body for optimized HTML output\n\n3. **HTML Node**\n   - The first HTML node extracts the main book container blocks using:  \n     `.row &gt; li`\n   - The second HTML node parses out the individual fields:  \n     - `title`: `h3 &gt; a` (via the `title` attribute)  \n     - `price`: `.price_color`\n\n4. **Sort Node**\n   - Sorts books by `price` in descending order  \n   - Note: price is extracted as a string, ensure it's parsable if you plan to use numeric filtering later\n\n5. **Convert to CSV**\n   - The JSON data is passed into a Convert node and transformed into a CSV file\n\n6. **Gmail**\n   - Sends the CSV as an attachment to a designated email\n\n \n\n---\n\n### üîÑ How to customize this workflow\n\n- **Extract more data**: Add more CSS selectors in the second HTML node to pull fields like author, availability, or product links\n- **Switch destinations**: Replace Gmail with Slack, Google Drive, Dropbox, or another platform\n- **Adjust sorting**: Sort alphabetically or based on another extracted value\n- **Use a different source**: As long as the site structure is consistent, this can scrape any listing-like page\n- **Trigger differently**: Use a webhook, form submission, or schedule trigger instead of Google Sheets\n\n---\n\n### ‚ö†Ô∏è Dependencies and Notes\n\n- This workflow uses **Dumpling AI** to perform the web scraping. This requires an API key and uses credits per request.\n- The **HTML node** depends on valid CSS selectors. If the site layout changes, the selectors may need to be updated.\n- Ensure you‚Äôre not scraping content from websites that prohibit automated scraping.\n",
  "featuredImage": "/data/workflows/3701/3701.webp",
  "author": {
    "id": 101,
    "slug": "yang",
    "name": "Yang",
    "avatar": ""
  },
  "categories": [
    "Market Research"
  ],
  "complexityLevel": "intermediate",
  "price": 0,
  "visitors": 3155,
  "downloads": 315,
  "createdAt": "2025-04-24T23:10:27.733Z",
  "updatedAt": "2026-01-16T08:31:09.442Z",
  "publishedAt": "2025-04-24T23:10:27.733Z",
  "nodes": 11,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/3701",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Scrape books from URL with Dumpling AI, clean HTML, save to Sheets, email as CSV",
    "workflowName": "Scrape books from URL with Dumpling AI, clean HTML, save to Sheets, email as CSV",
    "description": "### üë• Who is this for?\n\nThis workflow is ideal for virtual assistants, researchers, developers, automation specialists, and data analysts who need to regularly extract and organize structured product information (like books) from a website. It‚Äôs especially useful for those working with catalog-based websites who want to automate extraction and delivery of clean, sorted data.\n\n---\n\n### üß© What problem is this solving?\n\nManually copying product listings like book titles and prices from a website into a spreadsheet is slow and repetitive. This automation solves that problem by scraping content using Dumpling AI, extracting the right data using CSS selectors, and formatting it into a clean CSV file that is sent to your email‚Äîall triggered automatically when a new URL is added to Google Sheets.\n\n---\n\n### ‚öôÔ∏è What this workflow does\n\nThis template automates an entire content scraping and delivery process:\n\n- Watches a Google Sheet for new URLs\n- Scrapes the HTML content of the given webpage using Dumpling AI\n- Uses CSS selectors in the HTML node to extract each book from the page\n- Splits the HTML array into individual items\n- Extracts the book title and price from each HTML block\n- Sorts the books in descending order based on price\n- Converts the sorted data to a CSV file\n- Sends the CSV via email using Gmail\n\n---\n\n### üõ†Ô∏è Setup\n\n1. **Google Sheets**\n   - Create a sheet titled something like `URLs`\n   - Add your product listing URLs (e.g., [http://books.toscrape.com](http://books.toscrape.com))\n   - Connect the Google Sheets trigger node to your sheet\n   - Ensure you have proper credentials connected\n\n2. **Dumpling AI**\n   - Create an account at [Dumpling AI](https://app.dumplingai.com])   - Generate your API key\n   \n   - Set the HTTP Method to `POST` and pass the URL dynamically from the Google Sheet\n   - Use `Header Auth` to include your API key in the request header\n   - Make sure `\"cleaned\": \"True\"` is included in the body for optimized HTML output\n\n3. **HTML Node**\n   - The first HTML node extracts the main book container blocks using:  \n     `.row &gt; li`\n   - The second HTML node parses out the individual fields:  \n     - `title`: `h3 &gt; a` (via the `title` attribute)  \n     - `price`: `.price_color`\n\n4. **Sort Node**\n   - Sorts books by `price` in descending order  \n   - Note: price is extracted as a string, ensure it's parsable if you plan to use numeric filtering later\n\n5. **Convert to CSV**\n   - The JSON data is passed into a Convert node and transformed into a CSV file\n\n6. **Gmail**\n   - Sends the CSV as an attachment to a designated email\n\n \n\n---\n\n### üîÑ How to customize this workflow\n\n- **Extract more data**: Add more CSS selectors in the second HTML node to pull fields like author, availability, or product links\n- **Switch destinations**: Replace Gmail with Slack, Google Drive, Dropbox, or another platform\n- **Adjust sorting**: Sort alphabetically or based on another extracted value\n- **Use a different source**: As long as the site structure is consistent, this can scrape any listing-like page\n- **Trigger differently**: Use a webhook, form submission, or schedule trigger instead of Google Sheets\n\n---\n\n### ‚ö†Ô∏è Dependencies and Notes\n\n- This workflow uses **Dumpling AI** to perform the web scraping. This requires an API key and uses credits per request.\n- The **HTML node** depends on valid CSS selectors. If the site layout changes, the selectors may need to be updated.\n- Ensure you‚Äôre not scraping content from websites that prohibit automated scraping.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Convert to CSV File",
      "type": "n8n-nodes-base.convertToFile",
      "role": "convertToFile",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Extract all books from the page",
      "type": "n8n-nodes-base.html",
      "role": "html",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sort by price",
      "type": "n8n-nodes-base.sort",
      "role": "sort",
      "configDescription": "Version 1"
    },
    {
      "name": "Extract individual book price",
      "type": "n8n-nodes-base.html",
      "role": "html",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Send CSV via e-mail",
      "type": "n8n-nodes-base.gmail",
      "role": "gmail",
      "configDescription": "Version 2.1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Trigger- Watches For new URL in Spreadsheet",
      "type": "n8n-nodes-base.googleSheetsTrigger",
      "role": "googleSheetsTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Scrape Website Content with Dumpling AI",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.1"
    },
    {
      "name": "Split HTML Array into Individual Books",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    }
  ]
}