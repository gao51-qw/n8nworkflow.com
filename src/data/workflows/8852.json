{
  "id": 8852,
  "slug": "8852",
  "title": "Domain-specific web content crawler with depth control & text extraction",
  "description": "This template implements a recursive web crawler inside n8n. Starting from a given URL, it crawls linked pages up to a maximum depth (default: 3), extracts text and links, and returns the collected content via webhook.\n\n---\n\n## üöÄ How It Works\n\n1) **Webhook Trigger**  \n   Accepts a JSON body with a `url` field.  \n   Example payload:\n    \n    { \"url\": \"https://example.com\" }\n\n2) **Initialization**  \n   - Sets crawl parameters: `url`, `domain`, `maxDepth = 3`, and `depth = 0`.  \n   - Initializes global static data (`pending`, `visited`, `queued`, `pages`).\n\n3) **Recursive Crawling**  \n   - Fetches each page (HTTP Request).  \n   - Extracts body text and links (HTML node).  \n   - Cleans and deduplicates links.  \n   - Filters out:\n     - External domains (only same-site is followed)  \n     - Anchors (#), mailto/tel/javascript links  \n     - Non-HTML files (.pdf, .docx, .xlsx, .pptx)\n\n4) **Depth Control & Queue**  \n   - Tracks visited URLs  \n   - Stops at `maxDepth` to prevent infinite loops  \n   - Uses SplitInBatches to loop the queue\n\n5) **Data Collection**  \n   - Saves each crawled page (`url`, `depth`, `content`) into `pages[]`  \n   - When `pending = 0`, combines results\n\n6) **Output**  \n   - Responds via the Webhook node with:\n     - `combinedContent` (all pages concatenated)\n     - `pages[]` (array of individual results)\n   - Large results are chunked when exceeding ~12,000 characters\n\n---\n\n## üõ†Ô∏è Setup Instructions\n\n1) **Import Template**  \n   Load from n8n Community Templates.\n\n2) **Configure Webhook**  \n   - Open the **Webhook** node  \n   - Copy the Test URL (development) or Production URL (after deploy)  \n   - You‚Äôll POST crawl requests to this endpoint\n\n3) **Run a Test**  \n   Send a POST with JSON:\n\n    curl -X POST https://&lt;your-n8n&gt;/webhook/&lt;id&gt; \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"url\": \"https://example.com\"}'\n\n4) **View Response**  \n   The crawler returns a JSON object containing `combinedContent` and `pages[]`.\n\n---\n\n## ‚öôÔ∏è Configuration\n\n- **maxDepth**  \n  Default: 3. Adjust in the **Init Crawl Params** (Set) node.\n\n- **Timeouts**  \n  HTTP Request node timeout is 5 seconds per request; increase if needed.\n\n- **Filtering Rules**  \n  - Only same-domain links are followed (apex and `www` treated as same-site)  \n  - Skips anchors, `mailto:`, `tel:`, `javascript:`  \n  - Skips document links (.pdf, .docx, .xlsx, .pptx)  \n  - You can tweak the regex and logic in **Queue & Dedup Links** (Code) node\n\n---\n\n## üìå Limitations\n\n- No JavaScript rendering (static HTML only)  \n- No authentication/cookies/session handling  \n- Large sites can be slow or hit timeouts; chunking mitigates response size\n\n---\n\n## ‚úÖ Example Use Cases\n\n- Extract text across your site for AI ingestion / embeddings  \n- SEO/content audit and internal link checks  \n- Build a lightweight page corpus for downstream processing in n8n\n\n---\n\n## ‚è±Ô∏è Estimated Setup Time\n\n~10 minutes (import ‚Üí set webhook ‚Üí test request)\n\n",
  "featuredImage": "/data/workflows/8852/8852.webp",
  "author": {
    "id": 101,
    "slug": "leeseifer",
    "name": "Le Nguyen",
    "avatar": ""
  },
  "categories": [
    "Document Extraction"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 1428,
  "downloads": 142,
  "createdAt": "2025-09-23T07:34:54.369Z",
  "updatedAt": "2026-01-16T08:57:57.288Z",
  "publishedAt": "2025-09-23T07:34:54.369Z",
  "nodes": 18,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/8852",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Domain-specific web content crawler with depth control & text extraction",
    "workflowName": "Domain-specific web content crawler with depth control & text extraction",
    "description": "This template implements a recursive web crawler inside n8n. Starting from a given URL, it crawls linked pages up to a maximum depth (default: 3), extracts text and links, and returns the collected content via webhook.\n\n---\n\n## üöÄ How It Works\n\n1) **Webhook Trigger**  \n   Accepts a JSON body with a `url` field.  \n   Example payload:\n    \n    { \"url\": \"https://example.com\" }\n\n2) **Initialization**  \n   - Sets crawl parameters: `url`, `domain`, `maxDepth = 3`, and `depth = 0`.  \n   - Initializes global static data (`pending`, `visited`, `queued`, `pages`).\n\n3) **Recursive Crawling**  \n   - Fetches each page (HTTP Request).  \n   - Extracts body text and links (HTML node).  \n   - Cleans and deduplicates links.  \n   - Filters out:\n     - External domains (only same-site is followed)  \n     - Anchors (#), mailto/tel/javascript links  \n     - Non-HTML files (.pdf, .docx, .xlsx, .pptx)\n\n4) **Depth Control & Queue**  \n   - Tracks visited URLs  \n   - Stops at `maxDepth` to prevent infinite loops  \n   - Uses SplitInBatches to loop the queue\n\n5) **Data Collection**  \n   - Saves each crawled page (`url`, `depth`, `content`) into `pages[]`  \n   - When `pending = 0`, combines results\n\n6) **Output**  \n   - Responds via the Webhook node with:\n     - `combinedContent` (all pages concatenated)\n     - `pages[]` (array of individual results)\n   - Large results are chunked when exceeding ~12,000 characters\n\n---\n\n## üõ†Ô∏è Setup Instructions\n\n1) **Import Template**  \n   Load from n8n Community Templates.\n\n2) **Configure Webhook**  \n   - Open the **Webhook** node  \n   - Copy the Test URL (development) or Production URL (after deploy)  \n   - You‚Äôll POST crawl requests to this endpoint\n\n3) **Run a Test**  \n   Send a POST with JSON:\n\n    curl -X POST https://&lt;your-n8n&gt;/webhook/&lt;id&gt; \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"url\": \"https://example.com\"}'\n\n4) **View Response**  \n   The crawler returns a JSON object containing `combinedContent` and `pages[]`.\n\n---\n\n## ‚öôÔ∏è Configuration\n\n- **maxDepth**  \n  Default: 3. Adjust in the **Init Crawl Params** (Set) node.\n\n- **Timeouts**  \n  HTTP Request node timeout is 5 seconds per request; increase if needed.\n\n- **Filtering Rules**  \n  - Only same-domain links are followed (apex and `www` treated as same-site)  \n  - Skips anchors, `mailto:`, `tel:`, `javascript:`  \n  - Skips document links (.pdf, .docx, .xlsx, .pptx)  \n  - You can tweak the regex and logic in **Queue & Dedup Links** (Code) node\n\n---\n\n## üìå Limitations\n\n- No JavaScript rendering (static HTML only)  \n- No authentication/cookies/session handling  \n- Large sites can be slow or hit timeouts; chunking mitigates response size\n\n---\n\n## ‚úÖ Example Use Cases\n\n- Extract text across your site for AI ingestion / embeddings  \n- SEO/content audit and internal link checks  \n- Build a lightweight page corpus for downstream processing in n8n\n\n---\n\n## ‚è±Ô∏è Estimated Setup Time\n\n~10 minutes (import ‚Üí set webhook ‚Üí test request)",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "role": "webhook",
      "configDescription": "Version 2.1"
    },
    {
      "name": "Loop Links (Batches)",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 1"
    },
    {
      "name": "IF Crawl Depth OK?",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 1"
    },
    {
      "name": "Extract Body & Links",
      "type": "n8n-nodes-base.html",
      "role": "html",
      "configDescription": "Version 1"
    },
    {
      "name": "Attach URL/Depth to HTML",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Fetch HTML Page",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Seed Root Crawl Item",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.2"
    },
    {
      "name": "Collect Pages & Emit When Done",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Store Page Data",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 2"
    },
    {
      "name": "Merge Web Pages",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.2"
    },
    {
      "name": "Combine & Chunk",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Respond to Webhook",
      "type": "n8n-nodes-base.respondToWebhook",
      "role": "respondToWebhook",
      "configDescription": "Version 1.4"
    },
    {
      "name": "Init Globals",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Init Crawl Params",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 2"
    },
    {
      "name": "Requeue Link Item",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Queue & Dedup Links",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    }
  ]
}