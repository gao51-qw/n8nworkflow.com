{
  "id": 8467,
  "slug": "8467",
  "title": "Extract & store recently added n8n community workflows with ScrapeGraphAI and Gemini",
  "description": "This is an exaple of advanced automated data extraction and enrichment pipeline with [ScrapeGraphAI](https://dashboard.scrapegraphai.com/?via=n3witalia). Its primary purpose is to **systematically scrape the n8n community workflows website, extract detailed information** about recently added workflows, process that data using multiple AI models, and store the structured results in a Google Sheets spreadsheet.\n\nThis workflow demonstrates a sophisticated use of n8n to move beyond simple API calls and into the realm of intelligent, AI-driven web scraping and data processing, turning unstructured website content into valuable, structured business intelligence.\n\n\n---\n\n### Key Advantages\n\n* ✅ **Full Automation:** Once triggered (manually or on a schedule via the Schedule Trigger node), the entire process runs hands-free, from data collection to spreadsheet population.\n\n* ✅  **Powerful AI-Augmented Scraping:** It doesn't just scrape raw HTML. It uses multiple AI agents (Google Gemini, OpenAI) to:\n    *   **Understand page structure** to find the right data on the main list.\n    *   **Clean and purify content** from individual pages, removing and irrelevant information.\n    *   **Perform precise information extraction** to parse unstructured text into structured JSON data based on a defined schema (author, price, etc.).\n    *   **Generate intelligent summaries**, adding significant value by explaining the workflow's purpose in Italian.\n\n* ✅ **Robust and Structured Data Output:** The use of the **Structured Output Parser** and **Information Extractor** nodes ensures the data is clean, consistent, and ready for analysis. It outputs perfectly formatted JSON that maps directly to spreadsheet columns.\n\n* ✅ **Scalability via Batching:** The **Split In Batches** and **Loop Over Items** nodes allow the workflow to process a dynamically sized list of workflows. Whether there are 5 or 50 new workflows, it will process each one sequentially without failing.\n\n* ✅ **Effective Data Integration:** It seamlessly integrates with **Google Sheets**, acting as a simple and powerful database. This makes the collected data immediately accessible, shareable, and available for visualization in tools like Looker Studio.\n\n* ✅ **Resilience to Website Changes:** By using AI models trained to understand content and context (like \"find the 'Recently Added' section\" or \"find the author's name\"), the workflow is more resilient to minor cosmetic changes on the target website compared to traditional CSS/XPath selectors.\n\n\n---\n\n### **How It Works**\n\nThe workflow operates in two main phases:\n\n**Phase 1: Scraping the Main List**\n1.  **Trigger:** The workflow can be started manually (\"Execute Workflow\") or automatically on a schedule.\n2.  **Scraping:** The \"Scrape main page\" node (using [ScrapeGraphAI](https://dashboard.scrapegraphai.com/?via=n3witalia)) fetches and converts the `https://n8n.io/workflows/` page into clean Markdown format.\n3.  **Data Extraction:** An LLM chain (\"Extract 'Recently added'\") analyzes the Markdown. It is specifically instructed to identify all workflow titles and URLs within the \"Recently Added\" section and output them as a structured JSON array named `workflows`.\n4.  **Data Preparation:** The resulting array is set as a variable and then split out into individual items, preparing them for processing one-by-one.\n\n**Phase 2: Processing Individual Workflows**\n1.  **Loop:** The \"Loop Over Items\" node iterates through each workflow URL obtained from Phase 1.\n2.  **Scrape & Clean Detail Page:** For each URL, the \"Scrape single Workflow\" node fetches the detail page. Another LLM chain (\"Main content\") cleans the resulting Markdown, removing superfluous content and focusing only on the core article text.\n3.  **Information Extraction:** The cleaned Markdown is passed to an \"Information Extractor\" node. This uses a language model to locate and structure specific data points (title, URL, ID, author, categories, price) into a defined JSON schema.\n4.  **Summarization:** The cleaned Markdown is also sent to a Google Gemini node (\"Summarization content\"), which generates a concise Italian summary of the workflow's purpose and tools used.\n5.  **Data Consolidation & Export:** The extracted information and the generated summary are merged into a single data object. Finally, the \"Add row\" node maps all this data to the appropriate columns and appends it as a new row in a designated Google Sheet.\n\n---\n\n### **Set Up Steps**\n\nTo run this workflow, you need to configure the following credentials in your n8n instance:\n\n1.  **ScrapeGraphAI Account:** The \"Scrape main page\" and \"Scrape single Workflow\" nodes require valid ScrapeGraphAI API credentials named `ScrapegraphAI account`. Install the related Community node.\n2.  **Google Gemini Account:** Multiple nodes (\"Google Gemini Chat Model\", \"Summarization content\", etc.) require API credentials for Google Gemini named `Google Gemini(PaLM) (Eure)`.\n3.  **OpenAI Account:** The \"OpenAI Chat Model1\" node requires API credentials for OpenAI named `OpenAi account (Eure)`.\n4.  **Google Sheets Account:** The \"Add row\" node requires OAuth2 credentials for Google Sheets named `Google Sheets account`. You must also ensure the node is configured with the correct **Google Sheet ID** and that the sheet has a worksheet named `Foglio1` (or update the node to match your sheet's name).\n\n---\n\n### **Need help customizing?**  \n[Contact me](mailto:info@n3w.it) for consulting and support or add me on [Linkedin](https://www.linkedin.com/in/davideboizza/). ",
  "featuredImage": "/data/workflows/8467/8467.webp",
  "author": {
    "id": 101,
    "slug": "n3witalia",
    "name": "Davide",
    "avatar": ""
  },
  "categories": [
    "Document Extraction",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 310,
  "downloads": 31,
  "createdAt": "2025-09-10T22:42:26.181Z",
  "updatedAt": "2026-01-16T08:56:09.347Z",
  "publishedAt": "2025-09-10T22:42:26.181Z",
  "nodes": 21,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/8467",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Extract & store recently added n8n community workflows with ScrapeGraphAI and Gemini",
    "workflowName": "Extract & store recently added n8n community workflows with ScrapeGraphAI and Gemini",
    "description": "This is an exaple of advanced automated data extraction and enrichment pipeline with [ScrapeGraphAI](https://dashboard.scrapegraphai.com/?via=n3witalia). Its primary purpose is to **systematically scrape the n8n community workflows website, extract detailed information** about recently added workflows, process that data using multiple AI models, and store the structured results in a Google Sheets spreadsheet.\n\nThis workflow demonstrates a sophisticated use of n8n to move beyond simple API calls and into the realm of intelligent, AI-driven web scraping and data processing, turning unstructured website content into valuable, structured business intelligence.\n\n\n---\n\n### Key Advantages\n\n* ✅ **Full Automation:** Once triggered (manually or on a schedule via the Schedule Trigger node), the entire process runs hands-free, from data collection to spreadsheet population.\n\n* ✅  **Powerful AI-Augmented Scraping:** It doesn't just scrape raw HTML. It uses multiple AI agents (Google Gemini, OpenAI) to:\n    *   **Understand page structure** to find the right data on the main list.\n    *   **Clean and purify content** from individual pages, removing and irrelevant information.\n    *   **Perform precise information extraction** to parse unstructured text into structured JSON data based on a defined schema (author, price, etc.).\n    *   **Generate intelligent summaries**, adding significant value by explaining the workflow's purpose in Italian.\n\n* ✅ **Robust and Structured Data Output:** The use of the **Structured Output Parser** and **Information Extractor** nodes ensures the data is clean, consistent, and ready for analysis. It outputs perfectly formatted JSON that maps directly to spreadsheet columns.\n\n* ✅ **Scalability via Batching:** The **Split In Batches** and **Loop Over Items** nodes allow the workflow to process a dynamically sized list of workflows. Whether there are 5 or 50 new workflows, it will process each one sequentially without failing.\n\n* ✅ **Effective Data Integration:** It seamlessly integrates with **Google Sheets**, acting as a simple and powerful database. This makes the collected data immediately accessible, shareable, and available for visualization in tools like Looker Studio.\n\n* ✅ **Resilience to Website Changes:** By using AI models trained to understand content and context (like \"find the 'Recently Added' section\" or \"find the author's name\"), the workflow is more resilient to minor cosmetic changes on the target website compared to traditional CSS/XPath selectors.\n\n\n---\n\n### **How It Works**\n\nThe workflow operates in two main phases:\n\n**Phase 1: Scraping the Main List**\n1.  **Trigger:** The workflow can be started manually (\"Execute Workflow\") or automatically on a schedule.\n2.  **Scraping:** The \"Scrape main page\" node (using [ScrapeGraphAI](https://dashboard.scrapegraphai.com/?via=n3witalia)) fetches and converts the `https://n8n.io/workflows/` page into clean Markdown format.\n3.  **Data Extraction:** An LLM chain (\"Extract 'Recently added'\") analyzes the Markdown. It is specifically instructed to identify all workflow titles and URLs within the \"Recently Added\" section and output them as a structured JSON array named `workflows`.\n4.  **Data Preparation:** The resulting array is set as a variable and then split out into individual items, preparing them for processing one-by-one.\n\n**Phase 2: Processing Individual Workflows**\n1.  **Loop:** The \"Loop Over Items\" node iterates through each workflow URL obtained from Phase 1.\n2.  **Scrape & Clean Detail Page:** For each URL, the \"Scrape single Workflow\" node fetches the detail page. Another LLM chain (\"Main content\") cleans the resulting Markdown, removing superfluous content and focusing only on the core article text.\n3.  **Information Extraction:** The cleaned Markdown is passed to an \"Information Extractor\" node. This uses a language model to locate and structure specific data points (title, URL, ID, author, categories, price) into a defined JSON schema.\n4.  **Summarization:** The cleaned Markdown is also sent to a Google Gemini node (\"Summarization content\"), which generates a concise Italian summary of the workflow's purpose and tools used.\n5.  **Data Consolidation & Export:** The extracted information and the generated summary are merged into a single data object. Finally, the \"Add row\" node maps all this data to the appropriate columns and appends it as a new row in a designated Google Sheet.\n\n---\n\n### **Set Up Steps**\n\nTo run this workflow, you need to configure the following credentials in your n8n instance:\n\n1.  **ScrapeGraphAI Account:** The \"Scrape main page\" and \"Scrape single Workflow\" nodes require valid ScrapeGraphAI API credentials named `ScrapegraphAI account`. Install the related Community node.\n2.  **Google Gemini Account:** Multiple nodes (\"Google Gemini Chat Model\", \"Summarization content\", etc.) require API credentials for Google Gemini named `Google Gemini(PaLM) (Eure)`.\n3.  **OpenAI Account:** The \"OpenAI Chat Model1\" node requires API credentials for OpenAI named `OpenAi account (Eure)`.\n4.  **Google Sheets Account:** The \"Add row\" node requires OAuth2 credentials for Google Sheets named `Google Sheets account`. You must also ensure the node is configured with the correct **Google Sheet ID** and that the sheet has a worksheet named `Foglio1` (or update the node to match your sheet's name).\n\n---\n\n### **Need help customizing?**  \n[Contact me](mailto:info@n3w.it) for consulting and support or add me on [Linkedin](https://www.linkedin.com/in/davideboizza/).",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When clicking ‘Execute workflow’",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Google Gemini Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Structured Output Parser",
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "role": "outputParserStructured",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Loop Over Items",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "OpenAI Chat Model1",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Split Out",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Information Extractor",
      "type": "@n8n/n8n-nodes-langchain.informationExtractor",
      "role": "informationExtractor",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Google Gemini Chat Model1",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Merge",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.2"
    },
    {
      "name": "Google Gemini Chat Model2",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Scrape main page",
      "type": "n8n-nodes-scrapegraphai.scrapegraphAi",
      "role": "scrapegraphAi",
      "configDescription": "Version 1"
    },
    {
      "name": "Extract \"Recently added\"",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "role": "chainLlm",
      "configDescription": "Version 1.7"
    },
    {
      "name": "Set array",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Scrape single Workflow",
      "type": "n8n-nodes-scrapegraphai.scrapegraphAi",
      "role": "scrapegraphAi",
      "configDescription": "Version 1"
    },
    {
      "name": "Main content",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "role": "chainLlm",
      "configDescription": "Version 1.7"
    },
    {
      "name": "Summarization content",
      "type": "@n8n/n8n-nodes-langchain.googleGemini",
      "role": "googleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Set content",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Add row",
      "type": "n8n-nodes-base.googleSheets",
      "role": "googleSheets",
      "configDescription": "Version 4.7"
    },
    {
      "name": "Schedule Trigger",
      "type": "n8n-nodes-base.scheduleTrigger",
      "role": "scheduleTrigger",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}