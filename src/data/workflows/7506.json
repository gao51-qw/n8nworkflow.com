{
  "id": 7506,
  "slug": "7506",
  "title": "Auto-respond to Slack messages with GPT and Pinecone Vector RAG context",
  "description": "üõ† GPT-5 + Pinecone-Powered Slack Auto-Responder ‚Äî Real-Time, Context-Aware Replies for IT & Engineering Teams\n\nDescription\nCut down on context-switching and keep your Slack threads moving with an AI agent that responds on your behalf, pulling real-time knowledge from a Pinecone vector database. Built for IT, DevOps, and engineering environments, this n8n workflow ensures every reply is accurate, context-aware, and instantly available‚Äîwithout you lifting a finger.\n\nCheck out step-by-step video build of workflows like these here:\nhttps://www.youtube.com/@automatewithmarc\n\nHow It Works\n\nSlack Listener: Triggers when you‚Äôre mentioned or messaged in relevant channels.\n\nPinecone RAG Retrieval: Pulls the most relevant technical details from your indexed documents, architecture notes, or runbooks.\n\nGPT-5 Processing: Formats the retrieved data into a clear, concise, and technically accurate reply.\n\nThread-Aware Memory: Maintains the conversation state to avoid repeating answers.\n\nSlack Send-as-User: Posts the message under your identity for seamless integration into team workflows.\n\nWhy IT Teams Will Love It\n\nüìö Always up-to-date ‚Äî If your Pinecone index is refreshed with system docs, runbooks, or KB articles, the bot will always deliver the latest info.\n\nüèó Technical context retention ‚Äî Perfect for answering ongoing infrastructure or incident threads.\n\n‚è± Reduced interruption time ‚Äî No more breaking focus to answer ‚Äúquick questions.‚Äù\n\nüîê Controlled outputs ‚Äî Tune GPT-5 to deliver fact-based, low-fluff responses for critical environments.\n\nCommon Use Cases\n\nDevOps: Automated responses to common CI/CD, deployment, or incident queries.\n\nSupport Engineering: Pulling troubleshooting steps directly from KB entries.\n\nProject Coordination: Instant status updates pulled from sprint or release notes.\n\nPro Tips for Deployment\n\nKeep your Pinecone vector DB updated with the latest architecture diagrams, release notes, and SOPs.\n\nUse embeddings tuned for technical documentation to improve retrieval accuracy.\n\nAdd channel-specific prompts if different teams require different response styles (e.g., #devops vs #product).",
  "featuredImage": "/data/workflows/7506/7506.webp",
  "author": {
    "id": 101,
    "slug": "marconi",
    "name": "Automate With Marc",
    "avatar": ""
  },
  "categories": [
    "AI RAG",
    "Multimodal AI"
  ],
  "complexityLevel": "intermediate",
  "price": 0,
  "visitors": 604,
  "downloads": 60,
  "createdAt": "2025-08-17T09:12:28.372Z",
  "updatedAt": "2026-01-16T08:50:39.077Z",
  "publishedAt": "2025-08-17T09:12:28.372Z",
  "nodes": 11,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/7506",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Auto-respond to Slack messages with GPT and Pinecone Vector RAG context",
    "workflowName": "Auto-respond to Slack messages with GPT and Pinecone Vector RAG context",
    "description": "üõ† GPT-5 + Pinecone-Powered Slack Auto-Responder ‚Äî Real-Time, Context-Aware Replies for IT & Engineering Teams\n\nDescription\nCut down on context-switching and keep your Slack threads moving with an AI agent that responds on your behalf, pulling real-time knowledge from a Pinecone vector database. Built for IT, DevOps, and engineering environments, this n8n workflow ensures every reply is accurate, context-aware, and instantly available‚Äîwithout you lifting a finger.\n\nCheck out step-by-step video build of workflows like these here:\nhttps://www.youtube.com/@automatewithmarc\n\nHow It Works\n\nSlack Listener: Triggers when you‚Äôre mentioned or messaged in relevant channels.\n\nPinecone RAG Retrieval: Pulls the most relevant technical details from your indexed documents, architecture notes, or runbooks.\n\nGPT-5 Processing: Formats the retrieved data into a clear, concise, and technically accurate reply.\n\nThread-Aware Memory: Maintains the conversation state to avoid repeating answers.\n\nSlack Send-as-User: Posts the message under your identity for seamless integration into team workflows.\n\nWhy IT Teams Will Love It\n\nüìö Always up-to-date ‚Äî If your Pinecone index is refreshed with system docs, runbooks, or KB articles, the bot will always deliver the latest info.\n\nüèó Technical context retention ‚Äî Perfect for answering ongoing infrastructure or incident threads.\n\n‚è± Reduced interruption time ‚Äî No more breaking focus to answer ‚Äúquick questions.‚Äù\n\nüîê Controlled outputs ‚Äî Tune GPT-5 to deliver fact-based, low-fluff responses for critical environments.\n\nCommon Use Cases\n\nDevOps: Automated responses to common CI/CD, deployment, or incident queries.\n\nSupport Engineering: Pulling troubleshooting steps directly from KB entries.\n\nProject Coordination: Instant status updates pulled from sprint or release notes.\n\nPro Tips for Deployment\n\nKeep your Pinecone vector DB updated with the latest architecture diagrams, release notes, and SOPs.\n\nUse embeddings tuned for technical documentation to improve retrieval accuracy.\n\nAdd channel-specific prompts if different teams require different response styles (e.g., #devops vs #product).",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Slack Trigger",
      "type": "n8n-nodes-base.slackTrigger",
      "role": "slackTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Simple Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Send a message",
      "type": "n8n-nodes-base.slack",
      "role": "slack",
      "configDescription": "Version 2.3"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "GPT 5 Slack Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 2"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Pinecone Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStorePinecone",
      "role": "vectorStorePinecone",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Embeddings OpenAI",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}