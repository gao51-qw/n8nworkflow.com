{
  "id": 8138,
  "slug": "8138",
  "title": "Filter URLs with AI-powered robots.txt compliance & source verification",
  "description": "# URL Officer - Respect robots.txt and Avoid Undesirable Sources\n\n## üé¨ Overview\n**Version** : 1.0\n\nThe URL Officer workflow automates the filtering of URLs by checking them against a database of forbidden sources and the rules defined in robots.txt files. It proactively respects robot exclusion protocols and user-defined banned sources to aid in lawful and ethical web automation. Designed primarily as a sub-workflow, it serves automation pipelines with robust URL validation to avoid undesirable or restricted sources.\n\n\n## ‚ú® Features\n\n- **Dual-layer URL Filtering**: Checks URLs against a manually maintained forbidden sources list and robots.txt restrictions.\n- **Automated robots.txt Retrieval & Update**: Automatically fetches and updates robots.txt content for new or outdated sources (older than 3 days).\n- **AI-backed robots.txt Interpretation**: Uses AI models to interpret robots.txt comments and restrictions, ensuring nuanced compliance.\n- **Configurable User-Agent Identification**: Allows customization of User-Agent strings that are checked against robots.txt directives.\n- **Sub-Workflow Ready**: Easily integrates as a sub-workflow for link validation in larger automation pipelines.\n- **Multi-Model AI Support**: Supports mistral, groq, and gemini AI models for enhanced robots.txt compliance checks.\n- **Detailed Diagnostic Outputs**: Returns comprehensive link allowance statuses and metadata for use in downstream processing.\n- **Database Integration**: Utilizes PostgreSQL to store and manage robots.txt content and banned source lists.\n\n\n## üë§ Who is this for?\n\nIdeal for developers, data engineers, researchers, or businesses implementing web crawlers, scrapers, or any automation that processes URLs. This workflow helps your compliance with source restrictions and avoids content from blacklisted sites, reducing legal exposure and promoting ethical data use.\n\n\n## üí° What problem does this solve?\n\nURL Officer addresses the challenge of automating URL validation by combining manual blacklist filtering with automated and AI-assisted robots.txt parsing. It prevents accidental scraping or processing from undesirable or disallowed sources, helping automate respect for webmasters' policies and legal boundaries.\n\n\n## üîç What this workflow does\n\nWhen given a URL, the workflow:\n\n- Extracts the base URL.\n- Checks the URL against a manually configured banned sources list (stored in database).\n- Fetches robots.txt for new or stale sources (older than 3 days).\n- Performs a programmatic parse and check of robots.txt directives against the URL using the specified User-Agent.\n- Runs an AI model to analyze robots.txt content and confirm if the URL is allowed, taking into account any special comments or prohibitions relevant to the automation goal.\n- Returns a final \"allow or disallow\" determination for both the URL and its base URL, along with metadata about the robots.txt fetch status and timing.\n\n\n## üîÑ Workflow Steps\n\n### 1. Input Parsing & Base URL Extraction\n\n- Accepts workflow arguments including the URL, User-Agent information, automation goal, and AI model choice.\n- Extracts and normalizes the base URL for processing.\n\n### 2. Forbidden Source Check\n\n- Queries PostgreSQL tables containing banned sources.\n- Immediately rejects URLs matching forbidden sources.\n\n### 3. robots.txt Handling\n\n- Checks if robots.txt content for the source is in the database and is recent (under 3 days old).\n- If missing or outdated, fetches the robots.txt file from the base URL and updates the database.\n\n### 4. Code-Based robots.txt Analysis\n\n- Parses robots.txt directives, matching the User-Agent to appropriate groups.\n- Checks if the URL and base URL paths are allowed according to the parsed rules.\n- Uses a conservative URL and agent matching algorithm for prefix-based allow/disallow checks.\n\n### 5. AI-Based robots.txt Verification\n\n- Uses the selected AI model (mistral, groq, or gemini) to analyze robots.txt content and comments regarding allowed automation use.\n- Applies AI understanding to confirm or override automated code checks based on the automation's goal.\n\n### 6. Output Preparation\n\n- Produces output indicating permission statuses (`allow_link` and `allow_baseUrl`), original URLs, User-Agent info, fetch timestamps, and whether robots.txt was successfully retrieved.\n- Designed to be consumed by other workflows as a validation step.\n\n\n## üîÄ Expected Input / Configuration\n\nThe workflow is configured primarily via workflow input arguments:\n\n| Parameter         | Description                                                                                           | Type    |\n|-------------------|---------------------------------------------------------------------------------------------------|---------|\n| `link`            | The URL to be checked.                                                                              | String  |\n| `userAgent`       | User-Agent string representing your automation, used for robots.txt checks.                        | String  |\n| `userAgent_extra` | Additional User-Agent information such as version or contact info.                                 | String  |\n| `automationGoal`  | Description of your automation‚Äôs purpose, used by the AI to verify suitability against robots.txt. | String  |\n| `model`           | AI model to use for the robots.txt compliance check. Options: mistral, groq, gemini.               | String  |\n\n### Database Requirements\n\n- PostgreSQL database configured with credentials accessible to the workflow.\n- Two tables: one for banned sources (manually maintained) and one for robots.txt content with timestamps.\n- The workflow auto-creates and manages these tables.\n- Recommended to use a containerized PostgreSQL instance (Podman or Docker).\n\n\n## üì¶ Expected Output\n\nA structured JSON object containing:\n\n| Output Key      | Description                                                     |\n|-----------------|-----------------------------------------------------------------|\n| `link`          | The URL that was checked.                                       |\n| `baseUrl`       | The base URL of the checked link.                              |\n| `allow_link`    | Boolean indicating if the link is allowed according to checks.|\n| `allow_baseUrl` | Boolean indicating if the base URL is allowed.                |\n| `userAgent`     | User-Agent string used in the check.                            |\n| `userAgent_extra`| Additional User-Agent metadata.                                |\n| `robots_fetched`| Boolean, true if robots.txt content was successfully fetched.  |\n| `fetched_at`    | Timestamp of the last robots.txt content fetch.                |\n\n\n## üìå Example\n\nExample input payload:\n![printscreenurlofficer_example.png](fileId:2295)\n![printscreen1.png](fileId:2296)\n![printscreen2.png](fileId:2297)\n![printscreen3.png](fileId:2298)\n\n\n## ‚öôÔ∏è n8n Setup Used\n\n- **n8n version:** 1.108.2  \n- **Platform:** Podman 4.3.1 on Linux  \n- **PostgreSQL:** Running in Podman 4.3.1 container  \n- **LLM Models:** mistral-small-latest, llama-3.1-8b-instant (Groq), gemini-2.5-flash  \n- **Date:** 2025-08-29  \n\n\n## ‚ö° Requirements to Use / Setup\n\n- Self-hosted or cloud n8n instance with database connectivity.\n- PostgreSQL database configured and accessible by n8n.\n- Setup PostgreSQL using the recommended containerized deployment or your preferred method.\n- Configure database credentials inside the workflow.\n- Provide API credentials for your chosen AI model (mistral, groq, gemini).\n- Manually maintain the banned sources list in the database.\n- Familiarity with n8n variables and sub-workflow integration is recommended.\n- Internet connectivity for fetching robots.txt files.\n\n\n## ‚ö†Ô∏è Notes, Assumptions & Warnings\n\n- Database tables used by this workflow are automatically created and managed by the workflow.\n- robots.txt refresh interval is set to every 3 days; this can be adjusted by modifying the workflow.\n- The robots.txt parser is relatively simple and does not support wildcard (*) or end-of-string ($) rules.\n- User-Agent matching is substring-based and longer string matches take precedence.\n- AI analysis adds a human-like understanding of robots.txt comments and prohibitions but depends on the quality and capability of the chosen AI model.\n- This workflow does NOT handle:\n  - Terms of Service compliance.\n  - Preference for official APIs over HTML scraping.\n  - Rate-limiting or request throttling.\n  - Handling paywalled or restricted content.\n  - De-duplication or filtering beyond the banned sources list.\n  - Encryption or secure storage.\n- You remain responsible for ensuring your automation complies with legal, ethical, and platform-specific rules.\n- The workflow is designed as a sub-workflow; integrate it into larger automation processes to validate URLs.\n\n\n## üõ† PostgreSQL Setup Instructions (Self-Hosted Route)\n\nAvailable inside the Workflow Notes, alongside podman commands.\n\n\n## ‚ÑπÔ∏è About Us\n\nThis workflow was developed by the Hybroht team. Our goal is to create tools that harness the possibilities of technology and more. We aim to continuously improve and expand functionalities based on community feedback and evolving use cases.\n\n\nFor questions, support, or feedback, please contact us at: [contact@hybroht.com](mailto:contact@hybroht.com)\n\n---\n\n### ‚öñÔ∏è Warranty & Legal Notice\n\nThis workflow is provided \"as-is\" without warranties of any kind. By using this workflow, you agree that you are responsible for complying with all applicable laws, regulations, and terms of service related to your data sources and automations. Please review all relevant legal terms and use this workflow responsibly.\n\nHybroht disclaims any liability arising from use or misuse of this workflow. This tool assists with robots.txt compliance but is not a substitute for full legal or compliance advice.\n\nYou can view the full license terms [here](https://hybroht.com/docs/policies/licenses/n8n_workflows/general_workflow_license_v1_0.md). Please review them before making your purchase.\n\n**By purchasing this product, you agree to these terms.**\n\n---\n",
  "featuredImage": "/data/workflows/8138/8138.webp",
  "author": {
    "id": 101,
    "slug": "hybroht",
    "name": "Hybroht",
    "avatar": ""
  },
  "categories": [
    "SecOps",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 29,
  "downloads": 2,
  "createdAt": "2025-09-01T22:26:47.327Z",
  "updatedAt": "2026-01-16T08:54:20.283Z",
  "publishedAt": "2025-09-01T22:26:47.327Z",
  "nodes": 70,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/8138",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Filter URLs with AI-powered robots.txt compliance & source verification",
    "workflowName": "Filter URLs with AI-powered robots.txt compliance & source verification",
    "description": "# URL Officer - Respect robots.txt and Avoid Undesirable Sources\n\n## üé¨ Overview\n**Version** : 1.0\n\nThe URL Officer workflow automates the filtering of URLs by checking them against a database of forbidden sources and the rules defined in robots.txt files. It proactively respects robot exclusion protocols and user-defined banned sources to aid in lawful and ethical web automation. Designed primarily as a sub-workflow, it serves automation pipelines with robust URL validation to avoid undesirable or restricted sources.\n\n\n## ‚ú® Features\n\n- **Dual-layer URL Filtering**: Checks URLs against a manually maintained forbidden sources list and robots.txt restrictions.\n- **Automated robots.txt Retrieval & Update**: Automatically fetches and updates robots.txt content for new or outdated sources (older than 3 days).\n- **AI-backed robots.txt Interpretation**: Uses AI models to interpret robots.txt comments and restrictions, ensuring nuanced compliance.\n- **Configurable User-Agent Identification**: Allows customization of User-Agent strings that are checked against robots.txt directives.\n- **Sub-Workflow Ready**: Easily integrates as a sub-workflow for link validation in larger automation pipelines.\n- **Multi-Model AI Support**: Supports mistral, groq, and gemini AI models for enhanced robots.txt compliance checks.\n- **Detailed Diagnostic Outputs**: Returns comprehensive link allowance statuses and metadata for use in downstream processing.\n- **Database Integration**: Utilizes PostgreSQL to store and manage robots.txt content and banned source lists.\n\n\n## üë§ Who is this for?\n\nIdeal for developers, data engineers, researchers, or businesses implementing web crawlers, scrapers, or any automation that processes URLs. This workflow helps your compliance with source restrictions and avoids content from blacklisted sites, reducing legal exposure and promoting ethical data use.\n\n\n## üí° What problem does this solve?\n\nURL Officer addresses the challenge of automating URL validation by combining manual blacklist filtering with automated and AI-assisted robots.txt parsing. It prevents accidental scraping or processing from undesirable or disallowed sources, helping automate respect for webmasters' policies and legal boundaries.\n\n\n## üîç What this workflow does\n\nWhen given a URL, the workflow:\n\n- Extracts the base URL.\n- Checks the URL against a manually configured banned sources list (stored in database).\n- Fetches robots.txt for new or stale sources (older than 3 days).\n- Performs a programmatic parse and check of robots.txt directives against the URL using the specified User-Agent.\n- Runs an AI model to analyze robots.txt content and confirm if the URL is allowed, taking into account any special comments or prohibitions relevant to the automation goal.\n- Returns a final \"allow or disallow\" determination for both the URL and its base URL, along with metadata about the robots.txt fetch status and timing.\n\n\n## üîÑ Workflow Steps\n\n### 1. Input Parsing & Base URL Extraction\n\n- Accepts workflow arguments including the URL, User-Agent information, automation goal, and AI model choice.\n- Extracts and normalizes the base URL for processing.\n\n### 2. Forbidden Source Check\n\n- Queries PostgreSQL tables containing banned sources.\n- Immediately rejects URLs matching forbidden sources.\n\n### 3. robots.txt Handling\n\n- Checks if robots.txt content for the source is in the database and is recent (under 3 days old).\n- If missing or outdated, fetches the robots.txt file from the base URL and updates the database.\n\n### 4. Code-Based robots.txt Analysis\n\n- Parses robots.txt directives, matching the User-Agent to appropriate groups.\n- Checks if the URL and base URL paths are allowed according to the parsed rules.\n- Uses a conservative URL and agent matching algorithm for prefix-based allow/disallow checks.\n\n### 5. AI-Based robots.txt Verification\n\n- Uses the selected AI model (mistral, groq, or gemini) to analyze robots.txt content and comments regarding allowed automation use.\n- Applies AI understanding to confirm or override automated code checks based on the automation's goal.\n\n### 6. Output Preparation\n\n- Produces output indicating permission statuses (`allow_link` and `allow_baseUrl`), original URLs, User-Agent info, fetch timestamps, and whether robots.txt was successfully retrieved.\n- Designed to be consumed by other workflows as a validation step.\n\n\n## üîÄ Expected Input / Configuration\n\nThe workflow is configured primarily via workflow input arguments:\n\n| Parameter         | Description                                                                                           | Type    |\n|-------------------|---------------------------------------------------------------------------------------------------|---------|\n| `link`            | The URL to be checked.                                                                              | String  |\n| `userAgent`       | User-Agent string representing your automation, used for robots.txt checks.                        | String  |\n| `userAgent_extra` | Additional User-Agent information such as version or contact info.                                 | String  |\n| `automationGoal`  | Description of your automation‚Äôs purpose, used by the AI to verify suitability against robots.txt. | String  |\n| `model`           | AI model to use for the robots.txt compliance check. Options: mistral, groq, gemini.               | String  |\n\n### Database Requirements\n\n- PostgreSQL database configured with credentials accessible to the workflow.\n- Two tables: one for banned sources (manually maintained) and one for robots.txt content with timestamps.\n- The workflow auto-creates and manages these tables.\n- Recommended to use a containerized PostgreSQL instance (Podman or Docker).\n\n\n## üì¶ Expected Output\n\nA structured JSON object containing:\n\n| Output Key      | Description                                                     |\n|-----------------|-----------------------------------------------------------------|\n| `link`          | The URL that was checked.                                       |\n| `baseUrl`       | The base URL of the checked link.                              |\n| `allow_link`    | Boolean indicating if the link is allowed according to checks.|\n| `allow_baseUrl` | Boolean indicating if the base URL is allowed.                |\n| `userAgent`     | User-Agent string used in the check.                            |\n| `userAgent_extra`| Additional User-Agent metadata.                                |\n| `robots_fetched`| Boolean, true if robots.txt content was successfully fetched.  |\n| `fetched_at`    | Timestamp of the last robots.txt content fetch.                |\n\n\n## üìå Example\n\nExample input payload:\n![printscreenurlofficer_example.png](fileId:2295)\n![printscreen1.png](fileId:2296)\n![printscreen2.png](fileId:2297)\n![printscreen3.png](fileId:2298)\n\n\n## ‚öôÔ∏è n8n Setup Used\n\n- **n8n version:** 1.108.2  \n- **Platform:** Podman 4.3.1 on Linux  \n- **PostgreSQL:** Running in Podman 4.3.1 container  \n- **LLM Models:** mistral-small-latest, llama-3.1-8b-instant (Groq), gemini-2.5-flash  \n- **Date:** 2025-08-29  \n\n\n## ‚ö° Requirements to Use / Setup\n\n- Self-hosted or cloud n8n instance with database connectivity.\n- PostgreSQL database configured and accessible by n8n.\n- Setup PostgreSQL using the recommended containerized deployment or your preferred method.\n- Configure database credentials inside the workflow.\n- Provide API credentials for your chosen AI model (mistral, groq, gemini).\n- Manually maintain the banned sources list in the database.\n- Familiarity with n8n variables and sub-workflow integration is recommended.\n- Internet connectivity for fetching robots.txt files.\n\n\n## ‚ö†Ô∏è Notes, Assumptions & Warnings\n\n- Database tables used by this workflow are automatically created and managed by the workflow.\n- robots.txt refresh interval is set to every 3 days; this can be adjusted by modifying the workflow.\n- The robots.txt parser is relatively simple and does not support wildcard (*) or end-of-string ($) rules.\n- User-Agent matching is substring-based and longer string matches take precedence.\n- AI analysis adds a human-like understanding of robots.txt comments and prohibitions but depends on the quality and capability of the chosen AI model.\n- This workflow does NOT handle:\n  - Terms of Service compliance.\n  - Preference for official APIs over HTML scraping.\n  - Rate-limiting or request throttling.\n  - Handling paywalled or restricted content.\n  - De-duplication or filtering beyond the banned sources list.\n  - Encryption or secure storage.\n- You remain responsible for ensuring your automation complies with legal, ethical, and platform-specific rules.\n- The workflow is designed as a sub-workflow; integrate it into larger automation processes to validate URLs.\n\n\n## üõ† PostgreSQL Setup Instructions (Self-Hosted Route)\n\nAvailable inside the Workflow Notes, alongside podman commands.\n\n\n## ‚ÑπÔ∏è About Us\n\nThis workflow was developed by the Hybroht team. Our goal is to create tools that harness the possibilities of technology and more. We aim to continuously improve and expand functionalities based on community feedback and evolving use cases.\n\n\nFor questions, support, or feedback, please contact us at: [contact@hybroht.com](mailto:contact@hybroht.com)\n\n---\n\n### ‚öñÔ∏è Warranty & Legal Notice\n\nThis workflow is provided \"as-is\" without warranties of any kind. By using this workflow, you agree that you are responsible for complying with all applicable laws, regulations, and terms of service related to your data sources and automations. Please review all relevant legal terms and use this workflow responsibly.\n\nHybroht disclaims any liability arising from use or misuse of this workflow. This tool assists with robots.txt compliance but is not a substitute for full legal or compliance advice.\n\nYou can view the full license terms [here](https://hybroht.com/docs/policies/licenses/n8n_workflows/general_workflow_license_v1_0.md). Please review them before making your purchase.\n\n**By purchasing this product, you agree to these terms.**\n\n---",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Schedule Trigger",
      "type": "n8n-nodes-base.scheduleTrigger",
      "role": "scheduleTrigger",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Get Base URL",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Prepare Robots.txt Check",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Assume Disallow",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Information Extractor",
      "type": "@n8n/n8n-nodes-langchain.informationExtractor",
      "role": "informationExtractor",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Model Selector",
      "type": "@n8n/n8n-nodes-langchain.modelSelector",
      "role": "modelSelector",
      "configDescription": "Version 1"
    },
    {
      "name": "Mistral Cloud Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatMistralCloud",
      "role": "lmChatMistralCloud",
      "configDescription": "Version 1"
    },
    {
      "name": "Groq Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatGroq",
      "role": "lmChatGroq",
      "configDescription": "Version 1"
    },
    {
      "name": "Google Gemini Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Get Robots.txt",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Check Robots.txt",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Prepare Robots.txt Check 2",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "If Link Allowed",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Prepare Output for Link Disallowed",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "If Link Allowed 2",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Create robots.txt Table",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "Check robots.txt Table",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "If robots.txt Found and Updated",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Output",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Prepare Output",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Prepare Output for Link Allowed",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Upsert robots.txt Table",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "Delete robots.txt Table",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "Start",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "role": "executeWorkflowTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Check all robots.txt Table",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "Assume False Output",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Create forbidden urls table",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "Check forbidden urls table",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "If forbidden url Detected",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Return Disallow",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Check all forbidden urls Table",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "Delete forbidden urls Table",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Manual Upsert robots.txt Table",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "Manual Upsert forbidden urls Table",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "Start (Tests)",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Manual Create forbidden urls table",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note7",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note15",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note31",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note34",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note9",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note10",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note11",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note12",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note13",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note28",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Manually Create robots.txt Table",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "Sticky Note29",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note30",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note32",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note33",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note35",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note36",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note37",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note38",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note39",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note40",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note41",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note42",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note43",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note44",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note14",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}