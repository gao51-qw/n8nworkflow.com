{
  "id": 8272,
  "slug": "8272",
  "title": "Dynamic MCP server selection with OpenAI GPT-4.1 and contextual AI reranker",
  "description": "## PROBLEM\nThousands of MCP Servers exist and many are updated daily, making server selection difficult for LLMs.\n- Current approaches require manually downloading and configuring servers, limiting flexibility.\n- When multiple servers are pre-configured, LLMs get overwhelmed and confused about which server to use for specific tasks.\n\n### This template enables dynamic server selection from a live PulseMCP directory of 5000+ servers.\n\n## How it works\n- A user query goes to an LLM that decides whether to use MCP servers to fulfill a given query and provides reasoning for its decision.\n- Next, we fetch MCP Servers from Pulse MCP API and format them as documents for reranking\n- Now, we use Contextual AI's Reranker to score and rank all MCP Servers based on our query and instructions\n\n## How to set up\n- Sign up for a free trial of Contextual AI [here](https://app.contextual.ai/) to find CONTEXTUALAI_API_KEY.\n- Click on variables option in left panel and add a new environment variable CONTEXTUALAI_API_KEY.\n- For the baseline model, we have used GPT 4.1 mini, you can find your OpenAI API key[ here](https://platform.openai.com/api-keys)\n\n## How to customize the workflow\n- We use chat trigger to initate the workflow. Feel free to replace it with a webhook or other trigger as required.\n- We use OpenAI's GPT 4.1 mini as the baseline model and reranker prompt generator. You can swap out this section to use the LLM of your choice.\n- We fetch 5000 MCP Servers from the PulseMCP directory as a baseline number, feel free to adjust this parameter as required.\n- We are using Contextual AI's ctxl-rerank-v2-instruct-multilingual reranker model, which can be swapped with any one of the following rerankers: \n  1) ctxl-rerank-v2-instruct-multilingual\n  2) ctxl-rerank-v2-instruct-multilingual-mini\n  3)  ctxl-rerank-v1-instruct\n- You can checkout this [blog](https://contextual.ai/blog/context-engineering-for-your-mcp-client/) for more information about rerankers to learn more about them.\n\n## Good to know:\n- Contextual AI Reranker (with full MCP docs): ~$0.035/query\nIncludes 0.035 for reranking + ~$0.0001 for OpenAI instruction generation.\n- OpenAI Baseline: ~$0.017/query\n",
  "featuredImage": "/data/workflows/8272/8272.webp",
  "author": {
    "id": 101,
    "slug": "jinash",
    "name": "Jinash Rouniyar",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 285,
  "downloads": 28,
  "createdAt": "2025-09-05T03:48:50.424Z",
  "updatedAt": "2026-01-16T08:55:03.472Z",
  "publishedAt": "2025-09-05T03:48:50.424Z",
  "nodes": 16,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/8272",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Dynamic MCP server selection with OpenAI GPT-4.1 and contextual AI reranker",
    "workflowName": "Dynamic MCP server selection with OpenAI GPT-4.1 and contextual AI reranker",
    "description": "## PROBLEM\nThousands of MCP Servers exist and many are updated daily, making server selection difficult for LLMs.\n- Current approaches require manually downloading and configuring servers, limiting flexibility.\n- When multiple servers are pre-configured, LLMs get overwhelmed and confused about which server to use for specific tasks.\n\n### This template enables dynamic server selection from a live PulseMCP directory of 5000+ servers.\n\n## How it works\n- A user query goes to an LLM that decides whether to use MCP servers to fulfill a given query and provides reasoning for its decision.\n- Next, we fetch MCP Servers from Pulse MCP API and format them as documents for reranking\n- Now, we use Contextual AI's Reranker to score and rank all MCP Servers based on our query and instructions\n\n## How to set up\n- Sign up for a free trial of Contextual AI [here](https://app.contextual.ai/) to find CONTEXTUALAI_API_KEY.\n- Click on variables option in left panel and add a new environment variable CONTEXTUALAI_API_KEY.\n- For the baseline model, we have used GPT 4.1 mini, you can find your OpenAI API key[ here](https://platform.openai.com/api-keys)\n\n## How to customize the workflow\n- We use chat trigger to initate the workflow. Feel free to replace it with a webhook or other trigger as required.\n- We use OpenAI's GPT 4.1 mini as the baseline model and reranker prompt generator. You can swap out this section to use the LLM of your choice.\n- We fetch 5000 MCP Servers from the PulseMCP directory as a baseline number, feel free to adjust this parameter as required.\n- We are using Contextual AI's ctxl-rerank-v2-instruct-multilingual reranker model, which can be swapped with any one of the following rerankers: \n  1) ctxl-rerank-v2-instruct-multilingual\n  2) ctxl-rerank-v2-instruct-multilingual-mini\n  3)  ctxl-rerank-v1-instruct\n- You can checkout this [blog](https://contextual.ai/blog/context-engineering-for-your-mcp-client/) for more information about rerankers to learn more about them.\n\n## Good to know:\n- Contextual AI Reranker (with full MCP docs): ~$0.035/query\nIncludes 0.035 for reranking + ~$0.0001 for OpenAI instruction generation.\n- OpenAI Baseline: ~$0.017/query",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "If",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Merge",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.2"
    },
    {
      "name": "Merge1",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.2"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "User-Query",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.3"
    },
    {
      "name": "LLM Agent for Decision-Making",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 2.2"
    },
    {
      "name": "PulseMCP Fetch MCP Servers",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Final Response1",
      "type": "@n8n/n8n-nodes-langchain.chat",
      "role": "chat",
      "configDescription": "Version 1"
    },
    {
      "name": "Parse MCP Server list into documents w metadata",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Format the top 5 results",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Final Response2",
      "type": "@n8n/n8n-nodes-langchain.chat",
      "role": "chat",
      "configDescription": "Version 1"
    },
    {
      "name": "Rerank documents",
      "type": "n8n-nodes-contextualai.contextualAi",
      "role": "contextualAi",
      "configDescription": "Version 1"
    }
  ]
}