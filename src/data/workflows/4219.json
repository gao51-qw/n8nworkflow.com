{
  "id": 4219,
  "slug": "4219",
  "title": "Create AI-ready vector datasets from web content with Claude, Ollama & Qdrant",
  "description": "# AI-Powered Web Data Pipeline with n8n\n\n![f70ca7d4da7544ce92333c57c5e62954.png](fileId:1324)\n## How It Works\n\nThis `n8n` workflow builds an **AI-powered web data pipeline** that automates the entire process of:\n\n- **Extraction**\n- **Structuring**\n- **Vectorization**\n- **Storage**\n\nIt integrates multiple advanced tools to transform messy web pages into clean, searchable vector databases.\n\n###  Integrated Tools\n\n- **Scrapeless**  \n  Bypasses JavaScript-heavy websites and anti-bot protections to reliably extract HTML content.\n\n- **Claude AI**  \n  Uses LLMs to analyze unstructured HTML and generate clean, structured JSON data.\n\n- **Ollama Embeddings**  \n  Generates local vector embeddings from structured text using the `all-minilm` model.\n\n- **Qdrant Vector DB**  \n  Stores semantic vector data for fast and meaningful search capabilities.\n\n- **Webhook Notifications**  \n  Sends real-time updates when workflows complete or errors occur.\n\nFrom messy webpages to structured vector data — this pipeline is perfect for building intelligent agents, knowledge bases, or research automation tools.\n\n---\n\n##  Setup Steps\n\n### 1. Install n8n\n\n&gt; Requires Node.js v18 / v20 / v22\n\n```\nnpm install -g n8n\nn8n\n```\nAfter installation, access the n8n interface via:\n\n**URL:** [http://localhost:5678](http://localhost:5678)\n\n---\n\n### 2. Set Up Scrapeless\n\n1. Register at: [Scrapeless](https://app.scrapeless.com/passport/login?utm_source=n8n&&utm_campaign=webdatapipe)  \n2. Copy your **API token**  \n3. Paste the token into the `HTTP Request` node labeled **\"Scrapeless Web Request\"**\n\n---\n\n### 3. Set Up Claude API (Anthropic)\n\n1. Sign up at Anthropic Console\n2. Generate your **Claude API key**  \n3. Add the API key to the following nodes:\n   - `Claude Extractor`\n   - `AI Data Checker`\n   - `Claude AI Agent`\n\n---\n\n### 4. Install and Run Ollama\n\n#### macOS\n\n```\nbrew install ollama\n\n```\n\n\n####  Linux\n\n```\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n**Windows**\nDownload the installer from: https://ollama.com\n\n#### Start Ollama Server\n```\nollama serve\n```\n#### Pull Embedding Model\n```\nollama pull all-minilm\n```\n### 5. Install Qdrant (via Docker)\n```\ndocker pull qdrant/qdrant\n\ndocker run -d \\\n  --name qdrant-server \\\n  -p 6333:6333 -p 6334:6334 \\\n  -v $(pwd)/qdrant_storage:/qdrant/storage \\\n  qdrant/qdrant\n```\n Test if Qdrant is running:\n```\ncurl http://localhost:6333/healthz\n```\n\n### 6. Configure the n8n Workflow\n - Modify the Trigger (Manual or Scheduled)\n\n - Input your Target URLs and Collection Name in the designated nodes\n\n - Paste all required API Tokens / Keys into their corresponding nodes\n\n - Ensure your Qdrant and Ollama services are running\n\n## Ideal Use Cases\n - Custom AI Chatbots\n\n - Private Search Engines\n\n - Research Tools\n\n - Internal Knowledge Bases\n\n - Content Monitoring Pipelines\n",
  "featuredImage": "/data/workflows/4219/4219.webp",
  "author": {
    "id": 101,
    "slug": "scrapelessofficial",
    "name": "scrapeless official",
    "avatar": ""
  },
  "categories": [
    "Document Extraction",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 1316,
  "downloads": 131,
  "createdAt": "2025-05-19T09:06:47.948Z",
  "updatedAt": "2026-01-16T08:33:22.146Z",
  "publishedAt": "2025-05-19T09:06:47.948Z",
  "nodes": 20,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/4219",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Create AI-ready vector datasets from web content with Claude, Ollama & Qdrant",
    "workflowName": "Create AI-ready vector datasets from web content with Claude, Ollama & Qdrant",
    "description": "# AI-Powered Web Data Pipeline with n8n\n\n![f70ca7d4da7544ce92333c57c5e62954.png](fileId:1324)\n## How It Works\n\nThis `n8n` workflow builds an **AI-powered web data pipeline** that automates the entire process of:\n\n- **Extraction**\n- **Structuring**\n- **Vectorization**\n- **Storage**\n\nIt integrates multiple advanced tools to transform messy web pages into clean, searchable vector databases.\n\n###  Integrated Tools\n\n- **Scrapeless**  \n  Bypasses JavaScript-heavy websites and anti-bot protections to reliably extract HTML content.\n\n- **Claude AI**  \n  Uses LLMs to analyze unstructured HTML and generate clean, structured JSON data.\n\n- **Ollama Embeddings**  \n  Generates local vector embeddings from structured text using the `all-minilm` model.\n\n- **Qdrant Vector DB**  \n  Stores semantic vector data for fast and meaningful search capabilities.\n\n- **Webhook Notifications**  \n  Sends real-time updates when workflows complete or errors occur.\n\nFrom messy webpages to structured vector data — this pipeline is perfect for building intelligent agents, knowledge bases, or research automation tools.\n\n---\n\n##  Setup Steps\n\n### 1. Install n8n\n\n&gt; Requires Node.js v18 / v20 / v22\n\n```\nnpm install -g n8n\nn8n\n```\nAfter installation, access the n8n interface via:\n\n**URL:** [http://localhost:5678](http://localhost:5678)\n\n---\n\n### 2. Set Up Scrapeless\n\n1. Register at: [Scrapeless](https://app.scrapeless.com/passport/login?utm_source=n8n&&utm_campaign=webdatapipe)  \n2. Copy your **API token**  \n3. Paste the token into the `HTTP Request` node labeled **\"Scrapeless Web Request\"**\n\n---\n\n### 3. Set Up Claude API (Anthropic)\n\n1. Sign up at Anthropic Console\n2. Generate your **Claude API key**  \n3. Add the API key to the following nodes:\n   - `Claude Extractor`\n   - `AI Data Checker`\n   - `Claude AI Agent`\n\n---\n\n### 4. Install and Run Ollama\n\n#### macOS\n\n```\nbrew install ollama\n\n```\n\n\n####  Linux\n\n```\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n**Windows**\nDownload the installer from: https://ollama.com\n\n#### Start Ollama Server\n```\nollama serve\n```\n#### Pull Embedding Model\n```\nollama pull all-minilm\n```\n### 5. Install Qdrant (via Docker)\n```\ndocker pull qdrant/qdrant\n\ndocker run -d \\\n  --name qdrant-server \\\n  -p 6333:6333 -p 6334:6334 \\\n  -v $(pwd)/qdrant_storage:/qdrant/storage \\\n  qdrant/qdrant\n```\n Test if Qdrant is running:\n```\ncurl http://localhost:6333/healthz\n```\n\n### 6. Configure the n8n Workflow\n - Modify the Trigger (Manual or Scheduled)\n\n - Input your Target URLs and Collection Name in the designated nodes\n\n - Paste all required API Tokens / Keys into their corresponding nodes\n\n - Ensure your Qdrant and Ollama services are running\n\n## Ideal Use Cases\n - Custom AI Chatbots\n\n - Private Search Engines\n\n - Research Tools\n\n - Internal Knowledge Bases\n\n - Content Monitoring Pipelines",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When clicking 'Test workflow'",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Set Fields - URL and Webhook URL",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Scrapeless Web Request",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Format Claude Output",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Check Collection Exists",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Collection Exists Check",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2"
    },
    {
      "name": "Create Qdrant Collection",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Scrapeless Config Info",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Claude Data extractor",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Ollama Embeddings",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Qdrant Vector store",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Claude AI Agent",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Webhook for structured AI agent response",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Expot data webhook",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "AI Data Checker",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    }
  ]
}