{
  "id": 3537,
  "slug": "3537",
  "title": "Process multiple prompts in parallel with Azure OpenAI Batch API",
  "description": "# Process Multiple Prompts in Parallel with Azure OpenAI Batch API\n\n## Who is this for?\nThis workflow is designed for developers and data scientists who want to efficiently send multiple prompts to the Azure OpenAI Batch API and retrieve responses in a single batch process. It is particularly useful for applications that require processing large volumes of text data, such as chatbots, content generation, or data analysis.\n\n## What problem is this workflow solving?\nSending multiple prompts to the Azure OpenAI API can be time-consuming and inefficient if done sequentially. This workflow automates the process of batching requests, allowing users to submit multiple prompts at once and retrieve the results in a streamlined manner. This not only saves time but also optimizes resource usage.\n\n## What this workflow does\nThis workflow:\n1. Accepts an array of requests, each containing a prompt and associated parameters.\n2. Converts the requests into a JSONL format suitable for batch processing.\n3. Uploads the batch file to the Azure OpenAI API.\n4. Creates a batch job to process the prompts.\n5. Polls for the job status and retrieves the output once processing is complete.\n6. Parses the output and returns the results.\n\n### Key Features of Azure OpenAI Batch API\nThe Azure OpenAI Batch API is designed to handle large-scale and high-volume processing tasks efficiently. Key features include:\n- **Asynchronous Processing**: Handle groups of requests with separate quotas, targeting a 24-hour turnaround at 50% less cost than global standards.\n- **Batch Requests**: Send a large number of requests in a single file, avoiding disruption to online workloads.\n  \n### Key Use Cases\n- **Large-Scale Data Processing**: Quickly analyze extensive datasets in parallel.\n- **Content Generation**: Create large volumes of text, such as product descriptions or articles.\n- **Document Review and Summarization**: Automate the review and summarization of lengthy documents.\n- **Customer Support Automation**: Handle numerous queries simultaneously for faster responses.\n- **Data Extraction and Analysis**: Extract and analyze information from vast amounts of unstructured data.\n- **Natural Language Processing (NLP) Tasks**: Perform tasks like sentiment analysis or translation on large datasets.\n- **Marketing and Personalization**: Generate personalized content and recommendations at scale.\n\n## Setup\n1. **Azure OpenAI Credentials**: Ensure you have your Azure OpenAI API credentials set up in n8n.\n2. **Configure the Workflow**: \n   - Set the `az_openai_endpoint` in the \"Setup defaults\" node to your Azure OpenAI endpoint.\n   - Adjust the `api-version` in the \"Set desired 'api-version'\" node if necessary.\n3. **Run the Workflow**: Trigger the workflow using the \"Run example\" node to see it in action.\n\n## How to customize this workflow to your needs\n- **Modify Prompts**: Change the prompts in the \"One query example\" node to suit your application.\n- **Adjust Parameters**: Update the parameters in the requests to customize the behavior of the OpenAI model.\n- **Add More Requests**: You can add more requests in the input array to process additional prompts.\n\n## Example Input\n```json\n[\n  {\n    \"api-version\": \"2025-03-01-preview\",\n    \"requests\": [\n      {\n        \"custom_id\": \"first-prompt-in-my-batch\",\n        \"params\": {\n          \"messages\": [\n            {\n              \"content\": \"Hey ChatGPT, tell me a short fun fact about cats!\",\n              \"role\": \"user\"\n            }\n          ]\n        }\n      },\n      {\n        \"custom_id\": \"second-prompt-in-my-batch\",\n        \"params\": {\n          \"messages\": [\n            {\n              \"content\": \"Hey ChatGPT, tell me a short fun fact about bees!\",\n              \"role\": \"user\"\n            }\n          ]\n        }\n      }\n    ]\n  }\n]\n```\n\n## Example Output\n```json\n[\n  {\n    \"custom_id\": \"first-prompt-in-my-batch\",\n    \"response\": {\n      \"body\": {\n        \"choices\": [\n          {\n            \"message\": {\n              \"content\": \"Did you know that cats can make over 100 different sounds?\"\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"custom_id\": \"second-prompt-in-my-batch\",\n    \"response\": {\n      \"body\": {\n        \"choices\": [\n          {\n            \"message\": {\n              \"content\": \"Bees communicate through a unique dance called the 'waggle dance'.\"\n            }\n          }\n        ]\n      }\n    }\n  }\n]\n```\n\n## Additional Notes\n- **Job Management**: You can cancel a job at any time, and any remaining work will be canceled while already completed work is returned. You will be charged for any completed work.\n- **Data Residency**: Data stored at rest remains in the designated Azure geography, while data may be processed for inferencing in any Azure OpenAI location.\n- **Exponential Backoff**: If your batch jobs are large and hitting the enqueued token limit, certain regions support queuing multiple batch jobs with exponential backoff.\n\nThis template provides a comprehensive solution for efficiently processing multiple prompts using the Azure OpenAI Batch API, making it a valuable tool for developers and data scientists alike.",
  "featuredImage": "/data/workflows/3537/3537.webp",
  "author": {
    "id": 101,
    "slug": "greg",
    "name": "Greg Evseev",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "Multimodal AI"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 698,
  "downloads": 69,
  "createdAt": "2025-04-13T09:34:04.418Z",
  "updatedAt": "2026-01-16T08:30:14.470Z",
  "publishedAt": "2025-04-13T09:34:04.418Z",
  "nodes": 53,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/3537",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Process multiple prompts in parallel with Azure OpenAI Batch API",
    "workflowName": "Process multiple prompts in parallel with Azure OpenAI Batch API",
    "description": "# Process Multiple Prompts in Parallel with Azure OpenAI Batch API\n\n## Who is this for?\nThis workflow is designed for developers and data scientists who want to efficiently send multiple prompts to the Azure OpenAI Batch API and retrieve responses in a single batch process. It is particularly useful for applications that require processing large volumes of text data, such as chatbots, content generation, or data analysis.\n\n## What problem is this workflow solving?\nSending multiple prompts to the Azure OpenAI API can be time-consuming and inefficient if done sequentially. This workflow automates the process of batching requests, allowing users to submit multiple prompts at once and retrieve the results in a streamlined manner. This not only saves time but also optimizes resource usage.\n\n## What this workflow does\nThis workflow:\n1. Accepts an array of requests, each containing a prompt and associated parameters.\n2. Converts the requests into a JSONL format suitable for batch processing.\n3. Uploads the batch file to the Azure OpenAI API.\n4. Creates a batch job to process the prompts.\n5. Polls for the job status and retrieves the output once processing is complete.\n6. Parses the output and returns the results.\n\n### Key Features of Azure OpenAI Batch API\nThe Azure OpenAI Batch API is designed to handle large-scale and high-volume processing tasks efficiently. Key features include:\n- **Asynchronous Processing**: Handle groups of requests with separate quotas, targeting a 24-hour turnaround at 50% less cost than global standards.\n- **Batch Requests**: Send a large number of requests in a single file, avoiding disruption to online workloads.\n  \n### Key Use Cases\n- **Large-Scale Data Processing**: Quickly analyze extensive datasets in parallel.\n- **Content Generation**: Create large volumes of text, such as product descriptions or articles.\n- **Document Review and Summarization**: Automate the review and summarization of lengthy documents.\n- **Customer Support Automation**: Handle numerous queries simultaneously for faster responses.\n- **Data Extraction and Analysis**: Extract and analyze information from vast amounts of unstructured data.\n- **Natural Language Processing (NLP) Tasks**: Perform tasks like sentiment analysis or translation on large datasets.\n- **Marketing and Personalization**: Generate personalized content and recommendations at scale.\n\n## Setup\n1. **Azure OpenAI Credentials**: Ensure you have your Azure OpenAI API credentials set up in n8n.\n2. **Configure the Workflow**: \n   - Set the `az_openai_endpoint` in the \"Setup defaults\" node to your Azure OpenAI endpoint.\n   - Adjust the `api-version` in the \"Set desired 'api-version'\" node if necessary.\n3. **Run the Workflow**: Trigger the workflow using the \"Run example\" node to see it in action.\n\n## How to customize this workflow to your needs\n- **Modify Prompts**: Change the prompts in the \"One query example\" node to suit your application.\n- **Adjust Parameters**: Update the parameters in the requests to customize the behavior of the OpenAI model.\n- **Add More Requests**: You can add more requests in the input array to process additional prompts.\n\n## Example Input\n```json\n[\n  {\n    \"api-version\": \"2025-03-01-preview\",\n    \"requests\": [\n      {\n        \"custom_id\": \"first-prompt-in-my-batch\",\n        \"params\": {\n          \"messages\": [\n            {\n              \"content\": \"Hey ChatGPT, tell me a short fun fact about cats!\",\n              \"role\": \"user\"\n            }\n          ]\n        }\n      },\n      {\n        \"custom_id\": \"second-prompt-in-my-batch\",\n        \"params\": {\n          \"messages\": [\n            {\n              \"content\": \"Hey ChatGPT, tell me a short fun fact about bees!\",\n              \"role\": \"user\"\n            }\n          ]\n        }\n      }\n    ]\n  }\n]\n```\n\n## Example Output\n```json\n[\n  {\n    \"custom_id\": \"first-prompt-in-my-batch\",\n    \"response\": {\n      \"body\": {\n        \"choices\": [\n          {\n            \"message\": {\n              \"content\": \"Did you know that cats can make over 100 different sounds?\"\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"custom_id\": \"second-prompt-in-my-batch\",\n    \"response\": {\n      \"body\": {\n        \"choices\": [\n          {\n            \"message\": {\n              \"content\": \"Bees communicate through a unique dance called the 'waggle dance'.\"\n            }\n          }\n        ]\n      }\n    }\n  }\n]\n```\n\n## Additional Notes\n- **Job Management**: You can cancel a job at any time, and any remaining work will be canceled while already completed work is returned. You will be charged for any completed work.\n- **Data Residency**: Data stored at rest remains in the designated Azure geography, while data may be processed for inferencing in any Azure OpenAI location.\n- **Exponential Backoff**: If your batch jobs are large and hitting the enqueued token limit, certain regions support queuing multiple batch jobs with exponential backoff.\n\nThis template provides a comprehensive solution for efficiently processing multiple prompts using the Azure OpenAI Batch API, making it a valuable tool for developers and data scientists alike.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When Executed by Another Workflow",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "role": "executeWorkflowTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Parse response",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "If ended processing",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Batch Status Poll Interval",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Run example",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "One query example",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Delete original properties",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Construct 'requests' array",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "Build batch 'request' object for single query",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Simple Memory Store",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Fill Chat Memory with example data",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "role": "memoryManager",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Build batch 'request' object from Chat Memory and execution data",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Load Chat Memory Data",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "role": "memoryManager",
      "configDescription": "Version 1.1"
    },
    {
      "name": "First Prompt Result",
      "type": "n8n-nodes-base.executionData",
      "role": "executionData",
      "configDescription": "Version 1"
    },
    {
      "name": "Second Prompt Result",
      "type": "n8n-nodes-base.executionData",
      "role": "executionData",
      "configDescription": "Version 1"
    },
    {
      "name": "Split Out Parsed Results",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Filter Second Prompt Results",
      "type": "n8n-nodes-base.filter",
      "role": "filter",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Filter First Prompt Results",
      "type": "n8n-nodes-base.filter",
      "role": "filter",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note7",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note10",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note11",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Join two example requests into array",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.1"
    },
    {
      "name": "Truncate Chat Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "role": "memoryManager",
      "configDescription": "Version 1.1"
    },
    {
      "name": "JSON requests to JSONL",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Change file name and mimetype",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "If upload processed",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "File Upload Poll Interval",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Track file upload status",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Create batch job",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Track batch job progress",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Retrieve batch job output file",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Set desired 'api-version'",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Merge File and API Version",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.1"
    },
    {
      "name": "Remove original requests",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Convert requests jsonl to File",
      "type": "n8n-nodes-base.convertToFile",
      "role": "convertToFile",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Append custom_id for single query example",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Append custom_id for chat memory example",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Setup defaults",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Set defaults if not set already",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Execute Workflow 'Process Multiple Prompts in Parallel with Azure OpenAI Batch API'",
      "type": "n8n-nodes-base.executeWorkflow",
      "role": "executeWorkflow",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Upload batch file",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note9",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note12",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note13",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note14",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}