{
  "id": 4424,
  "slug": "4424",
  "title": "Evaluate AI agent response correctness with OpenAI and RAGAS methodology",
  "description": "### This n8n template demonstrates how to calculate the evaluation metric \"Correctness\" which in this scenario, measures the compares and classifies the agent's response against a set of ground truths.\n\nThe scoring approach is adapted from the open-source evaluations project [RAGAS](https://docs.ragas.io/) and you can see the source here [https://github.com/explodinggradients/ragas/blob/main/ragas/src/ragas/metrics/_answer_correctness.py](https://github.com/explodinggradients/ragas/blob/main/ragas/src/ragas/metrics/_answer_correctness.py)\n\n### How it works\n* This evaluation works best where the agent's response is allowed to be more verbose and conversational.\n* For our scoring, we classify the agent's response into 3 buckets: True Positive (in answer and ground truth), False Positive (in answer but not ground truth) and False Negative (not in answer but in ground truth).\n* We also calculate an average similarity score on the agent's response against all ground truths.\n* The classification and the similarity score is then averaged to give the final score. \n* A high score indicates the agent is accurate whereas a low score could indicate the agent has incorrect training data or is not providing a comprehensive enough answer.\n\n### Requirements\n* n8n version 1.94+\n* Check out this Google Sheet for a sample data [https://docs.google.com/spreadsheets/d/1YOnu2JJjlxd787AuYcg-wKbkjyjyZFgASYVV0jsij5Y/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1YOnu2JJjlxd787AuYcg-wKbkjyjyZFgASYVV0jsij5Y/edit?usp=sharing)",
  "featuredImage": "/data/workflows/4424/4424.webp",
  "author": {
    "id": 101,
    "slug": "jimleuk",
    "name": "Jimleuk",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "AI Summarization"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 1324,
  "downloads": 132,
  "createdAt": "2025-05-27T07:18:12.776Z",
  "updatedAt": "2026-01-16T08:34:18.691Z",
  "publishedAt": "2025-05-27T07:18:12.776Z",
  "nodes": 27,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/4424",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Evaluate AI agent response correctness with OpenAI and RAGAS methodology",
    "workflowName": "Evaluate AI agent response correctness with OpenAI and RAGAS methodology",
    "description": "### This n8n template demonstrates how to calculate the evaluation metric \"Correctness\" which in this scenario, measures the compares and classifies the agent's response against a set of ground truths.\n\nThe scoring approach is adapted from the open-source evaluations project [RAGAS](https://docs.ragas.io/) and you can see the source here [https://github.com/explodinggradients/ragas/blob/main/ragas/src/ragas/metrics/_answer_correctness.py](https://github.com/explodinggradients/ragas/blob/main/ragas/src/ragas/metrics/_answer_correctness.py)\n\n### How it works\n* This evaluation works best where the agent's response is allowed to be more verbose and conversational.\n* For our scoring, we classify the agent's response into 3 buckets: True Positive (in answer and ground truth), False Positive (in answer but not ground truth) and False Negative (not in answer but in ground truth).\n* We also calculate an average similarity score on the agent's response against all ground truths.\n* The classification and the similarity score is then averaged to give the final score. \n* A high score indicates the agent is accurate whereas a low score could indicate the agent has incorrect training data or is not providing a comprehensive enough answer.\n\n### Requirements\n* n8n version 1.94+\n* Check out this Google Sheet for a sample data [https://docs.google.com/spreadsheets/d/1YOnu2JJjlxd787AuYcg-wKbkjyjyZFgASYVV0jsij5Y/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1YOnu2JJjlxd787AuYcg-wKbkjyjyZFgASYVV0jsij5Y/edit?usp=sharing)",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Correctness Classifier",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "role": "chainLlm",
      "configDescription": "Version 1.7"
    },
    {
      "name": "Examples1",
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "role": "outputParserStructured",
      "configDescription": "Version 1.2"
    },
    {
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "OpenAI Chat Model1",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "When fetching a dataset row",
      "type": "n8n-nodes-base.evaluationTrigger",
      "role": "evaluationTrigger",
      "configDescription": "Version 4.6"
    },
    {
      "name": "Remap Input",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Evaluation",
      "type": "n8n-nodes-base.evaluation",
      "role": "evaluation",
      "configDescription": "Version 4.6"
    },
    {
      "name": "Set Input Fields",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "No Operation, do nothing",
      "type": "n8n-nodes-base.noOp",
      "role": "noOp",
      "configDescription": "Version 1"
    },
    {
      "name": "AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 2"
    },
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Merge",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.1"
    },
    {
      "name": "Calculate F1 Score",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Correctness Score",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Update Metrics",
      "type": "n8n-nodes-base.evaluation",
      "role": "evaluation",
      "configDescription": "Version 4.6"
    },
    {
      "name": "Update Outputs",
      "type": "n8n-nodes-base.evaluation",
      "role": "evaluation",
      "configDescription": "Version 4.6"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Get Embeddings",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "GroundTruth to Items",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Get Embeddings1",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Aggregate",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "Remap Embeddings",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Remap Embeddings1",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Create Embeddings Result",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.1"
    },
    {
      "name": "Calculate Similarity Score",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    }
  ]
}