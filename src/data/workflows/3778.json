{
  "id": 3778,
  "slug": "3778",
  "title": "Scrape Web Data with Bright Data, Google Gemini and MCP Automated AI Agent",
  "description": "### Disclaimer\nThis template is only available on n8n self-hosted as it's making use of the community node for MCP Client.\n\n![Scrape Web Data with Bright Data  MCP Client.png](fileId:1219)\n### Who this is for?\n\nThe Scrape Web Data with Bright Data and MCP Automated AI Agent workflow is built for professionals who need to automate large-scale, intelligent data extraction by utilizing the Bright Data MCP Server and Google Gemini.\n\nThis solution is ideal for:\n\n1. **Data Analysts** - Who require structured, enriched datasets for analysis and reporting.\n\n2. **Marketing Researchers** - Seeking fresh market intelligence from dynamic web sources.\n\n3. **Product Managers** - Who want competitive product and feature insights from various websites.\n\n4. **AI Developers** - Aiming to feed web data into downstream machine learning models.\n\n5. **Growth Hackers** - Looking for high-quality data to fuel campaigns, research, or strategic targeting.\n\n### What problem is this workflow solving?\n\nManually scraping websites, cleaning raw HTML data, and generating useful insights from it can be slow, error-prone, and non-scalable.\n\nThis workflow solves these problems by:\n\n1. Automating complex web data extraction through Bright Data’s MCP Server.\n\n2. Reducing the human effort needed for cleaning, parsing, and analyzing unstructured web content.\n\n3. Allowing seamless integration into further automation processes.\n\n### What this workflow does?\n\nThis n8n workflow performs the following steps:\n\n1. **Trigger**: Start manually.\n\n2. **Input URL(s)**: Specify the URL to perform the web scrapping.\n\n3. **Web Scraping (Bright Data)**: Use Bright Data’s MCP Server tools to accomplish the web data scrapping with markdown and html format.\n\n4. **Store / Output**: Save results into disk and also performs a Webhook notification.\n\n### Setup\n1. Please make sure to setup n8n locally with MCP Servers by navigating to [n8n-nodes-mcp](https://www.youtube.com/watch?v=NUb73ErUCsA)\n2. Please make sure to install the Bright Data MCP Server [@brightdata/mcp](https://www.npmjs.com/package/@brightdata/mcp)  on your local machine.\n3. Sign up at [Bright Data](https://brightdata.com/).\n4. Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.\n5. Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.\n6. In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).\n7. In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.\n\n![MCPClientAccount.png](fileId:1218)\n\nMake sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=&lt;your-token&gt;.\n8. Update the LinkedIn URL person and company workflow.\n9. Update the Webhook HTTP Request node with the Webhook endpoint of your choice.\n10. Update the file name and path to persist on disk.\n\n### How to customize this workflow to your needs\n1. **Different Inputs**: Instead of static URLs, accept URLs dynamically via webhook or form submissions.\n\n2. **Outputs**: Update the Webhook endpoints to send the response to Slack channels, Airtable, Notion, CRM systems, etc.",
  "featuredImage": "/data/workflows/3778/3778.webp",
  "author": {
    "id": 101,
    "slug": "ranjancse",
    "name": "Ranjan Dailata",
    "avatar": ""
  },
  "categories": [
    "Market Research",
    "AI Summarization"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 3872,
  "downloads": 387,
  "createdAt": "2025-04-28T23:06:34.655Z",
  "updatedAt": "2026-01-16T08:31:24.577Z",
  "publishedAt": "2025-04-28T23:06:34.655Z",
  "nodes": 19,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/3778",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Scrape Web Data with Bright Data, Google Gemini and MCP Automated AI Agent",
    "workflowName": "Scrape Web Data with Bright Data, Google Gemini and MCP Automated AI Agent",
    "description": "### Disclaimer\nThis template is only available on n8n self-hosted as it's making use of the community node for MCP Client.\n\n![Scrape Web Data with Bright Data  MCP Client.png](fileId:1219)\n### Who this is for?\n\nThe Scrape Web Data with Bright Data and MCP Automated AI Agent workflow is built for professionals who need to automate large-scale, intelligent data extraction by utilizing the Bright Data MCP Server and Google Gemini.\n\nThis solution is ideal for:\n\n1. **Data Analysts** - Who require structured, enriched datasets for analysis and reporting.\n\n2. **Marketing Researchers** - Seeking fresh market intelligence from dynamic web sources.\n\n3. **Product Managers** - Who want competitive product and feature insights from various websites.\n\n4. **AI Developers** - Aiming to feed web data into downstream machine learning models.\n\n5. **Growth Hackers** - Looking for high-quality data to fuel campaigns, research, or strategic targeting.\n\n### What problem is this workflow solving?\n\nManually scraping websites, cleaning raw HTML data, and generating useful insights from it can be slow, error-prone, and non-scalable.\n\nThis workflow solves these problems by:\n\n1. Automating complex web data extraction through Bright Data’s MCP Server.\n\n2. Reducing the human effort needed for cleaning, parsing, and analyzing unstructured web content.\n\n3. Allowing seamless integration into further automation processes.\n\n### What this workflow does?\n\nThis n8n workflow performs the following steps:\n\n1. **Trigger**: Start manually.\n\n2. **Input URL(s)**: Specify the URL to perform the web scrapping.\n\n3. **Web Scraping (Bright Data)**: Use Bright Data’s MCP Server tools to accomplish the web data scrapping with markdown and html format.\n\n4. **Store / Output**: Save results into disk and also performs a Webhook notification.\n\n### Setup\n1. Please make sure to setup n8n locally with MCP Servers by navigating to [n8n-nodes-mcp](https://www.youtube.com/watch?v=NUb73ErUCsA)\n2. Please make sure to install the Bright Data MCP Server [@brightdata/mcp](https://www.npmjs.com/package/@brightdata/mcp)  on your local machine.\n3. Sign up at [Bright Data](https://brightdata.com/).\n4. Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.\n5. Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.\n6. In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).\n7. In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.\n\n![MCPClientAccount.png](fileId:1218)\n\nMake sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=&lt;your-token&gt;.\n8. Update the LinkedIn URL person and company workflow.\n9. Update the Webhook HTTP Request node with the Webhook endpoint of your choice.\n10. Update the file name and path to persist on disk.\n\n### How to customize this workflow to your needs\n1. **Different Inputs**: Instead of static URLs, accept URLs dynamically via webhook or form submissions.\n\n2. **Outputs**: Update the Webhook endpoints to send the response to Slack channels, Airtable, Notion, CRM systems, etc.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.8"
    },
    {
      "name": "When clicking ‘Test workflow’",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "MCP Client list all tools for Bright Data",
      "type": "n8n-nodes-mcp.mcpClient",
      "role": "mcpClient",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "MCP Client List all tools",
      "type": "n8n-nodes-mcp.mcpClientTool",
      "role": "mcpClientTool",
      "configDescription": "Version 1"
    },
    {
      "name": "MCP Client Bright Data Web Scraper",
      "type": "n8n-nodes-mcp.mcpClient",
      "role": "mcpClient",
      "configDescription": "Version 1"
    },
    {
      "name": "Webhook for web scraper",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Set the URLs",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "MCP Client to Scrape as Markdown",
      "type": "n8n-nodes-mcp.mcpClientTool",
      "role": "mcpClientTool",
      "configDescription": "Version 1"
    },
    {
      "name": "MCP Client to Scrape as HTML",
      "type": "n8n-nodes-mcp.mcpClientTool",
      "role": "mcpClientTool",
      "configDescription": "Version 1"
    },
    {
      "name": "Google Gemini Chat Model for AI Agent",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Simple Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Webhook for Web Scraper AI Agent",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Set the URL with the Webhook URL and data format",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Create a binary data",
      "type": "n8n-nodes-base.function",
      "role": "function",
      "configDescription": "Version 1"
    },
    {
      "name": "Write the scraped content to disk",
      "type": "n8n-nodes-base.readWriteFile",
      "role": "readWriteFile",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}