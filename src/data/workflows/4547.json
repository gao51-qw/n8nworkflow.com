{
  "id": 4547,
  "slug": "4547",
  "title": "Documentation Lookup AI Agent using Context7 and Gemini",
  "description": "**This n8n workflow template uses community nodes and is only compatible with the self-hosted version of n8n.** \n\nThis workflow demonstrates how to build and expose a sophisticated n8n AI Agent as a single, callable tool using the Multi-Agent Collaboration Protocol (MCP). It allows external clients or other AI systems to easily query software library documentation via Context7, without needing to manage the underlying tool orchestration or complex conversational logic.\n\n**Core Idea:**\nInstead of building complex agentic loops on the client-side (e.g., in Python, a VS Code extension, or another AI development environment), this workflow offloads the entire agent's reasoning and tool-use process to n8n. The client simply sends a natural language query (like \"How do I use Flexbox in Tailwind CSS?\") to an SSE endpoint, and the n8n agent handles the rest.\n\n**Key Features & How It Works:**\n\n1.  **Public MCP Endpoint:**\n    *   The main workflow uses the `Context7 MCP Server Trigger` node to create an SSE endpoint. This makes the agent accessible to any MCP-compatible client.\n    *   The path for the endpoint is kept long and random for basic 'security by obscurity'.\n2.  **Tool Workflow as an Interface:**\n    *   A `Tool Workflow` node (named `call_context7_ai_agent` in this example) is connected to the MCP Server Trigger. This node defines the single \"tool\" that external clients will see and call.\n3.  **Dedicated AI Agent Sub-Workflow:**\n    *   The `call_context7_ai_agent` tool invokes a separate sub-workflow which contains the actual AI logic.\n    *   This sub-workflow starts with a `Context7 Workflow Start` node to receive the user's `query`.\n    *   A `Context7 AI Agent` node (using Google Gemini in this example) is the brain, equipped with:\n        *   A system prompt to guide its behavior.\n        *   `Simple Memory` to retain context for each execution (using `{{ $execution.id }}` as the session key).\n        *   Two specialized Context7 MCP client tools:\n            *   `context7-resolve-library-id`: To convert library names (e.g., 'Next.js') into Context7-specific IDs.\n            *   `context7-get-library-docs`: To fetch documentation using the resolved ID, with options for specific topics and token limits.\n4.  **Seamless Tool Use:** The AI Agent autonomously decides when and how to use the `resolve-library-id` and `get-library-docs` tools based on the user's query, handling the multi-step process internally.\n\n**Benefits of This Approach:**\n\n*   **Simplified Client Integration:** Clients interact with a single, powerful tool, sending a simple query.\n*   **Reduced Client-Side Token Consumption:** The detailed prompts, tool descriptions, and conversational turns are managed server-side by n8n, saving tokens on the client (especially useful if the client is another LLM).\n*   **Centralized Agent Management:** Update your agent's capabilities, tools, or LLM model within n8n without any changes needed on the client side.\n*   **Modularity for Agentic Systems:** Perfect for building complex, multi-agent systems where this n8n workflow can act as a specialized \"expert\" agent callable by others (e.g., from environments like Smithery).\n*   **Cost-Effective:** By using a potentially less expensive model (like Gemini Flash) for the agent's orchestration and leveraging the free tier or efficient pricing of services like Context7, you can build powerful solutions economically.\n\n**Use Cases:**\n\n*   Providing an intelligent documentation lookup service for coding assistants or IDE extensions.\n*   Creating specialized AI \"micro-agents\" that can be consumed by larger AI applications.\n*   Building internal knowledge base query systems accessible via a simple API-like interface.\n\n**Setup:**\n\n*   Ensure you have the necessary n8n credentials for Google Gemini (or your chosen LLM) and the Context7 MCP client tools.\n*   The `Path` in the `Context7 MCP Server Trigger` node should be unique and secure.\n*   Clients connect to the \"Production URL\" (SSE endpoint) provided by the trigger node.\n\nThis workflow is a great example of how n8n can serve as a powerful backend for building and deploying modular AI agents.\n\nI've made a video to try and explain this a bit too https://www.youtube.com/watch?v=dudvmyp7Pyg",
  "featuredImage": "/data/workflows/4547/4547.webp",
  "author": {
    "id": 101,
    "slug": "jez",
    "name": "Jez",
    "avatar": ""
  },
  "categories": [
    "Internal Wiki",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 1599,
  "downloads": 159,
  "createdAt": "2025-06-01T03:32:24.786Z",
  "updatedAt": "2026-01-16T08:34:54.952Z",
  "publishedAt": "2025-06-01T03:32:24.786Z",
  "nodes": 18,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/4547",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Documentation Lookup AI Agent using Context7 and Gemini",
    "workflowName": "Documentation Lookup AI Agent using Context7 and Gemini",
    "description": "**This n8n workflow template uses community nodes and is only compatible with the self-hosted version of n8n.** \n\nThis workflow demonstrates how to build and expose a sophisticated n8n AI Agent as a single, callable tool using the Multi-Agent Collaboration Protocol (MCP). It allows external clients or other AI systems to easily query software library documentation via Context7, without needing to manage the underlying tool orchestration or complex conversational logic.\n\n**Core Idea:**\nInstead of building complex agentic loops on the client-side (e.g., in Python, a VS Code extension, or another AI development environment), this workflow offloads the entire agent's reasoning and tool-use process to n8n. The client simply sends a natural language query (like \"How do I use Flexbox in Tailwind CSS?\") to an SSE endpoint, and the n8n agent handles the rest.\n\n**Key Features & How It Works:**\n\n1.  **Public MCP Endpoint:**\n    *   The main workflow uses the `Context7 MCP Server Trigger` node to create an SSE endpoint. This makes the agent accessible to any MCP-compatible client.\n    *   The path for the endpoint is kept long and random for basic 'security by obscurity'.\n2.  **Tool Workflow as an Interface:**\n    *   A `Tool Workflow` node (named `call_context7_ai_agent` in this example) is connected to the MCP Server Trigger. This node defines the single \"tool\" that external clients will see and call.\n3.  **Dedicated AI Agent Sub-Workflow:**\n    *   The `call_context7_ai_agent` tool invokes a separate sub-workflow which contains the actual AI logic.\n    *   This sub-workflow starts with a `Context7 Workflow Start` node to receive the user's `query`.\n    *   A `Context7 AI Agent` node (using Google Gemini in this example) is the brain, equipped with:\n        *   A system prompt to guide its behavior.\n        *   `Simple Memory` to retain context for each execution (using `{{ $execution.id }}` as the session key).\n        *   Two specialized Context7 MCP client tools:\n            *   `context7-resolve-library-id`: To convert library names (e.g., 'Next.js') into Context7-specific IDs.\n            *   `context7-get-library-docs`: To fetch documentation using the resolved ID, with options for specific topics and token limits.\n4.  **Seamless Tool Use:** The AI Agent autonomously decides when and how to use the `resolve-library-id` and `get-library-docs` tools based on the user's query, handling the multi-step process internally.\n\n**Benefits of This Approach:**\n\n*   **Simplified Client Integration:** Clients interact with a single, powerful tool, sending a simple query.\n*   **Reduced Client-Side Token Consumption:** The detailed prompts, tool descriptions, and conversational turns are managed server-side by n8n, saving tokens on the client (especially useful if the client is another LLM).\n*   **Centralized Agent Management:** Update your agent's capabilities, tools, or LLM model within n8n without any changes needed on the client side.\n*   **Modularity for Agentic Systems:** Perfect for building complex, multi-agent systems where this n8n workflow can act as a specialized \"expert\" agent callable by others (e.g., from environments like Smithery).\n*   **Cost-Effective:** By using a potentially less expensive model (like Gemini Flash) for the agent's orchestration and leveraging the free tier or efficient pricing of services like Context7, you can build powerful solutions economically.\n\n**Use Cases:**\n\n*   Providing an intelligent documentation lookup service for coding assistants or IDE extensions.\n*   Creating specialized AI \"micro-agents\" that can be consumed by larger AI applications.\n*   Building internal knowledge base query systems accessible via a simple API-like interface.\n\n**Setup:**\n\n*   Ensure you have the necessary n8n credentials for Google Gemini (or your chosen LLM) and the Context7 MCP client tools.\n*   The `Path` in the `Context7 MCP Server Trigger` node should be unique and secure.\n*   Clients connect to the \"Production URL\" (SSE endpoint) provided by the trigger node.\n\nThis workflow is a great example of how n8n can serve as a powerful backend for building and deploying modular AI agents.\n\nI've made a video to try and explain this a bit too https://www.youtube.com/watch?v=dudvmyp7Pyg",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Google Gemini Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "context7-resolve-library-id",
      "type": "n8n-nodes-mcp.mcpClientTool",
      "role": "mcpClientTool",
      "configDescription": "Version 1"
    },
    {
      "name": "context7-get-library-docs",
      "type": "n8n-nodes-mcp.mcpClientTool",
      "role": "mcpClientTool",
      "configDescription": "Version 1"
    },
    {
      "name": "Context7 MCP Server Trigger",
      "type": "@n8n/n8n-nodes-langchain.mcpTrigger",
      "role": "mcpTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Context7 AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 2"
    },
    {
      "name": "Context7 Workflow Start",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "role": "executeWorkflowTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Simple Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "call_context7_ai_agent",
      "type": "@n8n/n8n-nodes-langchain.toolWorkflow",
      "role": "toolWorkflow",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note7",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note9",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}