{
  "id": 7186,
  "slug": "7186",
  "title": "Web scraper: extract website content from sitemaps to Google Drive",
  "description": "A reliable, no-frills web scraper that extracts content directly from websites using their sitemaps. Perfect for content audits, migrations, and research when you need straightforward HTML extraction without external dependencies.\n\nHow It Works\nThis streamlined workflow takes a practical approach to web scraping by leveraging XML sitemaps and direct HTTP requests. Here's how it delivers consistent results:\n\nDirect Sitemap Processing: The workflow starts by fetching your target website's XML sitemap and parsing it to extract all available page URLs. This eliminates guesswork and ensures comprehensive coverage of the site's content structure.\n\nRobust HTTP Scraping: Each page is scraped using direct HTTP requests with realistic browser headers that mimic legitimate web traffic. The scraper includes comprehensive error handling and timeout protection to handle various website configurations gracefully.\n\nIntelligent Content Extraction: The workflow uses sophisticated JavaScript parsing to extract meaningful content from raw HTML. It automatically identifies page titles through multiple methods (title tags, Open Graph metadata, H1 headers) and converts HTML structure into readable text format.\n\nFramework Detection: Built-in detection identifies whether sites use WordPress, Divi themes, or heavy JavaScript frameworks. This helps explain content extraction quality and provides valuable insights about the site's technical architecture.\n\nRich Metadata Collection: Each scraped page includes detailed metadata like word count, HTML size, response codes, and technical indicators. This data is formatted into comprehensive markdown files with YAML frontmatter for easy analysis and organization.\n\nRespectful Rate Limiting: The workflow includes a 3-second delay between page requests to respect server resources and avoid overwhelming target websites. The processing is sequential and controlled to maintain ethical scraping practices.\n\nDetailed Success Reporting: Every scraped page generates a report showing extraction success, potential issues (like JavaScript dependencies), and technical details about the site's structure and framework.\n\nSetup Steps\n\nConfigure Google Drive Integration\n\nConnect your Google Drive account in the \"Save to Google Drive\" node\nReplace YOUR_GOOGLE_DRIVE_CREDENTIAL_ID with your actual Google Drive credential ID\nCreate a dedicated folder for your scraped content in Google Drive\nCopy the folder ID from the Google Drive URL (the long string after /folders/)\nReplace YOUR_GOOGLE_DRIVE_FOLDER_ID_HERE with your actual folder ID in both the folderId field and cachedResultUrl\nUpdate YOUR_FOLDER_NAME_HERE with your folder's actual name\n\n\nSet Your Target Website\n\nIn the \"Set Sitemap URL\" node, replace https://yourwebsitehere.com/page-sitemap.xml with your target website's sitemap URL\nCommon sitemap locations include /sitemap.xml, /page-sitemap.xml, or /sitemap_index.xml\nTip: Not sure where your sitemap is? Use a free online tool like https://seomator.com/sitemap-finder\nVerify the sitemap URL loads correctly in your browser before running the workflow\n\n\nUpdate Workflow IDs (Automatic)\n\nWhen you import this workflow, n8n will automatically generate new IDs for YOUR_WORKFLOW_ID_HERE, YOUR_VERSION_ID_HERE, YOUR_INSTANCE_ID_HERE, and YOUR_WEBHOOK_ID_HERE\nNo manual changes needed for these placeholders\n\n\nAdjust Processing Limits (Optional)\n\nThe \"Limit URLs (Optional)\" node is currently disabled for full site scraping\nEnable this node and set a smaller number (like 5-10) for initial testing\nFor large websites, consider running in batches to manage processing time and storage\n\n\nCustomize Rate Limiting (Optional)\n\nThe \"Wait Between Pages\" node is set to 3 seconds by default\nIncrease the delay for more respectful scraping of busy sites\nDecrease only if you have permission and the target site can handle faster requests\n\n\nTest Your Configuration\n\nEnable the \"Limit URLs (Optional)\" node and set it to 3-5 pages for testing\nClick \"Test workflow\" to verify the setup works correctly\nCheck your Google Drive folder to confirm files are being created with proper content\nReview the generated markdown files to assess content extraction quality\n\n\nRun Full Extraction\n\nDisable the \"Limit URLs (Optional)\" node for complete site scraping\nExecute the workflow and monitor the execution log for any errors\nLarge websites may take considerable time to process completely (plan for several hours for sites with hundreds of pages)\n\n\nReview Results\n\nEach generated file includes technical metadata to help you assess extraction quality\nLook for indicators like \"Limited Content\" warnings for JavaScript-heavy pages\nFiles include word counts and framework detection to help you understand the site's structure\n\n\n\nFramework Compatibility: This scraper is specifically designed to work well with WordPress sites, Divi themes, and many JavaScript-heavy frameworks. The intelligent content extraction handles dynamic content effectively and provides detailed feedback about framework detection. While some single-page applications (SPAs) that render entirely through JavaScript may have limited content extraction, most modern websites including those built with popular CMS platforms will work excellently with this scraper.\nImportant Notes: Always ensure you have permission to scrape your target website and respect their robots.txt guidelines. The workflow includes respectful delays and error handling, but monitor your usage to maintain ethical scraping practices.RetryClaude can make mistakes. Please double-check responses.",
  "featuredImage": "/data/workflows/7186/7186.webp",
  "author": {
    "id": 101,
    "slug": "replybotz",
    "name": "Wolf Bishop",
    "avatar": ""
  },
  "categories": [
    "Market Research",
    "Multimodal AI"
  ],
  "complexityLevel": "intermediate",
  "price": 0,
  "visitors": 3906,
  "downloads": 390,
  "createdAt": "2025-08-08T15:11:11.466Z",
  "updatedAt": "2026-01-16T08:48:54.721Z",
  "publishedAt": "2025-08-08T15:11:11.466Z",
  "nodes": 13,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/7186",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Web scraper: extract website content from sitemaps to Google Drive",
    "workflowName": "Web scraper: extract website content from sitemaps to Google Drive",
    "description": "A reliable, no-frills web scraper that extracts content directly from websites using their sitemaps. Perfect for content audits, migrations, and research when you need straightforward HTML extraction without external dependencies.\n\nHow It Works\nThis streamlined workflow takes a practical approach to web scraping by leveraging XML sitemaps and direct HTTP requests. Here's how it delivers consistent results:\n\nDirect Sitemap Processing: The workflow starts by fetching your target website's XML sitemap and parsing it to extract all available page URLs. This eliminates guesswork and ensures comprehensive coverage of the site's content structure.\n\nRobust HTTP Scraping: Each page is scraped using direct HTTP requests with realistic browser headers that mimic legitimate web traffic. The scraper includes comprehensive error handling and timeout protection to handle various website configurations gracefully.\n\nIntelligent Content Extraction: The workflow uses sophisticated JavaScript parsing to extract meaningful content from raw HTML. It automatically identifies page titles through multiple methods (title tags, Open Graph metadata, H1 headers) and converts HTML structure into readable text format.\n\nFramework Detection: Built-in detection identifies whether sites use WordPress, Divi themes, or heavy JavaScript frameworks. This helps explain content extraction quality and provides valuable insights about the site's technical architecture.\n\nRich Metadata Collection: Each scraped page includes detailed metadata like word count, HTML size, response codes, and technical indicators. This data is formatted into comprehensive markdown files with YAML frontmatter for easy analysis and organization.\n\nRespectful Rate Limiting: The workflow includes a 3-second delay between page requests to respect server resources and avoid overwhelming target websites. The processing is sequential and controlled to maintain ethical scraping practices.\n\nDetailed Success Reporting: Every scraped page generates a report showing extraction success, potential issues (like JavaScript dependencies), and technical details about the site's structure and framework.\n\nSetup Steps\n\nConfigure Google Drive Integration\n\nConnect your Google Drive account in the \"Save to Google Drive\" node\nReplace YOUR_GOOGLE_DRIVE_CREDENTIAL_ID with your actual Google Drive credential ID\nCreate a dedicated folder for your scraped content in Google Drive\nCopy the folder ID from the Google Drive URL (the long string after /folders/)\nReplace YOUR_GOOGLE_DRIVE_FOLDER_ID_HERE with your actual folder ID in both the folderId field and cachedResultUrl\nUpdate YOUR_FOLDER_NAME_HERE with your folder's actual name\n\n\nSet Your Target Website\n\nIn the \"Set Sitemap URL\" node, replace https://yourwebsitehere.com/page-sitemap.xml with your target website's sitemap URL\nCommon sitemap locations include /sitemap.xml, /page-sitemap.xml, or /sitemap_index.xml\nTip: Not sure where your sitemap is? Use a free online tool like https://seomator.com/sitemap-finder\nVerify the sitemap URL loads correctly in your browser before running the workflow\n\n\nUpdate Workflow IDs (Automatic)\n\nWhen you import this workflow, n8n will automatically generate new IDs for YOUR_WORKFLOW_ID_HERE, YOUR_VERSION_ID_HERE, YOUR_INSTANCE_ID_HERE, and YOUR_WEBHOOK_ID_HERE\nNo manual changes needed for these placeholders\n\n\nAdjust Processing Limits (Optional)\n\nThe \"Limit URLs (Optional)\" node is currently disabled for full site scraping\nEnable this node and set a smaller number (like 5-10) for initial testing\nFor large websites, consider running in batches to manage processing time and storage\n\n\nCustomize Rate Limiting (Optional)\n\nThe \"Wait Between Pages\" node is set to 3 seconds by default\nIncrease the delay for more respectful scraping of busy sites\nDecrease only if you have permission and the target site can handle faster requests\n\n\nTest Your Configuration\n\nEnable the \"Limit URLs (Optional)\" node and set it to 3-5 pages for testing\nClick \"Test workflow\" to verify the setup works correctly\nCheck your Google Drive folder to confirm files are being created with proper content\nReview the generated markdown files to assess content extraction quality\n\n\nRun Full Extraction\n\nDisable the \"Limit URLs (Optional)\" node for complete site scraping\nExecute the workflow and monitor the execution log for any errors\nLarge websites may take considerable time to process completely (plan for several hours for sites with hundreds of pages)\n\n\nReview Results\n\nEach generated file includes technical metadata to help you assess extraction quality\nLook for indicators like \"Limited Content\" warnings for JavaScript-heavy pages\nFiles include word counts and framework detection to help you understand the site's structure\n\n\n\nFramework Compatibility: This scraper is specifically designed to work well with WordPress sites, Divi themes, and many JavaScript-heavy frameworks. The intelligent content extraction handles dynamic content effectively and provides detailed feedback about framework detection. While some single-page applications (SPAs) that render entirely through JavaScript may have limited content extraction, most modern websites including those built with popular CMS platforms will work excellently with this scraper.\nImportant Notes: Always ensure you have permission to scrape your target website and respect their robots.txt guidelines. The workflow includes respectful delays and error handling, but monitor your usage to maintain ethical scraping practices.RetryClaude can make mistakes. Please double-check responses.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Working Scraper Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Start Working Scraper",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Set Sitemap URL",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Get Sitemap",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Parse Sitemap XML",
      "type": "n8n-nodes-base.xml",
      "role": "xml",
      "configDescription": "Version 1"
    },
    {
      "name": "Split Sitemap URLs",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Limit URLs (Optional)",
      "type": "n8n-nodes-base.limit",
      "role": "limit",
      "configDescription": "Version 1"
    },
    {
      "name": "Loop URLs",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "Scrape Page",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Extract Content",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Format Content",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Save to Google Drive",
      "type": "n8n-nodes-base.googleDrive",
      "role": "googleDrive",
      "configDescription": "Version 3"
    },
    {
      "name": "Wait Between Pages",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    }
  ]
}