{
  "id": 6961,
  "slug": "6961",
  "title": "Vision RAG and image embeddings using Cohere Command-A and Embed v4",
  "description": "### Cohere's new multimodal model releases make building your own Vision RAG agents a breeze. If you're new to Multimodal RAG and for the intent of this template, it means to embed and retrieve only document scans relevant to a query and then have a vision model read those scans to answer.\n\nThe benefits being (1) the vision model doesn't need to keep all document scans in context (expensive) and (2) ability to query on graphical content such as charts, graphs and tables.\n\n### How it works\n* Page extracts from a technology report containing graphs and charts are downloaded, converted to base64 and embedded using Cohere's Embed v4 model.\n* This produces embedding vectors which we will associate with the original page url and store them in our Qdrant vector store collection using the Qdrant community node.\n* Our Vision RAG agent is split into 2 parts; one regular AI agent for chat and a second Q&A agent powered by Cohere's Command-A-vision model which is required to read contents of images.\n* When a query requires access to the technology report, the Q&A agent branch is activated. This branch performs a vector search on our image embeddings and returns a list of matching image urls. These urls are then used as input for our vision model along with the user's original query.\n* The Q&A vision agent can then reply to the user using the \"respond to chat\" node.\n* Because both agents share the same memory space, it would be the same conversation to the user.\n\n### How to use\n* Ensure you have a Cohere account and sufficient credit to avoid rate limit or token usage restrictions.\n* For embeddings, swap out the page extracts for your own. You may need to split and convert document pages to images if you want to use image embeddings.\n* For chat, you may want to structure the agent(s) in another way which makes sense for your environment eg. using MCP servers.\n\n### Requirements\n* Cohere account for Embeddings and LLM\n* Qdrant for vector store\n",
  "featuredImage": "/data/workflows/6961/6961.webp",
  "author": {
    "id": 101,
    "slug": "jimleuk",
    "name": "Jimleuk",
    "avatar": ""
  },
  "categories": [
    "Document Extraction",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 1226,
  "downloads": 122,
  "createdAt": "2025-08-04T11:46:32.810Z",
  "updatedAt": "2026-01-16T08:47:49.590Z",
  "publishedAt": "2025-08-04T11:46:32.810Z",
  "nodes": 38,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/6961",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Vision RAG and image embeddings using Cohere Command-A and Embed v4",
    "workflowName": "Vision RAG and image embeddings using Cohere Command-A and Embed v4",
    "description": "### Cohere's new multimodal model releases make building your own Vision RAG agents a breeze. If you're new to Multimodal RAG and for the intent of this template, it means to embed and retrieve only document scans relevant to a query and then have a vision model read those scans to answer.\n\nThe benefits being (1) the vision model doesn't need to keep all document scans in context (expensive) and (2) ability to query on graphical content such as charts, graphs and tables.\n\n### How it works\n* Page extracts from a technology report containing graphs and charts are downloaded, converted to base64 and embedded using Cohere's Embed v4 model.\n* This produces embedding vectors which we will associate with the original page url and store them in our Qdrant vector store collection using the Qdrant community node.\n* Our Vision RAG agent is split into 2 parts; one regular AI agent for chat and a second Q&A agent powered by Cohere's Command-A-vision model which is required to read contents of images.\n* When a query requires access to the technology report, the Q&A agent branch is activated. This branch performs a vector search on our image embeddings and returns a list of matching image urls. These urls are then used as input for our vision model along with the user's original query.\n* The Q&A vision agent can then reply to the user using the \"respond to chat\" node.\n* Because both agents share the same memory space, it would be the same conversation to the user.\n\n### How to use\n* Ensure you have a Cohere account and sufficient credit to avoid rate limit or token usage restrictions.\n* For embeddings, swap out the page extracts for your own. You may need to split and convert document pages to images if you want to use image embeddings.\n* For chat, you may want to structure the agent(s) in another way which makes sense for your environment eg. using MCP servers.\n\n### Requirements\n* Cohere account for Embeddings and LLM\n* Qdrant for vector store",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When clicking ‘Execute workflow’",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Technology and Innovation Report 2025",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Download Page",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Split Out Urls",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Image Embeddings with Cohere Embed 4",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Prepare Points",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Aggregate Points",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "Insert Points",
      "type": "n8n-nodes-qdrant.qdrant",
      "role": "qdrant",
      "configDescription": "Version 1"
    },
    {
      "name": "Batch 5",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "Page Ref",
      "type": "n8n-nodes-base.noOp",
      "role": "noOp",
      "configDescription": "Version 1"
    },
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.3"
    },
    {
      "name": "AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 2.2"
    },
    {
      "name": "If has Tool Call?",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Respond to Chat",
      "type": "@n8n/n8n-nodes-langchain.chat",
      "role": "chat",
      "configDescription": "Version 1"
    },
    {
      "name": "Simple Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Respond to Chat1",
      "type": "@n8n/n8n-nodes-langchain.chat",
      "role": "chat",
      "configDescription": "Version 1"
    },
    {
      "name": "Simple Memory1",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Technology Innovation Report Tool",
      "type": "@n8n/n8n-nodes-langchain.toolCode",
      "role": "toolCode",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Image Understanding via Command-A-Vision",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Chat Model via Command-R",
      "type": "@n8n/n8n-nodes-langchain.lmChatCohere",
      "role": "lmChatCohere",
      "configDescription": "Version 1"
    },
    {
      "name": "Get Query",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Convert Image to Base64",
      "type": "n8n-nodes-base.extractFromFile",
      "role": "extractFromFile",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Create Collection",
      "type": "n8n-nodes-qdrant.qdrant",
      "role": "qdrant",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Get Relevant Images",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreQdrant",
      "role": "vectorStoreQdrant",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Embeddings",
      "type": "@n8n/n8n-nodes-langchain.embeddingsCohere",
      "role": "embeddingsCohere",
      "configDescription": "Version 1"
    },
    {
      "name": "Aggregate Results",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note7",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Quick Confirmation",
      "type": "@n8n/n8n-nodes-langchain.chat",
      "role": "chat",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note9",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note10",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}