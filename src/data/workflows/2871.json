{
  "id": 2871,
  "slug": "2871",
  "title": "RAG: context-aware chunking | Google Drive to Pinecone via OpenRouter & Gemini",
  "description": "Workflow based **on** the following article.\nhttps://www.anthropic.com/news/contextual-retrieval\n\nThis n8n automation is designed to extract, process, and store content from documents into a **Pinecone** vector store using context-based chunking. The workflow enhances retrieval accuracy in **RAG (Retrieval-Augmented Generation)** setups by ensuring each chunk retains meaningful context.\n\n**Workflow Breakdown:**\nðŸ”¹ **Google Drive** - Retrieve Document:\nThe automation starts by fetching a source document from Google Drive. This document contains structured content, with predefined boundary markers for easy segmentation.\n\nðŸ”¹ **Extract Text Content** - Once retrieved, the documentâ€™s text is extracted for processing. Special section boundary markers are used to divide the text into logical sections.\n\nðŸ”¹ **Code Node** - Create Context-Based Chunks:\nA custom code node processes the extracted text, identifying section boundaries and splitting the document into meaningful chunks. Each chunk is structured to retain its context within the entire document.\n\nðŸ”¹ **Loop Node** - Process Each Chunk:\nThe workflow loops through each chunk, ensuring they are processed individually while maintaining a connection to the overall document context.\n\nðŸ”¹ **Agent Node** - Generate Context for Each Chunk:\nWe use an Agent node powered by OpenAIâ€™s GPT-4.0-mini via OpenRouter to generate contextual metadata for each chunk, ensuring better retrieval accuracy.\n\nðŸ”¹ **Prepend Context to Chunks & Create Embeddings** - The generated context is prepended to the original chunk, creating context-rich embeddings that improve searchability.\n\nðŸ”¹ **Google Gemini** - Text Embeddings:\nThe processed text is passed through Google Gemini text-embedding-004, which converts the text into semantic vector representations.\n\nðŸ”¹ **Pinecone Vector Store** - Store Embeddings:\nThe final embeddings, along with the enriched chunk content and metadata, are stored in Pinecone, making them easily retrievable for RAG-based AI applications.\n\n**Use Case:**\nThis automation enhances RAG retrieval by ensuring each chunk is contextually aware of the entire document, leading to more accurate AI responses. Itâ€™s perfect for applications that require semantic search, AI-powered knowledge management, or intelligent document retrieval.\n\nBy implementing context-based chunking, this workflow ensures that LLMs retrieve the most relevant data, improving response quality and accuracy in AI-driven applications.\n\n[![Video Thumbnail](https://img.youtube.com/vi/qBeWP65I4hg/maxresdefault.jpg)](https://www.youtube.com/watch?v=qBeWP65I4hg)",
  "featuredImage": "/data/workflows/2871/2871.webp",
  "author": {
    "id": 101,
    "slug": "ailistmaster",
    "name": "Udit Rawat",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 10040,
  "downloads": 1004,
  "createdAt": "2025-02-09T19:03:21.513Z",
  "updatedAt": "2026-01-16T08:26:57.999Z",
  "publishedAt": "2025-02-09T19:03:21.513Z",
  "nodes": 17,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/2871",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "RAG: context-aware chunking | Google Drive to Pinecone via OpenRouter & Gemini",
    "workflowName": "RAG: context-aware chunking | Google Drive to Pinecone via OpenRouter & Gemini",
    "description": "Workflow based **on** the following article.\nhttps://www.anthropic.com/news/contextual-retrieval\n\nThis n8n automation is designed to extract, process, and store content from documents into a **Pinecone** vector store using context-based chunking. The workflow enhances retrieval accuracy in **RAG (Retrieval-Augmented Generation)** setups by ensuring each chunk retains meaningful context.\n\n**Workflow Breakdown:**\nðŸ”¹ **Google Drive** - Retrieve Document:\nThe automation starts by fetching a source document from Google Drive. This document contains structured content, with predefined boundary markers for easy segmentation.\n\nðŸ”¹ **Extract Text Content** - Once retrieved, the documentâ€™s text is extracted for processing. Special section boundary markers are used to divide the text into logical sections.\n\nðŸ”¹ **Code Node** - Create Context-Based Chunks:\nA custom code node processes the extracted text, identifying section boundaries and splitting the document into meaningful chunks. Each chunk is structured to retain its context within the entire document.\n\nðŸ”¹ **Loop Node** - Process Each Chunk:\nThe workflow loops through each chunk, ensuring they are processed individually while maintaining a connection to the overall document context.\n\nðŸ”¹ **Agent Node** - Generate Context for Each Chunk:\nWe use an Agent node powered by OpenAIâ€™s GPT-4.0-mini via OpenRouter to generate contextual metadata for each chunk, ensuring better retrieval accuracy.\n\nðŸ”¹ **Prepend Context to Chunks & Create Embeddings** - The generated context is prepended to the original chunk, creating context-rich embeddings that improve searchability.\n\nðŸ”¹ **Google Gemini** - Text Embeddings:\nThe processed text is passed through Google Gemini text-embedding-004, which converts the text into semantic vector representations.\n\nðŸ”¹ **Pinecone Vector Store** - Store Embeddings:\nThe final embeddings, along with the enriched chunk content and metadata, are stored in Pinecone, making them easily retrievable for RAG-based AI applications.\n\n**Use Case:**\nThis automation enhances RAG retrieval by ensuring each chunk is contextually aware of the entire document, leading to more accurate AI responses. Itâ€™s perfect for applications that require semantic search, AI-powered knowledge management, or intelligent document retrieval.\n\nBy implementing context-based chunking, this workflow ensures that LLMs retrieve the most relevant data, improving response quality and accuracy in AI-driven applications.\n\n[![Video Thumbnail](https://img.youtube.com/vi/qBeWP65I4hg/maxresdefault.jpg)](https://www.youtube.com/watch?v=qBeWP65I4hg)",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When clicking â€˜Test workflowâ€™",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Loop Over Items",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "OpenRouter Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "role": "lmChatOpenRouter",
      "configDescription": "Version 1"
    },
    {
      "name": "Pinecone Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStorePinecone",
      "role": "vectorStorePinecone",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings Google Gemini",
      "type": "@n8n/n8n-nodes-langchain.embeddingsGoogleGemini",
      "role": "embeddingsGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1"
    },
    {
      "name": "Recursive Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
      "role": "textSplitterRecursiveCharacterTextSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "Get Document From Google Drive",
      "type": "n8n-nodes-base.googleDrive",
      "role": "googleDrive",
      "configDescription": "Version 3"
    },
    {
      "name": "Extract Text Data From Google Document",
      "type": "n8n-nodes-base.extractFromFile",
      "role": "extractFromFile",
      "configDescription": "Version 1"
    },
    {
      "name": "Split Document Text Into Sections",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Prepare Sections For Looping",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "AI Agent - Prepare Context",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.7"
    },
    {
      "name": "Concatenate the context and section text",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}