{
  "id": 2315,
  "slug": "2315",
  "title": "Autonomous AI crawler",
  "description": "This workflow with AI agent is designed to navigate through the page to retrieve specific type of information (in this example: social media profile links). \n\nThe agent is equipped with 2 tools:\n- **text tool:** to retrieve all the text from the page, \n- **URLs tool:** to extract all possible links from the page.\n\nðŸ’¡ You can edit prompt and JSON schema connected to the agent in order to return other data then social media profile links. \n\nðŸ‘‰ This workflow uses Supabase as storage (input/output). Feel free to change it to any other database of your choice.  \n\nðŸŽ¬ See this workflow in action [in my YouTube video](https://youtu.be/2W09puFZwtY). \n\n## How it works?\n\nThe workflow uses the input URL (website) as a starting point to retrieve the data (e.g. example.com). Using the \"URLs tool\", the agent is able to retrieve all links from the page and navigate to them. \n\nFor example, if you want to retrieve contact information, agent will try to find a subpage that might contain this information (e.g. example.com/contact) and extract the information using the text tool.  \n\n## Set up steps\n\n1. Connect database with input data (website addresses) or pin sample data to trigger node. \n2. Configure the crawling agent to retrieve the desired data (e.g. modify prompt and/or parsing schema).\n3. Set credentials for OpenAI.\n4. Optionally: split agent tools to separate workflows. \n\nIf you like this workflow, please subscribe to [my YouTube channel](https://www.youtube.com/@workfloows/) and/or [my newsletter](https://workfloows.com/).",
  "featuredImage": "/data/workflows/2315/2315.webp",
  "author": {
    "id": 101,
    "slug": "workfloows",
    "name": "Oskar",
    "avatar": ""
  },
  "categories": [
    "Lead Generation",
    "AI Chatbot"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 72203,
  "downloads": 7220,
  "createdAt": "2024-07-05T09:21:06.526Z",
  "updatedAt": "2026-01-16T08:23:58.747Z",
  "publishedAt": "2024-07-05T09:21:06.526Z",
  "nodes": 38,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/2315",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Autonomous AI crawler",
    "workflowName": "Autonomous AI crawler",
    "description": "This workflow with AI agent is designed to navigate through the page to retrieve specific type of information (in this example: social media profile links). \n\nThe agent is equipped with 2 tools:\n- **text tool:** to retrieve all the text from the page, \n- **URLs tool:** to extract all possible links from the page.\n\nðŸ’¡ You can edit prompt and JSON schema connected to the agent in order to return other data then social media profile links. \n\nðŸ‘‰ This workflow uses Supabase as storage (input/output). Feel free to change it to any other database of your choice.  \n\nðŸŽ¬ See this workflow in action [in my YouTube video](https://youtu.be/2W09puFZwtY). \n\n## How it works?\n\nThe workflow uses the input URL (website) as a starting point to retrieve the data (e.g. example.com). Using the \"URLs tool\", the agent is able to retrieve all links from the page and navigate to them. \n\nFor example, if you want to retrieve contact information, agent will try to find a subpage that might contain this information (e.g. example.com/contact) and extract the information using the text tool.  \n\n## Set up steps\n\n1. Connect database with input data (website addresses) or pin sample data to trigger node. \n2. Configure the crawling agent to retrieve the desired data (e.g. modify prompt and/or parsing schema).\n3. Set credentials for OpenAI.\n4. Optionally: split agent tools to separate workflows. \n\nIf you like this workflow, please subscribe to [my YouTube channel](https://www.youtube.com/@workfloows/) and/or [my newsletter](https://workfloows.com/).",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Text",
      "type": "@n8n/n8n-nodes-langchain.toolWorkflow",
      "role": "toolWorkflow",
      "configDescription": "Version 1.1"
    },
    {
      "name": "URLs",
      "type": "@n8n/n8n-nodes-langchain.toolWorkflow",
      "role": "toolWorkflow",
      "configDescription": "Version 1.1"
    },
    {
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1"
    },
    {
      "name": "JSON Parser",
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "role": "outputParserStructured",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Map company name and website",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.3"
    },
    {
      "name": "Execute workflow",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Get companies",
      "type": "n8n-nodes-base.supabase",
      "role": "supabase",
      "configDescription": "Version 1"
    },
    {
      "name": "Select company name and website",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.3"
    },
    {
      "name": "Set social media array",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.3"
    },
    {
      "name": "Merge all data",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 2.1"
    },
    {
      "name": "Insert new row",
      "type": "n8n-nodes-base.supabase",
      "role": "supabase",
      "configDescription": "Version 1"
    },
    {
      "name": "Convert HTML to Markdown",
      "type": "n8n-nodes-base.markdown",
      "role": "markdown",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Retrieve URLs",
      "type": "n8n-nodes-base.html",
      "role": "html",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Split out URLs",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Remove duplicated",
      "type": "n8n-nodes-base.removeDuplicates",
      "role": "removeDuplicates",
      "configDescription": "Version 1"
    },
    {
      "name": "Set domain to path",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.3"
    },
    {
      "name": "Filter out invalid URLs",
      "type": "n8n-nodes-base.filter",
      "role": "filter",
      "configDescription": "Version 2"
    },
    {
      "name": "Aggregate URLs",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "Filter out empty hrefs",
      "type": "n8n-nodes-base.filter",
      "role": "filter",
      "configDescription": "Version 2"
    },
    {
      "name": "Set domain (text)",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.3"
    },
    {
      "name": "Add protocool to domain (text)",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.3"
    },
    {
      "name": "Get website (text)",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Set response (text)",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.3"
    },
    {
      "name": "Set domain (URL)",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.3"
    },
    {
      "name": "Get website (URL)",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Set response (URL)",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.3"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note7",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Add protocool to domain (URL)",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.3"
    },
    {
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note9",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Crawl website",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.6"
    }
  ]
}