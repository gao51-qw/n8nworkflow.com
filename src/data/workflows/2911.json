{
  "id": 2911,
  "slug": "2911",
  "title": "Extract license plate number from image uploaded via an n8n form",
  "description": "## What it does\n\nThis is a simplistic demo workflow showing how to extract a license plate number from an image of a car submitted via a form – or in more general terms showcasing how you can:\n\n* use a form trigger to upload files and feed it into an LLM\n* use a changeable LLM model for image-to-text analysis\n\n\n## Set up steps\n\n* Import the workflow\n* Ensure you have registered and account, purchased some credits and created and API key for [OpenRouter.ai](https://openrouter.ai)\n* Create/adapt the OpenRouter credential with your indivial API key for OpenRouter\n* \"Test workflow\" and submit an image of a car with license plate to extract its number\n\n\n## How to adapt\n\nBy changing the \"prompt\" in th \"Settings\" node you can quickly adapt this exemplatory workflow to other image-to-text use cases, such as:\n\n* summarization: \"summarize what's seen in the image\"\n* location finding: \"identify the location where the image was taken\"\n* text extraction: \"extract all text from the image and return it as markdown\"\n\nThanks to using OpenRouter, you also can quickly experiment with finding good model choices by simply changing the \"model\" in the \"Settings\" node. The following models gave good results for this demo use-case:\n\n* google/gemini-2.0-flash-001\n* meta-llama/llama-3.2-90b-vision-instruct\n* openai/gpt-4o\n\nThe llama-3.2-11b and even claude-3.5-sonnet didn't recognize all characters in all test images.\n\nUsing a generic LLM-model offers a quick way of prototyping an image-to-text application. For specific use cases in serious and scalable production deployments, consider using an API based service specifically made to that purpose, such as:\n\n* Google Cloud Vision API\n* Microsoft Azure Computer Vision\n* Azure AI Document Intelligence\n* Amazon Textract\n\n",
  "featuredImage": "/data/workflows/2911/2911.webp",
  "author": {
    "id": 101,
    "slug": "dnolde",
    "name": "Daniel Nolde",
    "avatar": ""
  },
  "categories": [
    "Document Extraction",
    "Multimodal AI"
  ],
  "complexityLevel": "intermediate",
  "price": 0,
  "visitors": 2637,
  "downloads": 263,
  "createdAt": "2025-02-15T06:43:10.899Z",
  "updatedAt": "2026-01-16T08:27:08.831Z",
  "publishedAt": "2025-02-15T06:43:10.899Z",
  "nodes": 5,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/2911",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Extract license plate number from image uploaded via an n8n form",
    "workflowName": "Extract license plate number from image uploaded via an n8n form",
    "description": "## What it does\n\nThis is a simplistic demo workflow showing how to extract a license plate number from an image of a car submitted via a form – or in more general terms showcasing how you can:\n\n* use a form trigger to upload files and feed it into an LLM\n* use a changeable LLM model for image-to-text analysis\n\n\n## Set up steps\n\n* Import the workflow\n* Ensure you have registered and account, purchased some credits and created and API key for [OpenRouter.ai](https://openrouter.ai)\n* Create/adapt the OpenRouter credential with your indivial API key for OpenRouter\n* \"Test workflow\" and submit an image of a car with license plate to extract its number\n\n\n## How to adapt\n\nBy changing the \"prompt\" in th \"Settings\" node you can quickly adapt this exemplatory workflow to other image-to-text use cases, such as:\n\n* summarization: \"summarize what's seen in the image\"\n* location finding: \"identify the location where the image was taken\"\n* text extraction: \"extract all text from the image and return it as markdown\"\n\nThanks to using OpenRouter, you also can quickly experiment with finding good model choices by simply changing the \"model\" in the \"Settings\" node. The following models gave good results for this demo use-case:\n\n* google/gemini-2.0-flash-001\n* meta-llama/llama-3.2-90b-vision-instruct\n* openai/gpt-4o\n\nThe llama-3.2-11b and even claude-3.5-sonnet didn't recognize all characters in all test images.\n\nUsing a generic LLM-model offers a quick way of prototyping an image-to-text application. For specific use cases in serious and scalable production deployments, consider using an API based service specifically made to that purpose, such as:\n\n* Google Cloud Vision API\n* Microsoft Azure Computer Vision\n* Azure AI Document Intelligence\n* Amazon Textract",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Basic LLM Chain",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "role": "chainLlm",
      "configDescription": "Version 1.5"
    },
    {
      "name": "FormResultPage",
      "type": "n8n-nodes-base.form",
      "role": "form",
      "configDescription": "Version 1"
    },
    {
      "name": "OpenRouter LLM",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "role": "lmChatOpenRouter",
      "configDescription": "Version 1"
    },
    {
      "name": "Settings",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "FromTrigger",
      "type": "n8n-nodes-base.formTrigger",
      "role": "formTrigger",
      "configDescription": "Version 2.2"
    }
  ]
}