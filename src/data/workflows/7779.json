{
  "id": 7779,
  "slug": "7779",
  "title": "Build an advanced multi-query RAG system with Supabase and GPT-5",
  "description": "Go beyond basic Retrieval-Augmented Generation (RAG) with this advanced template. While a simple RAG setup can answer straightforward questions, it often fails when faced with complex queries and can be polluted by irrelevant information. This workflow introduces a sophisticated architecture that empowers your AI agent to think and act like a true research assistant.\n\nBy decoupling the agent from the knowledge base with a smart sub-workflow, this template enables multi-query decomposition, relevance-based filtering, and an intermediate reasoning step. The result is an AI agent that can handle complex questions, filter out noise, and synthesize high-quality, comprehensive answers based on your data in Supabase.\n\n## **Who is this for?**\n\n* **AI and automation developers:** Anyone building sophisticated Q&A bots, internal knowledge base assistants, or complex research agents.\n* **n8n power users:** Users looking to push the boundaries of AI agents in n8n by implementing production-ready, robust architectural patterns.\n* **Anyone building a RAG system:** This provides a superior architectural pattern that overcomes the common limitations of basic RAG setups, leading to dramatically better performance.\n\n## **What problem does this solve?**\n\n* **Handles complex questions:** A standard RAG agent sends one query and gets one set of results. This agent is designed to break down a complex question like \"How does natural selection work at the molecular, organismal, and population levels?\" into multiple, targeted sub-queries, ensuring all facets of the question are answered.\n* **Prevents low-quality answers:** A simple RAG agent can be fed irrelevant information if the semantic search returns low-quality matches. This workflow includes a crucial **relevance filtering** step, discarding any data chunks that fall below a set similarity score, ensuring the agent only reasons with high-quality context.\n* **Improves answer quality and coherence:** By introducing a dedicated **\"Think\" tool**, the agent has a private scratchpad to synthesize the information it has gathered from multiple queries. This intermediate reasoning step allows it to connect the dots and structure a more comprehensive and logical final answer.\n* **Gives you more control and flexibility:** By using a sub-workflow to handle data retrieval, you can add any custom logic you need (like filtering, formatting, or even calling other APIs) without complicating the main agent's design.\n\n## **How it works**\n\nThis template consists of a main agent workflow and a smart sub-workflow that handles knowledge retrieval.\n\n1.  **Multi-query decomposition:** When you ask the **AI Agent** a complex question, its system prompt instructs it to first break it down into an array of multiple, simpler sub-queries.\n2.  **Decoupling with a sub-workflow:** The agent doesn't have direct access to the vector store. Instead, it calls a **\"Query knowledge base\"** tool, which is a sub-workflow. It sends the entire array of sub-queries to this sub-workflow in a single tool call.\n3.  **Iterative retrieval & filtering (in the sub-workflow):** The sub-workflow loops through each sub-query. For each one, it queries your **Supabase Vector Store**. It then checks the similarity score of the returned data chunks and uses a **Filter** node to discard any that are not highly relevant (the default is a score &gt; 0.4).\n4.  **Intermediate reasoning step:** The sub-workflow returns all the high-quality, filtered information to the main agent. The agent is then instructed to use its **Think** tool to review this information, synthesize the key points, and structure a plan for its final, comprehensive answer.\n\n## **Setup**\n\n1.  **Connect your accounts:**\n    * **Supabase:** In the **sub-workflow** (\"RAG sub-workflow\"), connect your Supabase account to the **Supabase Vector Store** node and select your table.\n    * **OpenAI:** Connect your OpenAI account in two places: to the **Embeddings OpenAI** node (in the sub-workflow) and to the **OpenAI Chat Model** node (in the main workflow).\n2.  **Customize the agent's purpose:** In the main workflow, edit the **AI Agent's system prompt**. Change the context from a \"biology course\" to whatever your knowledge base is about.\n3.  **Adjust the relevance filter:** In the sub-workflow, you can change the `0.4` threshold in the **Filter** node to be more or less strict about the quality of the information you want the agent to use.\n4.  **Activate the workflow** and start asking complex questions!\n\n\n## **Taking it further**\n\n* **Integrate different vector stores:** The logic is decoupled. You can easily swap the Supabase Vector Store node in the sub-workflow with a Pinecone, Weaviate, or any other vector store node without changing the main agent's logic.\n* **Add more tools:** Give the main agent other capabilities, like a web search a way to interact with your tech stack. The agent can then decide whether to use its internal knowledge base, search the web, or both, to answer a question.\n* **Better prompting:** You could further work on the Agent's system prompt to increase its capacity to provide high-quality answers by being even better at leveraging the provided chunks.",
  "featuredImage": "/data/workflows/7779/7779.webp",
  "author": {
    "id": 101,
    "slug": "duv",
    "name": "Guillaume Duvernay",
    "avatar": ""
  },
  "categories": [
    "AI RAG",
    "Multimodal AI"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 1593,
  "downloads": 159,
  "createdAt": "2025-08-23T22:52:17.153Z",
  "updatedAt": "2026-01-16T08:52:18.961Z",
  "publishedAt": "2025-08-23T22:52:17.153Z",
  "nodes": 22,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/7779",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Build an advanced multi-query RAG system with Supabase and GPT-5",
    "workflowName": "Build an advanced multi-query RAG system with Supabase and GPT-5",
    "description": "Go beyond basic Retrieval-Augmented Generation (RAG) with this advanced template. While a simple RAG setup can answer straightforward questions, it often fails when faced with complex queries and can be polluted by irrelevant information. This workflow introduces a sophisticated architecture that empowers your AI agent to think and act like a true research assistant.\n\nBy decoupling the agent from the knowledge base with a smart sub-workflow, this template enables multi-query decomposition, relevance-based filtering, and an intermediate reasoning step. The result is an AI agent that can handle complex questions, filter out noise, and synthesize high-quality, comprehensive answers based on your data in Supabase.\n\n## **Who is this for?**\n\n* **AI and automation developers:** Anyone building sophisticated Q&A bots, internal knowledge base assistants, or complex research agents.\n* **n8n power users:** Users looking to push the boundaries of AI agents in n8n by implementing production-ready, robust architectural patterns.\n* **Anyone building a RAG system:** This provides a superior architectural pattern that overcomes the common limitations of basic RAG setups, leading to dramatically better performance.\n\n## **What problem does this solve?**\n\n* **Handles complex questions:** A standard RAG agent sends one query and gets one set of results. This agent is designed to break down a complex question like \"How does natural selection work at the molecular, organismal, and population levels?\" into multiple, targeted sub-queries, ensuring all facets of the question are answered.\n* **Prevents low-quality answers:** A simple RAG agent can be fed irrelevant information if the semantic search returns low-quality matches. This workflow includes a crucial **relevance filtering** step, discarding any data chunks that fall below a set similarity score, ensuring the agent only reasons with high-quality context.\n* **Improves answer quality and coherence:** By introducing a dedicated **\"Think\" tool**, the agent has a private scratchpad to synthesize the information it has gathered from multiple queries. This intermediate reasoning step allows it to connect the dots and structure a more comprehensive and logical final answer.\n* **Gives you more control and flexibility:** By using a sub-workflow to handle data retrieval, you can add any custom logic you need (like filtering, formatting, or even calling other APIs) without complicating the main agent's design.\n\n## **How it works**\n\nThis template consists of a main agent workflow and a smart sub-workflow that handles knowledge retrieval.\n\n1.  **Multi-query decomposition:** When you ask the **AI Agent** a complex question, its system prompt instructs it to first break it down into an array of multiple, simpler sub-queries.\n2.  **Decoupling with a sub-workflow:** The agent doesn't have direct access to the vector store. Instead, it calls a **\"Query knowledge base\"** tool, which is a sub-workflow. It sends the entire array of sub-queries to this sub-workflow in a single tool call.\n3.  **Iterative retrieval & filtering (in the sub-workflow):** The sub-workflow loops through each sub-query. For each one, it queries your **Supabase Vector Store**. It then checks the similarity score of the returned data chunks and uses a **Filter** node to discard any that are not highly relevant (the default is a score &gt; 0.4).\n4.  **Intermediate reasoning step:** The sub-workflow returns all the high-quality, filtered information to the main agent. The agent is then instructed to use its **Think** tool to review this information, synthesize the key points, and structure a plan for its final, comprehensive answer.\n\n## **Setup**\n\n1.  **Connect your accounts:**\n    * **Supabase:** In the **sub-workflow** (\"RAG sub-workflow\"), connect your Supabase account to the **Supabase Vector Store** node and select your table.\n    * **OpenAI:** Connect your OpenAI account in two places: to the **Embeddings OpenAI** node (in the sub-workflow) and to the **OpenAI Chat Model** node (in the main workflow).\n2.  **Customize the agent's purpose:** In the main workflow, edit the **AI Agent's system prompt**. Change the context from a \"biology course\" to whatever your knowledge base is about.\n3.  **Adjust the relevance filter:** In the sub-workflow, you can change the `0.4` threshold in the **Filter** node to be more or less strict about the quality of the information you want the agent to use.\n4.  **Activate the workflow** and start asking complex questions!\n\n\n## **Taking it further**\n\n* **Integrate different vector stores:** The logic is decoupled. You can easily swap the Supabase Vector Store node in the sub-workflow with a Pinecone, Weaviate, or any other vector store node without changing the main agent's logic.\n* **Add more tools:** Give the main agent other capabilities, like a web search a way to interact with your tech stack. The agent can then decide whether to use its internal knowledge base, search the web, or both, to answer a question.\n* **Better prompting:** You could further work on the Agent's system prompt to increase its capacity to provide high-quality answers by being even better at leveraging the provided chunks.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 2.2"
    },
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.3"
    },
    {
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Simple Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Split Out",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Aggregate chunks",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "Aggregate items",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "Any chunk?",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Clean RAG output",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Loop Over Items1",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "RAG sub-workflow",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "role": "executeWorkflowTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Query knowledge base",
      "type": "@n8n/n8n-nodes-langchain.toolWorkflow",
      "role": "toolWorkflow",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Supabase Vector Store1",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreSupabase",
      "role": "vectorStoreSupabase",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Embeddings OpenAI1",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Think",
      "type": "@n8n/n8n-nodes-langchain.toolThink",
      "role": "toolThink",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Keep score over 0.4",
      "type": "n8n-nodes-base.filter",
      "role": "filter",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Say no chunk match",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Prepare loop output",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    }
  ]
}