{
  "id": 4400,
  "slug": "4400",
  "title": "Build a PDF Document RAG System with Mistral OCR, Qdrant and Gemini AI",
  "description": "This workflow is designed to **process PDF documents** using **Mistral's OCR** capabilities, store the extracted text in a Qdrant vector database, and enable Retrieval-Augmented Generation (**RAG**) for answering questions. Here’s how it functions:  \n\nOnce configured, the workflow automates document ingestion, vectorization, and intelligent querying, enabling powerful RAG applications.\n\n---\n\n### **Benefits**\n\n* **End-to-End Automation**\n  No manual interaction is needed: documents are read, processed, and made queryable with minimal setup.\n\n* **Scalable and Modular**\n  The workflow uses subflows and batching, making it easy to scale and customize.\n\n* **Multi-Model Support**\n  Combines Mistral for OCR, OpenAI for embeddings, and Gemini for intelligent answering—taking advantage of the strengths of each.\n\n* **Real-Time Q\\&A**\n  With RAG integration, users can query document content through natural language and receive accurate responses grounded in the PDF data.\n\n* **Light or Full Mode**\n  Users can choose to index full page content or only summarized text, optimizing for either performance or richness.\n\n---\n\n### **How It Works** \n\n1. **PDF Processing with Mistral OCR**:  \n   - The workflow starts by uploading a PDF file to Mistral's API, which performs OCR to extract text and metadata.  \n   - The extracted content is split into manageable chunks (e.g., pages or sections) for further processing.  \n\n2. **Vector Storage in Qdrant**:  \n   - The extracted text is converted into embeddings using OpenAI's embedding model.  \n   - These embeddings are stored in a Qdrant vector database, enabling efficient similarity searches for RAG.  \n\n3. **Question-Answering with RAG**:  \n   - When a user submits a question via a chat interface, the workflow retrieves relevant text chunks from Qdrant using vector similarity.  \n   - A language model (Google Gemini) generates answers based on the retrieved context, providing accurate and context-aware responses.  \n\n4. **Optional Summarization**:  \n   - The workflow includes an optional summarization step using Google Gemini to condense the extracted text for faster processing or lighter RAG usage.  \n\n---\n\n### **Set Up Steps**  \nTo deploy this workflow in n8n, follow these steps:  \n\n1. **Configure Qdrant Database**:  \n   - Replace `QDRANTURL` and `COLLECTION` in the \"Create collection\" and \"Refresh collection\" nodes with your Qdrant instance details.  \n   - Ensure the Qdrant collection is configured with the correct vector size (e.g., 1536 for OpenAI embeddings) and distance metric (e.g., Cosine).  \n\n2. **Set Up Credentials**:  \n   - Add credentials for:  \n     - **Mistral Cloud API** (for OCR processing).  \n     - **OpenAI API** (for embeddings).  \n     - **Google Gemini API** (for chat and summarization).  \n     - **Google Drive** (if sourcing PDFs from Drive).  \n     - **Qdrant API** (for vector storage).  \n\n3. **PDF Source Configuration**:  \n   - If using Google Drive, specify the folder ID in the \"Search PDFs\" node.  \n   - Alternatively, modify the workflow to accept PDFs from other sources (e.g., direct uploads or external APIs).  \n\n4. **Customize Text Processing**:  \n   - Adjust chunk size and overlap in the \"Token Splitter\" node to optimize for your document type.  \n   - Choose between raw text or summarized content for RAG by toggling between the \"Set page\" and \"Summarization Chain\" nodes.  \n\n5. **Test the RAG**:  \n   - Trigger the workflow manually or via a chat message to verify OCR, embedding, and Qdrant storage.  \n   - Use the \"Question and Answer Chain\" node to test query responses.  \n\n6. **Optional Sub-Workflows**:  \n   - The workflow supports execution as a sub-workflow for batch processing (e.g., handling multiple PDFs).  \n\n----\n### **Need help customizing?**  \n[Contact me](mailto:info@n3w.it) for consulting and support or add me on [Linkedin](https://www.linkedin.com/in/davideboizza/). ",
  "featuredImage": "/data/workflows/4400/4400.webp",
  "author": {
    "id": 101,
    "slug": "n3witalia",
    "name": "Davide",
    "avatar": ""
  },
  "categories": [
    "Document Extraction",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 41620,
  "downloads": 4162,
  "createdAt": "2025-05-26T12:28:14.350Z",
  "updatedAt": "2026-01-16T08:34:11.605Z",
  "publishedAt": "2025-05-26T12:28:14.350Z",
  "nodes": 34,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/4400",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Build a PDF Document RAG System with Mistral OCR, Qdrant and Gemini AI",
    "workflowName": "Build a PDF Document RAG System with Mistral OCR, Qdrant and Gemini AI",
    "description": "This workflow is designed to **process PDF documents** using **Mistral's OCR** capabilities, store the extracted text in a Qdrant vector database, and enable Retrieval-Augmented Generation (**RAG**) for answering questions. Here’s how it functions:  \n\nOnce configured, the workflow automates document ingestion, vectorization, and intelligent querying, enabling powerful RAG applications.\n\n---\n\n### **Benefits**\n\n* **End-to-End Automation**\n  No manual interaction is needed: documents are read, processed, and made queryable with minimal setup.\n\n* **Scalable and Modular**\n  The workflow uses subflows and batching, making it easy to scale and customize.\n\n* **Multi-Model Support**\n  Combines Mistral for OCR, OpenAI for embeddings, and Gemini for intelligent answering—taking advantage of the strengths of each.\n\n* **Real-Time Q\\&A**\n  With RAG integration, users can query document content through natural language and receive accurate responses grounded in the PDF data.\n\n* **Light or Full Mode**\n  Users can choose to index full page content or only summarized text, optimizing for either performance or richness.\n\n---\n\n### **How It Works** \n\n1. **PDF Processing with Mistral OCR**:  \n   - The workflow starts by uploading a PDF file to Mistral's API, which performs OCR to extract text and metadata.  \n   - The extracted content is split into manageable chunks (e.g., pages or sections) for further processing.  \n\n2. **Vector Storage in Qdrant**:  \n   - The extracted text is converted into embeddings using OpenAI's embedding model.  \n   - These embeddings are stored in a Qdrant vector database, enabling efficient similarity searches for RAG.  \n\n3. **Question-Answering with RAG**:  \n   - When a user submits a question via a chat interface, the workflow retrieves relevant text chunks from Qdrant using vector similarity.  \n   - A language model (Google Gemini) generates answers based on the retrieved context, providing accurate and context-aware responses.  \n\n4. **Optional Summarization**:  \n   - The workflow includes an optional summarization step using Google Gemini to condense the extracted text for faster processing or lighter RAG usage.  \n\n---\n\n### **Set Up Steps**  \nTo deploy this workflow in n8n, follow these steps:  \n\n1. **Configure Qdrant Database**:  \n   - Replace `QDRANTURL` and `COLLECTION` in the \"Create collection\" and \"Refresh collection\" nodes with your Qdrant instance details.  \n   - Ensure the Qdrant collection is configured with the correct vector size (e.g., 1536 for OpenAI embeddings) and distance metric (e.g., Cosine).  \n\n2. **Set Up Credentials**:  \n   - Add credentials for:  \n     - **Mistral Cloud API** (for OCR processing).  \n     - **OpenAI API** (for embeddings).  \n     - **Google Gemini API** (for chat and summarization).  \n     - **Google Drive** (if sourcing PDFs from Drive).  \n     - **Qdrant API** (for vector storage).  \n\n3. **PDF Source Configuration**:  \n   - If using Google Drive, specify the folder ID in the \"Search PDFs\" node.  \n   - Alternatively, modify the workflow to accept PDFs from other sources (e.g., direct uploads or external APIs).  \n\n4. **Customize Text Processing**:  \n   - Adjust chunk size and overlap in the \"Token Splitter\" node to optimize for your document type.  \n   - Choose between raw text or summarized content for RAG by toggling between the \"Set page\" and \"Summarization Chain\" nodes.  \n\n5. **Test the RAG**:  \n   - Trigger the workflow manually or via a chat message to verify OCR, embedding, and Qdrant storage.  \n   - Use the \"Question and Answer Chain\" node to test query responses.  \n\n6. **Optional Sub-Workflows**:  \n   - The workflow supports execution as a sub-workflow for batch processing (e.g., handling multiple PDFs).  \n\n----\n### **Need help customizing?**  \n[Contact me](mailto:info@n3w.it) for consulting and support or add me on [Linkedin](https://www.linkedin.com/in/davideboizza/).",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Mistral Upload",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Mistral Signed URL",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Mistral DOC OCR",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "When clicking ‘Test workflow’",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Loop Over Items",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "Refresh collection",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Embeddings OpenAI",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1"
    },
    {
      "name": "Token Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterTokenSplitter",
      "role": "textSplitterTokenSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Question and Answer Chain",
      "type": "@n8n/n8n-nodes-langchain.chainRetrievalQa",
      "role": "chainRetrievalQa",
      "configDescription": "Version 1.5"
    },
    {
      "name": "Google Gemini Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Vector Store Retriever",
      "type": "@n8n/n8n-nodes-langchain.retrieverVectorStore",
      "role": "retrieverVectorStore",
      "configDescription": "Version 1"
    },
    {
      "name": "Qdrant Vector Store1",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreQdrant",
      "role": "vectorStoreQdrant",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Embeddings OpenAI1",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Code",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Wait",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Qdrant Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreQdrant",
      "role": "vectorStoreQdrant",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Loop Over Items1",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "Execute Workflow",
      "type": "n8n-nodes-base.executeWorkflow",
      "role": "executeWorkflow",
      "configDescription": "Version 1.2"
    },
    {
      "name": "When Executed by Another Workflow",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "role": "executeWorkflowTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Edit Fields1",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Create collection",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Summarization Chain",
      "type": "@n8n/n8n-nodes-langchain.chainSummarization",
      "role": "chainSummarization",
      "configDescription": "Version 2"
    },
    {
      "name": "Google Gemini Chat Model1",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Set page",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Set summary",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Search PDFs",
      "type": "n8n-nodes-base.googleDrive",
      "role": "googleDrive",
      "configDescription": "Version 3"
    },
    {
      "name": "Get PDF",
      "type": "n8n-nodes-base.googleDrive",
      "role": "googleDrive",
      "configDescription": "Version 3"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}