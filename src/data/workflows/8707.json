{
  "id": 8707,
  "slug": "8707",
  "title": "From sitemap crawling to vector storage: Creating an efficient workflow for RAG",
  "description": "**This template crawls a website from its sitemap, deduplicates URLs in Supabase, scrapes pages with Crawl4AI, cleans and validates the text, then stores content + metadata in a Supabase vector store using OpenAI embeddings. It’s a reliable, repeatable pipeline for building searchable knowledge bases, SEO research corpora, and RAG datasets.**\n⸻\n## **Good to know**\n\t•\tBuilt-in de-duplication via a scrape_queue table (status: pending/completed/error).\n\t•\tResilient flow: waits, retries, and marks failed tasks.\n\t•\tCosts depend on Crawl4AI usage and OpenAI embeddings.\n\t•\tReplace any placeholders (API keys, tokens, URLs) before running.\n\t•\tRespect website robots/ToS and applicable data laws when scraping.\n\n## **How it works**\n\t1.\tSitemap fetch & parse — Load sitemap.xml, extract all URLs.\n\t2.\tDe-dupe — Normalize URLs, check Supabase scrape_queue; insert only new ones.\n\t3.\tScrape — Send URLs to Crawl4AI; poll task status until completed.\n\t4.\tClean & score — Remove boilerplate/markup, detect content type, compute quality metrics, extract metadata (title, domain, language, length).\n\t5.\tChunk & embed — Split text, create OpenAI embeddings.\n\t6.\tStore — Upsert into Supabase vector store (documents) with metadata; update job status.\n## **Requirements**\n\t•\tSupabase (Postgres + Vector extension enabled)\n\t•\tCrawl4AI API key (or header auth)\n\t•\tOpenAI API key (for embeddings)\n\t•\tn8n credentials set for HTTP, Postgres/Supabase\n## **How to use**\n\t1.\tConfigure credentials (Supabase/Postgres, Crawl4AI, OpenAI).\n\t2.\t(Optional) Run the provided SQL to create scrape_queue and documents.\n\t3.\tSet your sitemap URL in the HTTP Request node.\n\t4.\tExecute the workflow (manual trigger) and monitor Supabase statuses.\n\t5.\tQuery your documents table or vector store from your app/RAG stack.\n\n## **Potential Use Cases**\n\nThis automation is ideal for:\n\n- Market research teams collecting competitive data\n- Content creators monitoring web trends\n- SEO specialists tracking website content updates\n- Analysts gathering structured data for insights\n- Anyone needing reliable, structured web content for analysis\n\n## **Need help customizing?**\nContact me for consulting and support: [LinkedIn](https://www.linkedin.com/in/mariela-ceo-founder/)\n",
  "featuredImage": "/data/workflows/8707/8707.webp",
  "author": {
    "id": 101,
    "slug": "marielabg",
    "name": "Mariela Slavenova",
    "avatar": ""
  },
  "categories": [
    "Market Research",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 787,
  "downloads": 78,
  "createdAt": "2025-09-18T07:44:34.895Z",
  "updatedAt": "2026-01-16T08:57:18.962Z",
  "publishedAt": "2025-09-18T07:44:34.895Z",
  "nodes": 40,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/8707",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "From sitemap crawling to vector storage: Creating an efficient workflow for RAG",
    "workflowName": "From sitemap crawling to vector storage: Creating an efficient workflow for RAG",
    "description": "**This template crawls a website from its sitemap, deduplicates URLs in Supabase, scrapes pages with Crawl4AI, cleans and validates the text, then stores content + metadata in a Supabase vector store using OpenAI embeddings. It’s a reliable, repeatable pipeline for building searchable knowledge bases, SEO research corpora, and RAG datasets.**\n⸻\n## **Good to know**\n\t•\tBuilt-in de-duplication via a scrape_queue table (status: pending/completed/error).\n\t•\tResilient flow: waits, retries, and marks failed tasks.\n\t•\tCosts depend on Crawl4AI usage and OpenAI embeddings.\n\t•\tReplace any placeholders (API keys, tokens, URLs) before running.\n\t•\tRespect website robots/ToS and applicable data laws when scraping.\n\n## **How it works**\n\t1.\tSitemap fetch & parse — Load sitemap.xml, extract all URLs.\n\t2.\tDe-dupe — Normalize URLs, check Supabase scrape_queue; insert only new ones.\n\t3.\tScrape — Send URLs to Crawl4AI; poll task status until completed.\n\t4.\tClean & score — Remove boilerplate/markup, detect content type, compute quality metrics, extract metadata (title, domain, language, length).\n\t5.\tChunk & embed — Split text, create OpenAI embeddings.\n\t6.\tStore — Upsert into Supabase vector store (documents) with metadata; update job status.\n## **Requirements**\n\t•\tSupabase (Postgres + Vector extension enabled)\n\t•\tCrawl4AI API key (or header auth)\n\t•\tOpenAI API key (for embeddings)\n\t•\tn8n credentials set for HTTP, Postgres/Supabase\n## **How to use**\n\t1.\tConfigure credentials (Supabase/Postgres, Crawl4AI, OpenAI).\n\t2.\t(Optional) Run the provided SQL to create scrape_queue and documents.\n\t3.\tSet your sitemap URL in the HTTP Request node.\n\t4.\tExecute the workflow (manual trigger) and monitor Supabase statuses.\n\t5.\tQuery your documents table or vector store from your app/RAG stack.\n\n## **Potential Use Cases**\n\nThis automation is ideal for:\n\n- Market research teams collecting competitive data\n- Content creators monitoring web trends\n- SEO specialists tracking website content updates\n- Analysts gathering structured data for insights\n- Anyone needing reliable, structured web content for analysis\n\n## **Need help customizing?**\nContact me for consulting and support: [LinkedIn](https://www.linkedin.com/in/mariela-ceo-founder/)",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When clicking ‘Test workflow’",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "HTTP Request",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "XML",
      "type": "n8n-nodes-base.xml",
      "role": "xml",
      "configDescription": "Version 1"
    },
    {
      "name": "Split Out",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Loop Over Items",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "Wait",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "If",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1"
    },
    {
      "name": "Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterCharacterTextSplitter",
      "role": "textSplitterCharacterTextSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings OpenAI",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Edit Fields",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Crawl4AI_Task Status",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Loop Over Items1",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "If2",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Split Out1",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Format the URL",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Check if the URL is in the Supabase Table",
      "type": "n8n-nodes-base.supabase",
      "role": "supabase",
      "configDescription": "Version 1"
    },
    {
      "name": "Format the Output from the Supabase node",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "If \"shouldInsert\" is true",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "URL in a new row",
      "type": "n8n-nodes-base.supabase",
      "role": "supabase",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "CREATE TABLE scrape_queue in Supabase",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "CREATE TABLE scrape_queue in Supabase1",
      "type": "n8n-nodes-base.postgres",
      "role": "postgres",
      "configDescription": "Version 2.6"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Crawl4ai Web Page Scrape",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Remove redundant data from the scraping",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Supabase Vector Store_documents",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreSupabase",
      "role": "vectorStoreSupabase",
      "configDescription": "Version 1"
    },
    {
      "name": "Get a row - scrape_queue Table",
      "type": "n8n-nodes-base.supabase",
      "role": "supabase",
      "configDescription": "Version 1"
    },
    {
      "name": "Update a row in scrape_queue Table",
      "type": "n8n-nodes-base.supabase",
      "role": "supabase",
      "configDescription": "Version 1"
    },
    {
      "name": "Update a row in scrape_queue Table1",
      "type": "n8n-nodes-base.supabase",
      "role": "supabase",
      "configDescription": "Version 1"
    },
    {
      "name": "Wait1",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Quality Filter Node",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Content Type Detection",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Better Metadata Extraction",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "If1",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Update a row in scrape_queue Table2",
      "type": "n8n-nodes-base.supabase",
      "role": "supabase",
      "configDescription": "Version 1"
    },
    {
      "name": "Task_id Counter",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    }
  ]
}