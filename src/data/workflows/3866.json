{
  "id": 3866,
  "slug": "3866",
  "title": "Asynchronous bulk web scraping with Bright Data & webhook notifications",
  "description": "### Who this is for\nThe Async Structured Bulk Data Extract with Bright Data Web Scraper workflow is designed for data engineers, market researchers, competitive intelligence teams, and automation developers who need to programmatically collect and structure high-volume data from the web using Bright Data's dataset and snapshot capabilities.\n\nThis workflow is built for:\n\n1. **Data Engineers** - Building large-scale ETL pipelines from web sources\n\n2. **Market Researchers** - Collecting bulk data for analysis across competitors or products\n\n3. **Growth Hackers & Analysts** - Mining structured datasets for insights\n\n4. **Automation Developers** - Needing reliable snapshot-triggered scrapers\n\n5. **Product Managers** - Overseeing data-backed decision-making using live web information\n\n### What problem is this workflow solving?\nWeb scraping at scale often requires asynchronous operations, including waiting for data preparation and snapshots to complete. Manual handling of this process can lead to timeouts, errors, or inconsistencies in results. \n\nThis workflow automates the entire process of submitting a scraping request, waiting for the snapshot, retrieving the data, and notifying downstream systems all in a structured, repeatable fashion.\n\nIt solves:\n\n1. Asynchronous snapshot completion handling\n\n2. Reliable retrieval of large datasets using Bright Data\n\n3. Automated delivery of scraped results via webhook\n\n4. Disk persistence for traceability or historical analysis\n\n### What this workflow does\n1. **Set Bright Data Dataset ID & Request URL**: Takes in the Dataset ID and Bright Data API endpoint used to trigger the scrape job\n\n2. **HTTP Request**: Sends an authenticated request to the Bright Data API to start a scraping snapshot job\n\n3. **Wait Until Snapshot is Ready**: Implements a loop or wait mechanism that checks snapshot status (e.g., polling every 30 seconds) until completion i.e ready state\n\n4. **Download Snapshot**: Downloads the structured dataset snapshot once ready\n\n5. **Persist Response to Disk**: Saves the dataset to disk for archival, review, or local processing\n\n6. **Webhook Notification**: Sends the final result or a summary of it to an external webhook\n\n### Setup\n\n- Sign up at [Bright Data](https://brightdata.com/).\n- Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.\n- In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).\n![Header Authentication.png](fileId:1266)\nThe Value field should be set with the \n**Bearer XXXXXXXXXXXXXX**. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.\n- Update the **Set Dataset Id, Request URL** for setting the brand content URL.\n- Update the Webhook HTTP Request node with the Webhook endpoint of your choice.\n\n### How to customize this workflow to your needs\n\n1. **Polling Strategy** : Adjust polling interval (e.g., every 15–60 seconds) based on snapshot complexity\n2. **Input Flexibility** : Accept datasetId and request URL dynamically from a webhook trigger or input form\n3. **Webhook Output** : Send notifications to -\n\n\t- Internal APIs – for use in dashboards\n\n\t- Zapier/Make – for multi-step automation\n\n4. **Persistence**\n\t- Save output to:\n\n\t\t- Remote FTP or SFTP storage\n\t\t- Amazon S3, Google Cloud Storage etc.\n",
  "featuredImage": "/data/workflows/3866/3866.webp",
  "author": {
    "id": 101,
    "slug": "ranjancse",
    "name": "Ranjan Dailata",
    "avatar": ""
  },
  "categories": [
    "Market Research"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 3304,
  "downloads": 330,
  "createdAt": "2025-05-04T22:44:02.145Z",
  "updatedAt": "2026-01-16T08:31:51.660Z",
  "publishedAt": "2025-05-04T22:44:02.145Z",
  "nodes": 16,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/3866",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Asynchronous bulk web scraping with Bright Data & webhook notifications",
    "workflowName": "Asynchronous bulk web scraping with Bright Data & webhook notifications",
    "description": "### Who this is for\nThe Async Structured Bulk Data Extract with Bright Data Web Scraper workflow is designed for data engineers, market researchers, competitive intelligence teams, and automation developers who need to programmatically collect and structure high-volume data from the web using Bright Data's dataset and snapshot capabilities.\n\nThis workflow is built for:\n\n1. **Data Engineers** - Building large-scale ETL pipelines from web sources\n\n2. **Market Researchers** - Collecting bulk data for analysis across competitors or products\n\n3. **Growth Hackers & Analysts** - Mining structured datasets for insights\n\n4. **Automation Developers** - Needing reliable snapshot-triggered scrapers\n\n5. **Product Managers** - Overseeing data-backed decision-making using live web information\n\n### What problem is this workflow solving?\nWeb scraping at scale often requires asynchronous operations, including waiting for data preparation and snapshots to complete. Manual handling of this process can lead to timeouts, errors, or inconsistencies in results. \n\nThis workflow automates the entire process of submitting a scraping request, waiting for the snapshot, retrieving the data, and notifying downstream systems all in a structured, repeatable fashion.\n\nIt solves:\n\n1. Asynchronous snapshot completion handling\n\n2. Reliable retrieval of large datasets using Bright Data\n\n3. Automated delivery of scraped results via webhook\n\n4. Disk persistence for traceability or historical analysis\n\n### What this workflow does\n1. **Set Bright Data Dataset ID & Request URL**: Takes in the Dataset ID and Bright Data API endpoint used to trigger the scrape job\n\n2. **HTTP Request**: Sends an authenticated request to the Bright Data API to start a scraping snapshot job\n\n3. **Wait Until Snapshot is Ready**: Implements a loop or wait mechanism that checks snapshot status (e.g., polling every 30 seconds) until completion i.e ready state\n\n4. **Download Snapshot**: Downloads the structured dataset snapshot once ready\n\n5. **Persist Response to Disk**: Saves the dataset to disk for archival, review, or local processing\n\n6. **Webhook Notification**: Sends the final result or a summary of it to an external webhook\n\n### Setup\n\n- Sign up at [Bright Data](https://brightdata.com/).\n- Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.\n- In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).\n![Header Authentication.png](fileId:1266)\nThe Value field should be set with the \n**Bearer XXXXXXXXXXXXXX**. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.\n- Update the **Set Dataset Id, Request URL** for setting the brand content URL.\n- Update the Webhook HTTP Request node with the Webhook endpoint of your choice.\n\n### How to customize this workflow to your needs\n\n1. **Polling Strategy** : Adjust polling interval (e.g., every 15–60 seconds) based on snapshot complexity\n2. **Input Flexibility** : Accept datasetId and request URL dynamically from a webhook trigger or input form\n3. **Webhook Output** : Send notifications to -\n\n\t- Internal APIs – for use in dashboards\n\n\t- Zapier/Make – for multi-step automation\n\n4. **Persistence**\n\t- Save output to:\n\n\t\t- Remote FTP or SFTP storage\n\t\t- Amazon S3, Google Cloud Storage etc.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When clicking ‘Test workflow’",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "If",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Set Snapshot Id",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Download Snapshot",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Wait",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Check on the errors",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Check Snapshot Status",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Initiate a Webhook Notification",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Aggregate JSON Response",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "Set Dataset Id, Request URL",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Create a binary data",
      "type": "n8n-nodes-base.function",
      "role": "function",
      "configDescription": "Version 1"
    },
    {
      "name": "Write the file to disk",
      "type": "n8n-nodes-base.readWriteFile",
      "role": "readWriteFile",
      "configDescription": "Version 1"
    },
    {
      "name": "HTTP Request to the specified URL",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    }
  ]
}