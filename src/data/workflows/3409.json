{
  "id": 3409,
  "slug": "3409",
  "title": "Batch process prompts with Anthropic Claude API",
  "description": "This workflow template provides a robust solution for efficiently sending multiple prompts to Anthropic's Claude models in a single batch request and retrieving the results. It leverages the Anthropic Batch API endpoint (`/v1/messages/batches`) for optimized processing and outputs each result as a separate item.\n\n**Core Functionality & Example Usage Included**\n\nThis template includes:\n1.  **The Core Batch Processing Workflow:** Designed to be called by another n8n workflow.\n2.  **An Example Usage Workflow:** A separate branch demonstrating how to prepare data and trigger the core workflow, including examples using simple strings and n8n's [Langchain Chat Memory nodes](https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorybufferwindow/).\n\n## Who is this for?\n\nThis template is designed for:\n\n* **Developers, data scientists, and researchers** who need to process large volumes of text prompts using Claude models via n8n.\n* **Content creators** looking to generate multiple pieces of content (e.g., summaries, Q&As, creative text) based on different inputs simultaneously.\n* **n8n users** who want to automate interactions with the Anthropic API beyond single requests, improve efficiency, and integrate batch processing into larger automation sequences.\n* Anyone needing to perform **bulk text generation or analysis** tasks with Claude programmatically.\n\n## What problem does this workflow solve?\n\nSending prompts to language models one by one can be slow and inefficient, especially when dealing with hundreds or thousands of requests. This workflow addresses that by:\n\n* **Batching:** Grouping multiple prompts into a single API call to Anthropic's dedicated batch endpoint (`/v1/messages/batches`).\n* **Efficiency:** Significantly reducing the time required compared to sequential processing.\n* **Scalability:** Handling large numbers of prompts (up to API limits) systematically.\n* **Automation:** Providing a ready-to-use, callable n8n structure for batch interactions with Claude.\n* **Structured Output:** Parsing the results and outputting each individual prompt's result as a separate n8n item.\n\n**Use Cases:**\n\n* Bulk content generation (e.g., product descriptions, summaries).\n* Large-scale question answering based on different contexts.\n* Sentiment analysis or data extraction across multiple text snippets.\n* Running the same prompt against many different inputs for research or testing.\n\n## What the Core Workflow does\n\n*(Triggered by the 'When Executed by Another Workflow' node)*\n\n1.  **Receive Input:** The workflow starts when called by another workflow (e.g., using the 'Execute Workflow' node). It expects input data containing:\n    * `anthropic-version` (string, e.g., \"2023-06-01\")\n    * `requests` (JSON array, where each object represents a single prompt request conforming to the Anthropic Batch API schema).\n2.  **Submit Batch Job:** Sends the formatted `requests` data via `POST` to the Anthropic API `/v1/messages/batches` endpoint to create a new batch job. Requires Anthropic credentials.\n3.  **Wait & Poll:** Enters a loop:\n    * Checks if the `processing_status` of the batch job is `ended`.\n    * If not `ended`, it waits for a set interval (**10 seconds** by default in the 'Batch Status Poll Interval' node).\n    * It then checks the batch job status again via `GET` to `/v1/messages/batches/{batch_id}`. Requires Anthropic credentials.\n    * This loop continues until the status is `ended`.\n4.  **Retrieve Results:** Once the batch job is complete, it fetches the results file by making a `GET` request to the `results_url` provided in the batch status response. Requires Anthropic credentials.\n5.  **Parse Results:** The results are typically returned in JSON Lines (`.jsonl`) format. The 'Parse response' Code node splits the response text by newlines and parses each line into a separate JSON object, storing them in an array field (e.g., `parsed`).\n6.  **Split Output:** The 'Split Out Parsed Results' node takes the array of parsed results and outputs each result object as an individual item from the workflow.\n\n## Prerequisites\n\n* An active **n8n instance** (Cloud or self-hosted).\n* An **Anthropic API account** with access granted to Claude models and the Batch API.\n* Your **Anthropic API Key**.\n* Basic understanding of n8n concepts (nodes, workflows, credentials, expressions, 'Execute Workflow' node).\n* Familiarity with JSON data structures for providing input prompts and understanding the output.\n* Understanding of the Anthropic Batch API request/response structure.\n* *(For Example Usage Branch)* Familiarity with n8n's Langchain nodes (`@n8n/n8n-nodes-langchain`) if you plan to adapt that part.\n\n## Setup\n\n1.  **Import Template:** Add this template to your n8n instance.\n2.  **Configure Credentials:**\n    * Navigate to the 'Credentials' section in your n8n instance.\n    * Click 'Add Credential'.\n    * Search for 'Anthropic' and select the Anthropic API credential type.\n    * Enter your Anthropic API Key and save the credential (e.g., name it \"Anthropic account\").\n3.  **Assign Credentials:** Open the workflow and locate the **three HTTP Request nodes** in the core workflow:\n    * `Submit batch`\n    * `Check batch status`\n    * `Get results`\n    In each of these nodes, select the Anthropic credential you just configured from the 'Credential for Anthropic API' dropdown.\n4.  **Review Input Format:** Understand the required input structure for the `When Executed by Another Workflow` trigger node. The primary inputs are `anthropic-version` (string) and `requests` (array). Refer to the **Sticky Notes** in the template and the [Anthropic Batch API documentation](https://docs.anthropic.com/en/api/batch) for the exact schema required within the `requests` array.\n5.  **Activate Workflow:** Save and activate the core workflow so it can be called by other workflows.\n\n**➡️ Quick Start & Input/Output Examples:** Look for the **Sticky Notes** within the workflow canvas! They provide crucial information, including examples of the required input JSON structure and the expected output format.\n\n## How to customize this workflow\n\n* **Input Source:** The core workflow is designed to be called. You will build *another* workflow that prepares the `anthropic-version` and `requests` array and then uses the 'Execute Workflow' node to trigger this template. The included example branch shows how to prepare this data.\n* **Model Selection & Parameters:** Model (`claude-3-opus-20240229`, etc.), `max_tokens`, `temperature`, and other parameters are defined *within each object* inside the `requests` array you pass to the workflow trigger. You configure these in the workflow *calling* this template.\n* **Polling Interval:** Modify the 'Wait' node ('Batch Status Poll Interval') duration if you need faster or slower status checks (default is 10 seconds). Be mindful of potential rate limits.\n* **Parsing Logic:** If Anthropic changes the result format or you have specific needs, modify the Javascript code within the 'Parse response' Code node.\n* **Error Handling:** Enhance the workflow with more specific error handling for API failures (e.g., using 'Error Trigger' or checking HTTP status codes) or batch processing issues (`batch.status === 'failed'`).\n* **Output Processing:** In the workflow that *calls* this template, add nodes after the 'Execute Workflow' node to process the individual result items returned (e.g., save to a database, spreadsheet, send notifications).\n\n## Example Usage Branch (Manual Trigger)\n\nThis template also contains a separate branch starting with the `Run example` Manual Trigger node.\n\n* **Purpose:** This branch demonstrates how to construct the necessary `anthropic-version` and `requests` array payload.\n* **Methods Shown:** It includes steps for:\n    * Creating a request object from a simple query string.\n    * Creating a request object using data from n8n's Langchain Chat Memory nodes (`@n8n/n8n-nodes-langchain`).\n* **Execution:** It merges these examples, constructs the final payload, and then uses the `Execute Workflow` node to call the main batch processing logic described above. It finishes by filtering the results for demonstration.\n* **Note:** This branch is for demonstration and testing. You would typically build your own data preparation logic in a separate workflow. The use of Langchain nodes is optional for the core batch functionality.\n\n## Notes\n\n* **API Limits:** According to the [Anthropic API documentation](https://docs.anthropic.com/en/api/creating-message-batches), batches can contain up to 100,000 requests and be up to 256 MB in total size. Ensure your n8n instance has sufficient resources for large batches.\n* **API Costs:** Using the Anthropic API, including the Batch API, incurs costs based on token usage. Monitor your usage via the Anthropic dashboard.\n* **Completion Time:** Batch processing time depends on the number and complexity of prompts and current API load. The polling mechanism accounts for this variability.\n* **Versioning:** Always include the `anthropic-version` header in your requests, as shown in the workflow and examples. Refer to [Anthropic API versioning documentation](https://docs.anthropic.com/en/api/versioning).\n",
  "featuredImage": "/data/workflows/3409/3409.webp",
  "author": {
    "id": 101,
    "slug": "greg",
    "name": "Greg Evseev",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "Multimodal AI"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 1787,
  "downloads": 178,
  "createdAt": "2025-04-02T19:20:04.824Z",
  "updatedAt": "2026-01-16T08:29:36.769Z",
  "publishedAt": "2025-04-02T19:20:04.824Z",
  "nodes": 39,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/3409",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Batch process prompts with Anthropic Claude API",
    "workflowName": "Batch process prompts with Anthropic Claude API",
    "description": "This workflow template provides a robust solution for efficiently sending multiple prompts to Anthropic's Claude models in a single batch request and retrieving the results. It leverages the Anthropic Batch API endpoint (`/v1/messages/batches`) for optimized processing and outputs each result as a separate item.\n\n**Core Functionality & Example Usage Included**\n\nThis template includes:\n1.  **The Core Batch Processing Workflow:** Designed to be called by another n8n workflow.\n2.  **An Example Usage Workflow:** A separate branch demonstrating how to prepare data and trigger the core workflow, including examples using simple strings and n8n's [Langchain Chat Memory nodes](https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorybufferwindow/).\n\n## Who is this for?\n\nThis template is designed for:\n\n* **Developers, data scientists, and researchers** who need to process large volumes of text prompts using Claude models via n8n.\n* **Content creators** looking to generate multiple pieces of content (e.g., summaries, Q&As, creative text) based on different inputs simultaneously.\n* **n8n users** who want to automate interactions with the Anthropic API beyond single requests, improve efficiency, and integrate batch processing into larger automation sequences.\n* Anyone needing to perform **bulk text generation or analysis** tasks with Claude programmatically.\n\n## What problem does this workflow solve?\n\nSending prompts to language models one by one can be slow and inefficient, especially when dealing with hundreds or thousands of requests. This workflow addresses that by:\n\n* **Batching:** Grouping multiple prompts into a single API call to Anthropic's dedicated batch endpoint (`/v1/messages/batches`).\n* **Efficiency:** Significantly reducing the time required compared to sequential processing.\n* **Scalability:** Handling large numbers of prompts (up to API limits) systematically.\n* **Automation:** Providing a ready-to-use, callable n8n structure for batch interactions with Claude.\n* **Structured Output:** Parsing the results and outputting each individual prompt's result as a separate n8n item.\n\n**Use Cases:**\n\n* Bulk content generation (e.g., product descriptions, summaries).\n* Large-scale question answering based on different contexts.\n* Sentiment analysis or data extraction across multiple text snippets.\n* Running the same prompt against many different inputs for research or testing.\n\n## What the Core Workflow does\n\n*(Triggered by the 'When Executed by Another Workflow' node)*\n\n1.  **Receive Input:** The workflow starts when called by another workflow (e.g., using the 'Execute Workflow' node). It expects input data containing:\n    * `anthropic-version` (string, e.g., \"2023-06-01\")\n    * `requests` (JSON array, where each object represents a single prompt request conforming to the Anthropic Batch API schema).\n2.  **Submit Batch Job:** Sends the formatted `requests` data via `POST` to the Anthropic API `/v1/messages/batches` endpoint to create a new batch job. Requires Anthropic credentials.\n3.  **Wait & Poll:** Enters a loop:\n    * Checks if the `processing_status` of the batch job is `ended`.\n    * If not `ended`, it waits for a set interval (**10 seconds** by default in the 'Batch Status Poll Interval' node).\n    * It then checks the batch job status again via `GET` to `/v1/messages/batches/{batch_id}`. Requires Anthropic credentials.\n    * This loop continues until the status is `ended`.\n4.  **Retrieve Results:** Once the batch job is complete, it fetches the results file by making a `GET` request to the `results_url` provided in the batch status response. Requires Anthropic credentials.\n5.  **Parse Results:** The results are typically returned in JSON Lines (`.jsonl`) format. The 'Parse response' Code node splits the response text by newlines and parses each line into a separate JSON object, storing them in an array field (e.g., `parsed`).\n6.  **Split Output:** The 'Split Out Parsed Results' node takes the array of parsed results and outputs each result object as an individual item from the workflow.\n\n## Prerequisites\n\n* An active **n8n instance** (Cloud or self-hosted).\n* An **Anthropic API account** with access granted to Claude models and the Batch API.\n* Your **Anthropic API Key**.\n* Basic understanding of n8n concepts (nodes, workflows, credentials, expressions, 'Execute Workflow' node).\n* Familiarity with JSON data structures for providing input prompts and understanding the output.\n* Understanding of the Anthropic Batch API request/response structure.\n* *(For Example Usage Branch)* Familiarity with n8n's Langchain nodes (`@n8n/n8n-nodes-langchain`) if you plan to adapt that part.\n\n## Setup\n\n1.  **Import Template:** Add this template to your n8n instance.\n2.  **Configure Credentials:**\n    * Navigate to the 'Credentials' section in your n8n instance.\n    * Click 'Add Credential'.\n    * Search for 'Anthropic' and select the Anthropic API credential type.\n    * Enter your Anthropic API Key and save the credential (e.g., name it \"Anthropic account\").\n3.  **Assign Credentials:** Open the workflow and locate the **three HTTP Request nodes** in the core workflow:\n    * `Submit batch`\n    * `Check batch status`\n    * `Get results`\n    In each of these nodes, select the Anthropic credential you just configured from the 'Credential for Anthropic API' dropdown.\n4.  **Review Input Format:** Understand the required input structure for the `When Executed by Another Workflow` trigger node. The primary inputs are `anthropic-version` (string) and `requests` (array). Refer to the **Sticky Notes** in the template and the [Anthropic Batch API documentation](https://docs.anthropic.com/en/api/batch) for the exact schema required within the `requests` array.\n5.  **Activate Workflow:** Save and activate the core workflow so it can be called by other workflows.\n\n**➡️ Quick Start & Input/Output Examples:** Look for the **Sticky Notes** within the workflow canvas! They provide crucial information, including examples of the required input JSON structure and the expected output format.\n\n## How to customize this workflow\n\n* **Input Source:** The core workflow is designed to be called. You will build *another* workflow that prepares the `anthropic-version` and `requests` array and then uses the 'Execute Workflow' node to trigger this template. The included example branch shows how to prepare this data.\n* **Model Selection & Parameters:** Model (`claude-3-opus-20240229`, etc.), `max_tokens`, `temperature`, and other parameters are defined *within each object* inside the `requests` array you pass to the workflow trigger. You configure these in the workflow *calling* this template.\n* **Polling Interval:** Modify the 'Wait' node ('Batch Status Poll Interval') duration if you need faster or slower status checks (default is 10 seconds). Be mindful of potential rate limits.\n* **Parsing Logic:** If Anthropic changes the result format or you have specific needs, modify the Javascript code within the 'Parse response' Code node.\n* **Error Handling:** Enhance the workflow with more specific error handling for API failures (e.g., using 'Error Trigger' or checking HTTP status codes) or batch processing issues (`batch.status === 'failed'`).\n* **Output Processing:** In the workflow that *calls* this template, add nodes after the 'Execute Workflow' node to process the individual result items returned (e.g., save to a database, spreadsheet, send notifications).\n\n## Example Usage Branch (Manual Trigger)\n\nThis template also contains a separate branch starting with the `Run example` Manual Trigger node.\n\n* **Purpose:** This branch demonstrates how to construct the necessary `anthropic-version` and `requests` array payload.\n* **Methods Shown:** It includes steps for:\n    * Creating a request object from a simple query string.\n    * Creating a request object using data from n8n's Langchain Chat Memory nodes (`@n8n/n8n-nodes-langchain`).\n* **Execution:** It merges these examples, constructs the final payload, and then uses the `Execute Workflow` node to call the main batch processing logic described above. It finishes by filtering the results for demonstration.\n* **Note:** This branch is for demonstration and testing. You would typically build your own data preparation logic in a separate workflow. The use of Langchain nodes is optional for the core batch functionality.\n\n## Notes\n\n* **API Limits:** According to the [Anthropic API documentation](https://docs.anthropic.com/en/api/creating-message-batches), batches can contain up to 100,000 requests and be up to 256 MB in total size. Ensure your n8n instance has sufficient resources for large batches.\n* **API Costs:** Using the Anthropic API, including the Batch API, incurs costs based on token usage. Monitor your usage via the Anthropic dashboard.\n* **Completion Time:** Batch processing time depends on the number and complexity of prompts and current API load. The polling mechanism accounts for this variability.\n* **Versioning:** Always include the `anthropic-version` header in your requests, as shown in the workflow and examples. Refer to [Anthropic API versioning documentation](https://docs.anthropic.com/en/api/versioning).",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Submit batch",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Check batch status",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "When Executed by Another Workflow",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "role": "executeWorkflowTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Get results",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Parse response",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "If ended processing",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Batch Status Poll Interval",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Run example",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "One query example",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Delete original properties",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Construct 'requests' array",
      "type": "n8n-nodes-base.aggregate",
      "role": "aggregate",
      "configDescription": "Version 1"
    },
    {
      "name": "Set desired 'anthropic-version'",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Execute Workflow 'Process Multiple Prompts in Parallel with Anthropic Claude Batch API'",
      "type": "n8n-nodes-base.executeWorkflow",
      "role": "executeWorkflow",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Build batch 'request' object for single query",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Simple Memory Store",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Fill Chat Memory with example data",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "role": "memoryManager",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Build batch 'request' object from Chat Memory and execution data",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Load Chat Memory Data",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "role": "memoryManager",
      "configDescription": "Version 1.1"
    },
    {
      "name": "First Prompt Result",
      "type": "n8n-nodes-base.executionData",
      "role": "executionData",
      "configDescription": "Version 1"
    },
    {
      "name": "Second Prompt Result",
      "type": "n8n-nodes-base.executionData",
      "role": "executionData",
      "configDescription": "Version 1"
    },
    {
      "name": "Split Out Parsed Results",
      "type": "n8n-nodes-base.splitOut",
      "role": "splitOut",
      "configDescription": "Version 1"
    },
    {
      "name": "Filter Second Prompt Results",
      "type": "n8n-nodes-base.filter",
      "role": "filter",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Filter First Prompt Results",
      "type": "n8n-nodes-base.filter",
      "role": "filter",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note7",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note9",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note10",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note11",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Join two example requests into array",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.1"
    },
    {
      "name": "Append execution data for single query example",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Append execution data for chat memory example",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Truncate Chat Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "role": "memoryManager",
      "configDescription": "Version 1.1"
    }
  ]
}