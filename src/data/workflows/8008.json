{
  "id": 8008,
  "slug": "8008",
  "title": "Smarter RAG agents with enriched retrieval and modular workflows",
  "description": "An extendable RAG template to build powerful, explainable AI assistants ‚Äî with query understanding, semantic metadata, and support for free-tier tools like Gemini, Gemma and Supabase.\n\n## Description\nThis workflow helps you build smart, production-ready RAG agents that go far beyond basic document Q&A.\n\nIt includes:\n\n‚úÖ File ingestion and chunking\n\n‚úÖ Asynchronous LLM-powered enrichment\n\n‚úÖ Filterable metadata-based search\n\n‚úÖ Gemma-based query understanding and generation\n\n‚úÖ Cohere re-ranking\n\n‚úÖ Memory persistence via Postgres\n\n\n\nEverything is modular, low-cost, and designed to run even with **free-tier LLMs and vector databases**.\n\nWhether you want to build a chatbot, internal knowledge assistant, documentation search engine, or a filtered content explorer ‚Äî this is your foundation.\n\n\n## ‚öôÔ∏è How It Works\nThis workflow is divided into 3 pipelines:\n\n### üì• Ingestion\n- Upload a PDF via form\n- Extract text and chunk it for embedding\n- Store in Supabase vector store using Google Gemini embeddings\n\n\n### üß† Enrichment (Async)\n- Scheduled task fetches new chunks\n- Each chunk is enriched with LLM metadata (topics, use_case, risks, audience level, summary, etc.)\n- Metadata is added to the vector DB for improved retrieval and filtering\n\n\n### ü§ñ Agent Chat\n- A user question triggers the RAG agent\n- Query Builder transforms it into keywords and filters\n- Vector DB is queried and reranked\n- The final answer is generated using only retrieved evidence, with references\n- Chat memory is managed via Postgres\n\n## üåü Key Features\n- **Asynchronous enrichment** ‚Üí Save tokens, batch process with free-tier LLMs like Gemma\n- **Metadata-aware** ‚Üí Improved filtering and reranking\n- **Explainable answers** ‚Üí Agent cites sources and sections\n- **Chat memory** ‚Üí Persistent context with Postgres\n- **Modular design** ‚Üí Swap LLMs, rerankers, vector DBs, and even enrichment schema\n- **Free to run** ‚Üí Built with Gemini, Gemma, Cohere, Supabase (free tier-compatible)\n\n\n\n## üîê Required Credentials\n\n|Tool|Use|\n|-|-|-|\n|Supabase w/ PostreSQL|Vector DB + storage|\n|Google Gemini/Gemma|Embeddings & LLM|\n|Cohere API|Re-ranking|\n|PostgreSQL|Chat memory|\n\n\n\n## üß∞ Customization Tips\n- Swap extractFromFile with Notion/Google Drive integrations\n\n- Extend Metadata Obtention prompt to fit your domain (e.g., financial, legal)\n\n- Replace LLMs with OpenAI, Mistral, or Ollama \n\n- Replace Postgre Chat Memory with Simple Memory or any other\n\n- Use a webhook instead of a form to automate ingestion\n\n- Connect to Telegram/Slack UI with a few extra nodes\n\n\n## üí° Use Cases\n- Company knowledge base bot (internal docs, SOPs)\n\n- Educational assistant with smart filtering (by topic or level)\n- Legal or policy assistant that cites source sections\n- Product documentation Q&A with multi-language support\n- Training material assistant that highlights risks/examples\n- Content Generation\n\n\n## üß† Who It‚Äôs For\n- Indie developers building smart chatbots\n- AI consultants prototyping Q&A assistants\n- Teams looking for an internal knowledge agent\n- Anyone building affordable, explainable AI tools\n\n## üöÄ Try It Out!\n\nDeploy a modular RAG assistant using n8n, Supabase, and Gemini ‚Äî fully customizable and almost free to run.\n\n#### 1. üìÅ Prepare Your PDFs\n\n- Use any internal documents, manuals, or reports in **PDF **format.\n\n- Optional: Add Google Drive integration to automate ingestion.\n\n#### 2. üß© Set Up Supabase\n\n- Create a free Supabase project\n\n- Use the table creation queries included in the workflow to set up your schema.\n\n- Add your *supabaseUrl *and *supabaseKey *in your n8n credentials.\n\n&gt; üí° Pro Tip:\nMake sure you match the embedding dimensions to your model.\nThis workflow uses *Gemini text-embedding-04* (768-dim) ‚Äî if switching to OpenAI, change your table vector size to 1536.\n\n#### 3. üß† Connect Gemini & Gemma\n\n- Use Gemini/Gemma for embeddings and optional metadata enrichment.\n\n- Or deploy locally for lightweight async LLM processing (via Ollama/HuggingFace).\n\n#### 4. ‚öôÔ∏è Import the Workflow in n8n\n\n- Open n8n (self-hosted or cloud).\n\n- Import the workflow file and paste your credentials.\n\n\nYou‚Äôre ready to ingest, enrich, and query your document base.\n\n\n## üí¨ Have Feedback or Ideas? I‚Äôd Love to Hear\n\nThis project is open, modular, and evolving ‚Äî just like great workflows should be :).\n\nIf you‚Äôve tried it, built on top of it, or have suggestions for improvement, I‚Äôd genuinely love to hear from you. Let‚Äôs share ideas, collaborate, or just connect as part of the n8n builder community.\n\nüìß [ascuncia.es@gmail.com](mailto:ascuncia.es+n8ncreator@gmail.com)\n\nüîó [Linkedin](https://www.linkedin.com/in/alejandro-scuncia-60a62348/)",
  "featuredImage": "/data/workflows/8008/8008.webp",
  "author": {
    "id": 101,
    "slug": "ascuncia",
    "name": "Alejandro Scuncia",
    "avatar": ""
  },
  "categories": [
    "Internal Wiki",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 2006,
  "downloads": 200,
  "createdAt": "2025-08-28T22:18:25.522Z",
  "updatedAt": "2026-01-16T08:53:32.431Z",
  "publishedAt": "2025-08-28T22:18:25.522Z",
  "nodes": 32,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/8008",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Smarter RAG agents with enriched retrieval and modular workflows",
    "workflowName": "Smarter RAG agents with enriched retrieval and modular workflows",
    "description": "An extendable RAG template to build powerful, explainable AI assistants ‚Äî with query understanding, semantic metadata, and support for free-tier tools like Gemini, Gemma and Supabase.\n\n## Description\nThis workflow helps you build smart, production-ready RAG agents that go far beyond basic document Q&A.\n\nIt includes:\n\n‚úÖ File ingestion and chunking\n\n‚úÖ Asynchronous LLM-powered enrichment\n\n‚úÖ Filterable metadata-based search\n\n‚úÖ Gemma-based query understanding and generation\n\n‚úÖ Cohere re-ranking\n\n‚úÖ Memory persistence via Postgres\n\n\n\nEverything is modular, low-cost, and designed to run even with **free-tier LLMs and vector databases**.\n\nWhether you want to build a chatbot, internal knowledge assistant, documentation search engine, or a filtered content explorer ‚Äî this is your foundation.\n\n\n## ‚öôÔ∏è How It Works\nThis workflow is divided into 3 pipelines:\n\n### üì• Ingestion\n- Upload a PDF via form\n- Extract text and chunk it for embedding\n- Store in Supabase vector store using Google Gemini embeddings\n\n\n### üß† Enrichment (Async)\n- Scheduled task fetches new chunks\n- Each chunk is enriched with LLM metadata (topics, use_case, risks, audience level, summary, etc.)\n- Metadata is added to the vector DB for improved retrieval and filtering\n\n\n### ü§ñ Agent Chat\n- A user question triggers the RAG agent\n- Query Builder transforms it into keywords and filters\n- Vector DB is queried and reranked\n- The final answer is generated using only retrieved evidence, with references\n- Chat memory is managed via Postgres\n\n## üåü Key Features\n- **Asynchronous enrichment** ‚Üí Save tokens, batch process with free-tier LLMs like Gemma\n- **Metadata-aware** ‚Üí Improved filtering and reranking\n- **Explainable answers** ‚Üí Agent cites sources and sections\n- **Chat memory** ‚Üí Persistent context with Postgres\n- **Modular design** ‚Üí Swap LLMs, rerankers, vector DBs, and even enrichment schema\n- **Free to run** ‚Üí Built with Gemini, Gemma, Cohere, Supabase (free tier-compatible)\n\n\n\n## üîê Required Credentials\n\n|Tool|Use|\n|-|-|-|\n|Supabase w/ PostreSQL|Vector DB + storage|\n|Google Gemini/Gemma|Embeddings & LLM|\n|Cohere API|Re-ranking|\n|PostgreSQL|Chat memory|\n\n\n\n## üß∞ Customization Tips\n- Swap extractFromFile with Notion/Google Drive integrations\n\n- Extend Metadata Obtention prompt to fit your domain (e.g., financial, legal)\n\n- Replace LLMs with OpenAI, Mistral, or Ollama \n\n- Replace Postgre Chat Memory with Simple Memory or any other\n\n- Use a webhook instead of a form to automate ingestion\n\n- Connect to Telegram/Slack UI with a few extra nodes\n\n\n## üí° Use Cases\n- Company knowledge base bot (internal docs, SOPs)\n\n- Educational assistant with smart filtering (by topic or level)\n- Legal or policy assistant that cites source sections\n- Product documentation Q&A with multi-language support\n- Training material assistant that highlights risks/examples\n- Content Generation\n\n\n## üß† Who It‚Äôs For\n- Indie developers building smart chatbots\n- AI consultants prototyping Q&A assistants\n- Teams looking for an internal knowledge agent\n- Anyone building affordable, explainable AI tools\n\n## üöÄ Try It Out!\n\nDeploy a modular RAG assistant using n8n, Supabase, and Gemini ‚Äî fully customizable and almost free to run.\n\n#### 1. üìÅ Prepare Your PDFs\n\n- Use any internal documents, manuals, or reports in **PDF **format.\n\n- Optional: Add Google Drive integration to automate ingestion.\n\n#### 2. üß© Set Up Supabase\n\n- Create a free Supabase project\n\n- Use the table creation queries included in the workflow to set up your schema.\n\n- Add your *supabaseUrl *and *supabaseKey *in your n8n credentials.\n\n&gt; üí° Pro Tip:\nMake sure you match the embedding dimensions to your model.\nThis workflow uses *Gemini text-embedding-04* (768-dim) ‚Äî if switching to OpenAI, change your table vector size to 1536.\n\n#### 3. üß† Connect Gemini & Gemma\n\n- Use Gemini/Gemma for embeddings and optional metadata enrichment.\n\n- Or deploy locally for lightweight async LLM processing (via Ollama/HuggingFace).\n\n#### 4. ‚öôÔ∏è Import the Workflow in n8n\n\n- Open n8n (self-hosted or cloud).\n\n- Import the workflow file and paste your credentials.\n\n\nYou‚Äôre ready to ingest, enrich, and query your document base.\n\n\n## üí¨ Have Feedback or Ideas? I‚Äôd Love to Hear\n\nThis project is open, modular, and evolving ‚Äî just like great workflows should be :).\n\nIf you‚Äôve tried it, built on top of it, or have suggestions for improvement, I‚Äôd genuinely love to hear from you. Let‚Äôs share ideas, collaborate, or just connect as part of the n8n builder community.\n\nüìß [ascuncia.es@gmail.com](mailto:ascuncia.es+n8ncreator@gmail.com)\n\nüîó [Linkedin](https://www.linkedin.com/in/alejandro-scuncia-60a62348/)",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Recursive Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
      "role": "textSplitterRecursiveCharacterTextSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "Delete Old Doc Rows",
      "type": "n8n-nodes-base.supabase",
      "role": "supabase",
      "configDescription": "Version 1"
    },
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Insert into Supabase Vectorstore",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreSupabase",
      "role": "vectorStoreSupabase",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings Google Gemini1",
      "type": "@n8n/n8n-nodes-langchain.embeddingsGoogleGemini",
      "role": "embeddingsGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "On form submission",
      "type": "n8n-nodes-base.formTrigger",
      "role": "formTrigger",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Extract from File",
      "type": "n8n-nodes-base.extractFromFile",
      "role": "extractFromFile",
      "configDescription": "Version 1"
    },
    {
      "name": "Loop Over Items",
      "type": "n8n-nodes-base.splitInBatches",
      "role": "splitInBatches",
      "configDescription": "Version 3"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Get many rows",
      "type": "n8n-nodes-base.supabase",
      "role": "supabase",
      "configDescription": "Version 1"
    },
    {
      "name": "Update a row",
      "type": "n8n-nodes-base.supabase",
      "role": "supabase",
      "configDescription": "Version 1"
    },
    {
      "name": "Merge",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 3.2"
    },
    {
      "name": "Wait",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Set File Data",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Metadata Obtention",
      "type": "@n8n/n8n-nodes-langchain.googleGemini",
      "role": "googleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Schedule Trigger",
      "type": "n8n-nodes-base.scheduleTrigger",
      "role": "scheduleTrigger",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Edit Fields",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Query Builder",
      "type": "@n8n/n8n-nodes-langchain.googleGemini",
      "role": "googleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "RAG Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 2"
    },
    {
      "name": "Reranker",
      "type": "@n8n/n8n-nodes-langchain.rerankerCohere",
      "role": "rerankerCohere",
      "configDescription": "Version 1"
    },
    {
      "name": "Google Gemini 2.0 Flash",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "role": "lmChatGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Postgres Chat Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryPostgresChat",
      "role": "memoryPostgresChat",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Embeddings Google Gemini",
      "type": "@n8n/n8n-nodes-langchain.embeddingsGoogleGemini",
      "role": "embeddingsGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Supabase Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreSupabase",
      "role": "vectorStoreSupabase",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note26",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note9",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}