{
  "id": 2344,
  "slug": "2344",
  "title": "Generating image embeddings via textual summarisation",
  "description": "This n8n template demonstrates an approach to image embeddings for purpose of building a quick image contextual search. Use-cases could for a personal photo library, product recommendations or searching through video footage.\n\n## How it works\n* A photo is imported into the workflow via Google Drive.\n* The photo is processed by the edit image node to extract colour information. This information forms part of our semantic metadata used to identify the image.\n* The photo is also processed by a vision-capable model which analyses the image and returns a short description with semantic keywords.\n* Both pieces of information about the image are combined with the metadata of the image to form a document describing the image.\n* This document is then inserted into our vector store as a text embedding which is associated with our image.\n* From here, the user can query the vector store as they would any document and the relevant image references and/or links should be returned.\n\n## Requirements\n* Google account to download image files from Google Drive.\n* OpenAI account for the Vision-capable AI and Embedding models.\n\n## Customise this workflow\n\nText summarisation is just one of many techniques to generate image embeddings. If the results are unsatisfactory, there are dedicated image embedding models such as [Google's vertex AI multimodal embeddings](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings).",
  "featuredImage": "/data/workflows/2344/2344.webp",
  "author": {
    "id": 101,
    "slug": "jimleuk",
    "name": "Jimleuk",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 9227,
  "downloads": 922,
  "createdAt": "2024-07-11T17:05:23.103Z",
  "updatedAt": "2026-01-16T08:24:10.163Z",
  "publishedAt": "2024-07-11T17:05:23.103Z",
  "nodes": 22,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/2344",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Generating image embeddings via textual summarisation",
    "workflowName": "Generating image embeddings via textual summarisation",
    "description": "This n8n template demonstrates an approach to image embeddings for purpose of building a quick image contextual search. Use-cases could for a personal photo library, product recommendations or searching through video footage.\n\n## How it works\n* A photo is imported into the workflow via Google Drive.\n* The photo is processed by the edit image node to extract colour information. This information forms part of our semantic metadata used to identify the image.\n* The photo is also processed by a vision-capable model which analyses the image and returns a short description with semantic keywords.\n* Both pieces of information about the image are combined with the metadata of the image to form a document describing the image.\n* This document is then inserted into our vector store as a text embedding which is associated with our image.\n* From here, the user can query the vector store as they would any document and the relevant image references and/or links should be returned.\n\n## Requirements\n* Google account to download image files from Google Drive.\n* OpenAI account for the Vision-capable AI and Embedding models.\n\n## Customise this workflow\n\nText summarisation is just one of many techniques to generate image embeddings. If the results are unsatisfactory, there are dedicated image embedding models such as [Google's vertex AI multimodal embeddings](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings).",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When clicking \"Test workflow\"",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Google Drive",
      "type": "n8n-nodes-base.googleDrive",
      "role": "googleDrive",
      "configDescription": "Version 3"
    },
    {
      "name": "Get Color Information",
      "type": "n8n-nodes-base.editImage",
      "role": "editImage",
      "configDescription": "Version 1"
    },
    {
      "name": "Resize Image",
      "type": "n8n-nodes-base.editImage",
      "role": "editImage",
      "configDescription": "Version 1"
    },
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1"
    },
    {
      "name": "Recursive Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
      "role": "textSplitterRecursiveCharacterTextSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Combine Image Analysis",
      "type": "n8n-nodes-base.merge",
      "role": "merge",
      "configDescription": "Version 2.1"
    },
    {
      "name": "Document for Embedding",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.3"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings OpenAI1",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note7",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "In-Memory Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreInMemory",
      "role": "vectorStoreInMemory",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings OpenAI",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1"
    },
    {
      "name": "Search for Image",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreInMemory",
      "role": "vectorStoreInMemory",
      "configDescription": "Version 1"
    },
    {
      "name": "Get Image Keywords",
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "role": "openAi",
      "configDescription": "Version 1.8"
    }
  ]
}