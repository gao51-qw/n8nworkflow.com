{
  "id": 2384,
  "slug": "2384",
  "title": "Chat with local LLMs using n8n and Ollama",
  "description": "## Chat with local LLMs using n8n and Ollama\nThis n8n workflow allows you to seamlessly interact with your self-hosted Large Language Models (LLMs) through a user-friendly chat interface. By connecting to Ollama, a powerful tool for managing local LLMs, you can send prompts and receive AI-generated responses directly within n8n.\n\n### Use cases\n* Private AI Interactions\nIdeal for scenarios where data privacy and confidentiality are important.\n* Cost-Effective LLM Usage\nAvoid ongoing cloud API costs by running models on your own hardware.\n* Experimentation & Learning\nA great way to explore and experiment with different LLMs in a local, controlled environment.\n* Prototyping & Development\nBuild and test AI-powered applications without relying on external services.\n\n### How it works\n1. When chat message received: Captures the user's input from the chat interface.\n2. Chat LLM Chain: Sends the input to the Ollama server and receives the AI-generated response.\n3. Delivers the LLM's response back to the chat interface.\n\n### Set up steps\n* Make sure Ollama is installed and running on your machine before executing this workflow.\n* Edit the Ollama address if different from the default.\n",
  "featuredImage": "/data/workflows/2384/2384.webp",
  "author": {
    "id": 101,
    "slug": "mihailtd",
    "name": "Mihai Farcas",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "AI Chatbot"
  ],
  "complexityLevel": "intermediate",
  "price": 0,
  "visitors": 169903,
  "downloads": 16990,
  "createdAt": "2024-08-19T06:04:24.673Z",
  "updatedAt": "2026-01-16T08:24:20.873Z",
  "publishedAt": "2024-08-19T06:04:24.673Z",
  "nodes": 5,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/2384",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Chat with local LLMs using n8n and Ollama",
    "workflowName": "Chat with local LLMs using n8n and Ollama",
    "description": "## Chat with local LLMs using n8n and Ollama\nThis n8n workflow allows you to seamlessly interact with your self-hosted Large Language Models (LLMs) through a user-friendly chat interface. By connecting to Ollama, a powerful tool for managing local LLMs, you can send prompts and receive AI-generated responses directly within n8n.\n\n### Use cases\n* Private AI Interactions\nIdeal for scenarios where data privacy and confidentiality are important.\n* Cost-Effective LLM Usage\nAvoid ongoing cloud API costs by running models on your own hardware.\n* Experimentation & Learning\nA great way to explore and experiment with different LLMs in a local, controlled environment.\n* Prototyping & Development\nBuild and test AI-powered applications without relying on external services.\n\n### How it works\n1. When chat message received: Captures the user's input from the chat interface.\n2. Chat LLM Chain: Sends the input to the Ollama server and receives the AI-generated response.\n3. Delivers the LLM's response back to the chat interface.\n\n### Set up steps\n* Make sure Ollama is installed and running on your machine before executing this workflow.\n* Edit the Ollama address if different from the default.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Ollama Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOllama",
      "role": "lmChatOllama",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Chat LLM Chain",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "role": "chainLlm",
      "configDescription": "Version 1.4"
    }
  ]
}