{
  "id": 6405,
  "slug": "6405",
  "title": "Scrape Twitter profiles with Bright Data API and export to Google Sheets",
  "description": "# üê¶ Twitter Profile Scraper via Bright Data API with Google Sheets Output\n\nA comprehensive n8n automation that scrapes Twitter profile data using Bright Data's Twitter dataset and stores comprehensive tweet analytics, user metrics, and engagement data directly into Google Sheets.\n\n## üìã Overview\n\nThis workflow provides an automated Twitter data collection solution that extracts profile information and tweet data from specified Twitter accounts within custom date ranges. Perfect for social media analytics, competitor research, brand monitoring, and content strategy analysis.\n\n## ‚ú® Key Features\n\nüîó **Form-Based Input**: Easy-to-use form for Twitter URL and date range selection  \nüê¶ **Twitter Integration**: Uses Bright Data's Twitter dataset for accurate data extraction  \nüìä **Comprehensive Data**: Captures tweets, engagement metrics, and profile information  \nüìà **Google Sheets Storage**: Automatically stores all data in organized spreadsheet format  \nüîÑ **Progress Monitoring**: Real-time status tracking with automatic retry mechanisms  \n‚ö° **Fast & Reliable**: Professional scraping with built-in error handling  \nüìÖ **Date Range Control**: Flexible time period selection for targeted data collection  \nüéØ **Customizable Fields**: Advanced data field selection and mapping\n\n## üéØ What This Workflow Does\n\n### Input\n- **Twitter Profile URL**: Target Twitter account for data scraping\n- **Date Range**: Start and end dates for tweet collection period\n- **Custom Fields**: Configurable data points to extract\n\n### Processing\n1. **Form Trigger**: Collects Twitter URL and date range from user input\n2. **API Request**: Sends scraping request to Bright Data with specified parameters\n3. **Progress Monitoring**: Continuously checks scraping job status until completion\n4. **Data Retrieval**: Downloads complete dataset when scraping is finished\n5. **Data Processing**: Formats and structures extracted information\n6. **Sheet Integration**: Automatically populates Google Sheets with organized data\n\n### Output Data Points\n\n| Field | Description | Example |\n|-------|-------------|---------|\n| user_posted | Username who posted the tweet | @elonmusk |\n| name | Display name of the user | Elon Musk |\n| description | Tweet content/text | \"Exciting updates coming soon...\" |\n| date_posted | When the tweet was posted | 2025-01-15T10:30:00Z |\n| likes | Number of likes on the tweet | 1,234 |\n| reposts | Number of retweets | 567 |\n| replies | Number of replies | 89 |\n| views | Total view count | 12,345 |\n| followers | User's follower count | 50M |\n| following | Users they follow | 123 |\n| is_verified | Verification status | true/false |\n| hashtags | Hashtags used in tweet | #AI #Technology |\n| photos | Image URLs in tweet | image1.jpg, image2.jpg |\n| videos | Video content URLs | video1.mp4 |\n| user_id | Unique user identifier | 12345678 |\n| timestamp | Data extraction timestamp | 2025-01-15T11:00:00Z |\n\n## üöÄ Setup Instructions\n\n### Prerequisites\n- n8n instance (self-hosted or cloud)\n- Bright Data account with Twitter dataset access\n- Google account with Sheets access\n- Valid Twitter profile URLs to scrape\n- 10-15 minutes for setup\n\n### Step 1: Import the Workflow\n1. Copy the JSON workflow code from the provided file\n2. In n8n: Workflows ‚Üí + Add workflow ‚Üí Import from JSON\n3. Paste JSON and click Import\n\n### Step 2: Configure Bright Data\n1. Set up Bright Data credentials:\n   - In n8n: Credentials ‚Üí + Add credential ‚Üí HTTP Header Auth\n   - Enter your Bright Data API credentials\n   - Test the connection\n2. Configure dataset:\n   - Ensure you have access to Twitter dataset (`gd_lwxkxvnf1cynvib9co`)\n   - Verify dataset permissions in Bright Data dashboard\n\n### Step 3: Configure Google Sheets Integration\n1. Create a Google Sheet:\n   - Go to Google Sheets\n   - Create a new spreadsheet named \"Twitter Data\" or similar\n   - Copy the Sheet ID from URL: `https://docs.google.com/spreadsheets/d/SHEET_ID_HERE/edit`\n2. Set up Google Sheets credentials:\n   - In n8n: Credentials ‚Üí + Add credential ‚Üí Google Sheets OAuth2 API\n   - Complete OAuth setup and test connection\n3. Prepare your data sheet with columns:\n   - Use the column headers from the data points table above\n   - The workflow will automatically populate these fields\n\n### Step 4: Update Workflow Settings\n1. Update Bright Data nodes:\n   - Open \"üöÄ Trigger Twitter Scraping\" node\n   - Replace `BRIGHT_DATA_API_KEY` with your actual API token\n   - Verify dataset ID is correct\n2. Update Google Sheets node:\n   - Open \"üìä Store Twitter Data in Google Sheet\" node\n   - Replace `YOUR_GOOGLE_SHEET_ID` with your Sheet ID\n   - Select your Google Sheets credential\n   - Choose the correct sheet/tab name\n\n### Step 5: Test & Activate\n1. Add test data:\n   - Use the form trigger to input a Twitter profile URL\n   - Set a small date range for testing (e.g., last 7 days)\n2. Test the workflow:\n   - Submit the form to trigger the workflow\n   - Monitor progress in n8n execution logs\n   - Verify data appears in Google Sheet\n   - Check all expected columns are populated\n\n## üìñ Usage Guide\n\n### Running the Workflow\n1. Access the workflow form trigger URL (available when workflow is active)\n2. Enter the Twitter profile URL you want to scrape\n3. Set the start and end dates for tweet collection\n4. Submit the form to initiate scraping\n5. Monitor progress - the workflow will automatically check status every minute\n6. Once complete, data will appear in your Google Sheet\n\n### Understanding the Data\nYour Google Sheet will show:\n- **Real-time tweet data** for the specified date range\n- **User engagement metrics** (likes, replies, retweets, views)\n- **Profile information** (followers, following, verification status)\n- **Content details** (hashtags, media URLs, quoted tweets)\n- **Timestamps** for each tweet and data extraction\n\n### Customizing Date Ranges\n- **Recent data**: Use last 7-30 days for current activity analysis\n- **Historical analysis**: Select specific months or quarters for trend analysis\n- **Event tracking**: Focus on specific date ranges around events or campaigns\n- **Comparative studies**: Use consistent time periods across different profiles\n\n## üîß Customization Options\n\n### Modifying Data Fields\nEdit the `custom_output_fields` array in the \"üöÄ Trigger Twitter Scraping\" node to add or remove data points:\n\n```json\n\"custom_output_fields\": [\n  \"id\", \"user_posted\", \"name\", \"description\", \n  \"date_posted\", \"likes\", \"reposts\", \"replies\",\n  \"views\", \"hashtags\", \"followers\", \"is_verified\"\n]\n```\n\n### Changing Google Sheet Structure\nModify the column mapping in the \"üìä Store Twitter Data in Google Sheet\" node to match your preferred sheet layout and add custom formulas or calculations.\n\n### Adding Multiple Recipients\nTo process multiple Twitter profiles:\n1. Modify the form to accept multiple URLs\n2. Add a loop node to process each URL separately\n3. Implement delays between requests to respect rate limits\n\n## üö® Troubleshooting\n\n### Common Issues & Solutions\n\n1. **\"Bright Data connection failed\"**\n   - **Cause**: Invalid API credentials or dataset access\n   - **Solution**: Verify credentials in Bright Data dashboard, check dataset permissions\n\n2. **\"No data extracted\"**\n   - **Cause**: Invalid Twitter URLs or private/protected accounts\n   - **Solution**: Verify URLs are valid public Twitter profiles, test with different accounts\n\n3. **\"Google Sheets permission denied\"**\n   - **Cause**: Incorrect credentials or sheet permissions\n   - **Solution**: Re-authenticate Google Sheets, check sheet sharing settings\n\n4. **\"Workflow timeout\"**\n   - **Cause**: Large date ranges or high-volume accounts\n   - **Solution**: Use smaller date ranges, implement pagination for high-volume accounts\n\n5. **\"Progress monitoring stuck\"**\n   - **Cause**: Scraping job failed or API issues\n   - **Solution**: Check Bright Data dashboard for job status, restart workflow if needed\n\n### Advanced Troubleshooting\n- Check execution logs in n8n for detailed error messages\n- Test individual nodes by running them separately\n- Verify data formats and ensure consistent field mapping\n- Monitor rate limits if scraping multiple profiles consecutively\n- Add error handling and implement retry logic for robust operation\n\n## üìä Use Cases & Examples\n\n### 1. Social Media Analytics\n**Goal**: Track engagement metrics and content performance\n- Monitor tweet engagement rates over time\n- Analyze hashtag effectiveness and reach\n- Track follower growth and audience interaction\n- Generate weekly/monthly performance reports\n\n### 2. Competitor Research\n**Goal**: Monitor competitor social media activity\n- Track competitor posting frequency and timing\n- Analyze competitor content themes and strategies\n- Monitor competitor engagement and audience response\n- Identify trending topics and hashtags in your industry\n\n### 3. Brand Monitoring\n**Goal**: Track brand mentions and sentiment analysis\n- Monitor specific Twitter accounts for brand mentions\n- Track hashtag campaigns and user-generated content\n- Analyze sentiment trends and audience feedback\n- Identify influencers and brand advocates\n\n### 4. Content Strategy Development\n**Goal**: Analyze successful content patterns\n- Identify high-performing tweet formats and topics\n- Track optimal posting times and frequencies\n- Analyze hashtag performance and reach\n- Study audience engagement patterns\n\n### 5. Market Research\n**Goal**: Collect social media data for market analysis\n- Gather consumer opinions and feedback\n- Track industry trends and discussions\n- Monitor product launches and market reactions\n- Support product development with social insights\n\n## ‚öô Advanced Configuration\n\n### Batch Processing Multiple Profiles\nTo monitor multiple Twitter accounts efficiently:\n1. Create a master sheet with profile URLs and date ranges\n2. Add a loop node to process each profile separately\n3. Implement delays between requests to respect rate limits\n4. Use separate sheets or tabs for different profiles\n\n### Adding Data Analysis\nEnhance the workflow with analytical capabilities:\n1. Create additional sheets for processed data and insights\n2. Add formulas to calculate engagement rates and trends\n3. Implement data visualization with charts and graphs\n4. Generate automated reports and summaries\n\n### Integration with Business Tools\nConnect the workflow to your existing systems:\n- **CRM Integration**: Update customer records with social media data\n- **Slack Notifications**: Send alerts when data collection is complete\n- **Database Storage**: Store data in PostgreSQL/MySQL for advanced analysis\n- **BI Tools**: Connect to Tableau/Power BI for comprehensive visualization\n\n## üìà Performance & Limits\n\n### Expected Performance\n- **Single profile**: 30 seconds to 5 minutes (depending on date range)\n- **Data accuracy**: 95%+ for public Twitter profiles\n- **Success rate**: 90%+ for accessible accounts\n- **Daily capacity**: 10-50 profiles (depends on rate limits and data volume)\n\n### Resource Usage\n- **Memory**: ~200MB per execution\n- **Storage**: Minimal (data stored in Google Sheets)\n- **API calls**: 1 Bright Data call + multiple Google Sheets calls per profile\n- **Bandwidth**: ~5-10MB per profile scraped\n- **Execution time**: 2-10 minutes for typical date ranges\n\n### Scaling Considerations\n- **Rate limiting**: Add delays for high-volume scraping\n- **Error handling**: Implement retry logic for failed requests\n- **Data validation**: Add checks for malformed or missing data\n- **Monitoring**: Track success/failure rates over time\n- **Cost optimization**: Monitor API usage to control costs\n\n## ü§ù Support & Community\n\n### Getting Help\n- **n8n Community Forum**: community.n8n.io\n- **Documentation**: docs.n8n.io\n- **Bright Data Support**: Contact through your dashboard\n- **GitHub Issues**: Report bugs and feature requests\n\n### Contributing\n- Share improvements with the community\n- Report issues and suggest enhancements\n- Create variations for specific use cases\n- Document best practices and lessons learned\n\n## üìã Quick Setup Checklist\n\n### Before You Start\n‚òê n8n instance running (self-hosted or cloud)  \n‚òê Bright Data account with Twitter dataset access  \n‚òê Google account with Sheets access  \n‚òê Valid Twitter profile URLs ready for scraping  \n‚òê 10-15 minutes available for setup\n\n### Setup Steps\n‚òê **Import Workflow** - Copy JSON and import to n8n  \n‚òê **Configure Bright Data** - Set up API credentials and test  \n‚òê **Create Google Sheet** - New sheet with proper column structure  \n‚òê **Set up Google Sheets credentials** - OAuth setup and test  \n‚òê **Update workflow settings** - Replace API keys and sheet IDs  \n‚òê **Test with sample data** - Add 1 Twitter URL and small date range  \n‚òê **Verify data flow** - Check data appears in Google Sheet correctly  \n‚òê **Activate workflow** - Enable form trigger for production use\n\n## Ready to Use! üéâ\n\n**Your workflow URL**: Access form trigger when workflow is active\n\nüéØ **Happy Twitter Scraping!** This workflow provides a solid foundation for automated Twitter data collection. Customize it to fit your specific social media analytics and research needs.\n\n\n\nFor any questions or support, please contact:  \n[info@incrementors.com](mailto:info@incrementors.com)  \nor fill out this form: [https://www.incrementors.com/contact-us/](https://www.incrementors.com/contact-us/)\n",
  "featuredImage": "/data/workflows/6405/6405.webp",
  "author": {
    "id": 101,
    "slug": "incrementors",
    "name": "Incrementors",
    "avatar": ""
  },
  "categories": [
    "Market Research"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 560,
  "downloads": 56,
  "createdAt": "2025-07-25T04:34:31.136Z",
  "updatedAt": "2026-01-16T08:44:50.521Z",
  "publishedAt": "2025-07-25T04:34:31.136Z",
  "nodes": 15,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/6405",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Scrape Twitter profiles with Bright Data API and export to Google Sheets",
    "workflowName": "Scrape Twitter profiles with Bright Data API and export to Google Sheets",
    "description": "# üê¶ Twitter Profile Scraper via Bright Data API with Google Sheets Output\n\nA comprehensive n8n automation that scrapes Twitter profile data using Bright Data's Twitter dataset and stores comprehensive tweet analytics, user metrics, and engagement data directly into Google Sheets.\n\n## üìã Overview\n\nThis workflow provides an automated Twitter data collection solution that extracts profile information and tweet data from specified Twitter accounts within custom date ranges. Perfect for social media analytics, competitor research, brand monitoring, and content strategy analysis.\n\n## ‚ú® Key Features\n\nüîó **Form-Based Input**: Easy-to-use form for Twitter URL and date range selection  \nüê¶ **Twitter Integration**: Uses Bright Data's Twitter dataset for accurate data extraction  \nüìä **Comprehensive Data**: Captures tweets, engagement metrics, and profile information  \nüìà **Google Sheets Storage**: Automatically stores all data in organized spreadsheet format  \nüîÑ **Progress Monitoring**: Real-time status tracking with automatic retry mechanisms  \n‚ö° **Fast & Reliable**: Professional scraping with built-in error handling  \nüìÖ **Date Range Control**: Flexible time period selection for targeted data collection  \nüéØ **Customizable Fields**: Advanced data field selection and mapping\n\n## üéØ What This Workflow Does\n\n### Input\n- **Twitter Profile URL**: Target Twitter account for data scraping\n- **Date Range**: Start and end dates for tweet collection period\n- **Custom Fields**: Configurable data points to extract\n\n### Processing\n1. **Form Trigger**: Collects Twitter URL and date range from user input\n2. **API Request**: Sends scraping request to Bright Data with specified parameters\n3. **Progress Monitoring**: Continuously checks scraping job status until completion\n4. **Data Retrieval**: Downloads complete dataset when scraping is finished\n5. **Data Processing**: Formats and structures extracted information\n6. **Sheet Integration**: Automatically populates Google Sheets with organized data\n\n### Output Data Points\n\n| Field | Description | Example |\n|-------|-------------|---------|\n| user_posted | Username who posted the tweet | @elonmusk |\n| name | Display name of the user | Elon Musk |\n| description | Tweet content/text | \"Exciting updates coming soon...\" |\n| date_posted | When the tweet was posted | 2025-01-15T10:30:00Z |\n| likes | Number of likes on the tweet | 1,234 |\n| reposts | Number of retweets | 567 |\n| replies | Number of replies | 89 |\n| views | Total view count | 12,345 |\n| followers | User's follower count | 50M |\n| following | Users they follow | 123 |\n| is_verified | Verification status | true/false |\n| hashtags | Hashtags used in tweet | #AI #Technology |\n| photos | Image URLs in tweet | image1.jpg, image2.jpg |\n| videos | Video content URLs | video1.mp4 |\n| user_id | Unique user identifier | 12345678 |\n| timestamp | Data extraction timestamp | 2025-01-15T11:00:00Z |\n\n## üöÄ Setup Instructions\n\n### Prerequisites\n- n8n instance (self-hosted or cloud)\n- Bright Data account with Twitter dataset access\n- Google account with Sheets access\n- Valid Twitter profile URLs to scrape\n- 10-15 minutes for setup\n\n### Step 1: Import the Workflow\n1. Copy the JSON workflow code from the provided file\n2. In n8n: Workflows ‚Üí + Add workflow ‚Üí Import from JSON\n3. Paste JSON and click Import\n\n### Step 2: Configure Bright Data\n1. Set up Bright Data credentials:\n   - In n8n: Credentials ‚Üí + Add credential ‚Üí HTTP Header Auth\n   - Enter your Bright Data API credentials\n   - Test the connection\n2. Configure dataset:\n   - Ensure you have access to Twitter dataset (`gd_lwxkxvnf1cynvib9co`)\n   - Verify dataset permissions in Bright Data dashboard\n\n### Step 3: Configure Google Sheets Integration\n1. Create a Google Sheet:\n   - Go to Google Sheets\n   - Create a new spreadsheet named \"Twitter Data\" or similar\n   - Copy the Sheet ID from URL: `https://docs.google.com/spreadsheets/d/SHEET_ID_HERE/edit`\n2. Set up Google Sheets credentials:\n   - In n8n: Credentials ‚Üí + Add credential ‚Üí Google Sheets OAuth2 API\n   - Complete OAuth setup and test connection\n3. Prepare your data sheet with columns:\n   - Use the column headers from the data points table above\n   - The workflow will automatically populate these fields\n\n### Step 4: Update Workflow Settings\n1. Update Bright Data nodes:\n   - Open \"üöÄ Trigger Twitter Scraping\" node\n   - Replace `BRIGHT_DATA_API_KEY` with your actual API token\n   - Verify dataset ID is correct\n2. Update Google Sheets node:\n   - Open \"üìä Store Twitter Data in Google Sheet\" node\n   - Replace `YOUR_GOOGLE_SHEET_ID` with your Sheet ID\n   - Select your Google Sheets credential\n   - Choose the correct sheet/tab name\n\n### Step 5: Test & Activate\n1. Add test data:\n   - Use the form trigger to input a Twitter profile URL\n   - Set a small date range for testing (e.g., last 7 days)\n2. Test the workflow:\n   - Submit the form to trigger the workflow\n   - Monitor progress in n8n execution logs\n   - Verify data appears in Google Sheet\n   - Check all expected columns are populated\n\n## üìñ Usage Guide\n\n### Running the Workflow\n1. Access the workflow form trigger URL (available when workflow is active)\n2. Enter the Twitter profile URL you want to scrape\n3. Set the start and end dates for tweet collection\n4. Submit the form to initiate scraping\n5. Monitor progress - the workflow will automatically check status every minute\n6. Once complete, data will appear in your Google Sheet\n\n### Understanding the Data\nYour Google Sheet will show:\n- **Real-time tweet data** for the specified date range\n- **User engagement metrics** (likes, replies, retweets, views)\n- **Profile information** (followers, following, verification status)\n- **Content details** (hashtags, media URLs, quoted tweets)\n- **Timestamps** for each tweet and data extraction\n\n### Customizing Date Ranges\n- **Recent data**: Use last 7-30 days for current activity analysis\n- **Historical analysis**: Select specific months or quarters for trend analysis\n- **Event tracking**: Focus on specific date ranges around events or campaigns\n- **Comparative studies**: Use consistent time periods across different profiles\n\n## üîß Customization Options\n\n### Modifying Data Fields\nEdit the `custom_output_fields` array in the \"üöÄ Trigger Twitter Scraping\" node to add or remove data points:\n\n```json\n\"custom_output_fields\": [\n  \"id\", \"user_posted\", \"name\", \"description\", \n  \"date_posted\", \"likes\", \"reposts\", \"replies\",\n  \"views\", \"hashtags\", \"followers\", \"is_verified\"\n]\n```\n\n### Changing Google Sheet Structure\nModify the column mapping in the \"üìä Store Twitter Data in Google Sheet\" node to match your preferred sheet layout and add custom formulas or calculations.\n\n### Adding Multiple Recipients\nTo process multiple Twitter profiles:\n1. Modify the form to accept multiple URLs\n2. Add a loop node to process each URL separately\n3. Implement delays between requests to respect rate limits\n\n## üö® Troubleshooting\n\n### Common Issues & Solutions\n\n1. **\"Bright Data connection failed\"**\n   - **Cause**: Invalid API credentials or dataset access\n   - **Solution**: Verify credentials in Bright Data dashboard, check dataset permissions\n\n2. **\"No data extracted\"**\n   - **Cause**: Invalid Twitter URLs or private/protected accounts\n   - **Solution**: Verify URLs are valid public Twitter profiles, test with different accounts\n\n3. **\"Google Sheets permission denied\"**\n   - **Cause**: Incorrect credentials or sheet permissions\n   - **Solution**: Re-authenticate Google Sheets, check sheet sharing settings\n\n4. **\"Workflow timeout\"**\n   - **Cause**: Large date ranges or high-volume accounts\n   - **Solution**: Use smaller date ranges, implement pagination for high-volume accounts\n\n5. **\"Progress monitoring stuck\"**\n   - **Cause**: Scraping job failed or API issues\n   - **Solution**: Check Bright Data dashboard for job status, restart workflow if needed\n\n### Advanced Troubleshooting\n- Check execution logs in n8n for detailed error messages\n- Test individual nodes by running them separately\n- Verify data formats and ensure consistent field mapping\n- Monitor rate limits if scraping multiple profiles consecutively\n- Add error handling and implement retry logic for robust operation\n\n## üìä Use Cases & Examples\n\n### 1. Social Media Analytics\n**Goal**: Track engagement metrics and content performance\n- Monitor tweet engagement rates over time\n- Analyze hashtag effectiveness and reach\n- Track follower growth and audience interaction\n- Generate weekly/monthly performance reports\n\n### 2. Competitor Research\n**Goal**: Monitor competitor social media activity\n- Track competitor posting frequency and timing\n- Analyze competitor content themes and strategies\n- Monitor competitor engagement and audience response\n- Identify trending topics and hashtags in your industry\n\n### 3. Brand Monitoring\n**Goal**: Track brand mentions and sentiment analysis\n- Monitor specific Twitter accounts for brand mentions\n- Track hashtag campaigns and user-generated content\n- Analyze sentiment trends and audience feedback\n- Identify influencers and brand advocates\n\n### 4. Content Strategy Development\n**Goal**: Analyze successful content patterns\n- Identify high-performing tweet formats and topics\n- Track optimal posting times and frequencies\n- Analyze hashtag performance and reach\n- Study audience engagement patterns\n\n### 5. Market Research\n**Goal**: Collect social media data for market analysis\n- Gather consumer opinions and feedback\n- Track industry trends and discussions\n- Monitor product launches and market reactions\n- Support product development with social insights\n\n## ‚öô Advanced Configuration\n\n### Batch Processing Multiple Profiles\nTo monitor multiple Twitter accounts efficiently:\n1. Create a master sheet with profile URLs and date ranges\n2. Add a loop node to process each profile separately\n3. Implement delays between requests to respect rate limits\n4. Use separate sheets or tabs for different profiles\n\n### Adding Data Analysis\nEnhance the workflow with analytical capabilities:\n1. Create additional sheets for processed data and insights\n2. Add formulas to calculate engagement rates and trends\n3. Implement data visualization with charts and graphs\n4. Generate automated reports and summaries\n\n### Integration with Business Tools\nConnect the workflow to your existing systems:\n- **CRM Integration**: Update customer records with social media data\n- **Slack Notifications**: Send alerts when data collection is complete\n- **Database Storage**: Store data in PostgreSQL/MySQL for advanced analysis\n- **BI Tools**: Connect to Tableau/Power BI for comprehensive visualization\n\n## üìà Performance & Limits\n\n### Expected Performance\n- **Single profile**: 30 seconds to 5 minutes (depending on date range)\n- **Data accuracy**: 95%+ for public Twitter profiles\n- **Success rate**: 90%+ for accessible accounts\n- **Daily capacity**: 10-50 profiles (depends on rate limits and data volume)\n\n### Resource Usage\n- **Memory**: ~200MB per execution\n- **Storage**: Minimal (data stored in Google Sheets)\n- **API calls**: 1 Bright Data call + multiple Google Sheets calls per profile\n- **Bandwidth**: ~5-10MB per profile scraped\n- **Execution time**: 2-10 minutes for typical date ranges\n\n### Scaling Considerations\n- **Rate limiting**: Add delays for high-volume scraping\n- **Error handling**: Implement retry logic for failed requests\n- **Data validation**: Add checks for malformed or missing data\n- **Monitoring**: Track success/failure rates over time\n- **Cost optimization**: Monitor API usage to control costs\n\n## ü§ù Support & Community\n\n### Getting Help\n- **n8n Community Forum**: community.n8n.io\n- **Documentation**: docs.n8n.io\n- **Bright Data Support**: Contact through your dashboard\n- **GitHub Issues**: Report bugs and feature requests\n\n### Contributing\n- Share improvements with the community\n- Report issues and suggest enhancements\n- Create variations for specific use cases\n- Document best practices and lessons learned\n\n## üìã Quick Setup Checklist\n\n### Before You Start\n‚òê n8n instance running (self-hosted or cloud)  \n‚òê Bright Data account with Twitter dataset access  \n‚òê Google account with Sheets access  \n‚òê Valid Twitter profile URLs ready for scraping  \n‚òê 10-15 minutes available for setup\n\n### Setup Steps\n‚òê **Import Workflow** - Copy JSON and import to n8n  \n‚òê **Configure Bright Data** - Set up API credentials and test  \n‚òê **Create Google Sheet** - New sheet with proper column structure  \n‚òê **Set up Google Sheets credentials** - OAuth setup and test  \n‚òê **Update workflow settings** - Replace API keys and sheet IDs  \n‚òê **Test with sample data** - Add 1 Twitter URL and small date range  \n‚òê **Verify data flow** - Check data appears in Google Sheet correctly  \n‚òê **Activate workflow** - Enable form trigger for production use\n\n## Ready to Use! üéâ\n\n**Your workflow URL**: Access form trigger when workflow is active\n\nüéØ **Happy Twitter Scraping!** This workflow provides a solid foundation for automated Twitter data collection. Customize it to fit your specific social media analytics and research needs.\n\n\n\nFor any questions or support, please contact:  \n[info@incrementors.com](mailto:info@incrementors.com)  \nor fill out this form: [https://www.incrementors.com/contact-us/](https://www.incrementors.com/contact-us/)",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "üì• User Input Trigger",
      "type": "n8n-nodes-base.formTrigger",
      "role": "formTrigger",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "üöÄ Trigger Twitter Scraping",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "üîÑ Monitor Scraping Progress",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "‚è±Ô∏è Delay Before Recheck",
      "type": "n8n-nodes-base.wait",
      "role": "wait",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "‚úÖ Is Scraping Ready?",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "üì¶ Fetch Twitter Data",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "üìä Store Twitter Data in Google Sheet",
      "type": "n8n-nodes-base.googleSheets",
      "role": "googleSheets",
      "configDescription": "Version 4.6"
    },
    {
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}