{
  "id": 7561,
  "slug": "7561",
  "title": "Customer support chatbot with RAG using OpenAI and Pinecone",
  "description": "# ðŸ¤– Simple RAG Customer Support Chatbot\n\n## ðŸ“‹ Overview\n\nThis intelligent customer support chatbot leverages Retrieval-Augmented Generation (RAG) to provide accurate, contextual responses by combining your knowledge base with AI capabilities. The system automatically retrieves relevant documents from your Pinecone vector store and uses them to generate informed responses through OpenAI's language models.\n\n## âš¡ Quick Setup\n\n1. Import Workflow\nImport this workflow template into your n8n instance\n2. Configure Credentials\nAdd the following API credentials:\n\n- OpenAI API Key: For chat completions and embeddings\n- Pinecone API Key: For vector database operations\n- Google Drive: For document auto ingestion\n\n3. Initialize Vector Store\nUse the \"Insert documents into Pinecone\" workflow to populate your knowledge base\n5. Activate Workflow\nEnable the main chat workflow to start receiving requests\n\n## ðŸ”§ How it Works\n\n**Main Chat Flow (Agent Workflow)**\n\nUser Message â†’ Memory Retrieval â†’ Vector Search â†’ Context Assembly â†’ AI Response â†’ Memory Update â†’ Response\n\n**Process Flow:**\n\nMessage Reception: Webhook receives user chat messages with session management\nMemory Retrieval: Loads conversation history for context continuity\nSemantic Search: Queries Pinecone vector store for relevant documents\nContext Assembly: Combines retrieved documents with conversation history\nAI Generation: OpenAI generates contextual response using assembled context\nMemory Storage: Updates conversation memory for future interactions\nResponse Delivery: Returns formatted response to user interface\n\n**Document Ingestion Flow**\n\nDocument Source â†’ Text Extraction â†’ Chunking â†’ Embedding â†’ Vector Storage\n\n**Process Flow:**\n\nDocument Trigger: Google Drive or manual file upload detection\nContent Extraction: Extracts text from various file formats (PDF, DOC, TXT)\nText Chunking: Splits documents into optimal chunks for embedding\nEmbedding Generation: Creates vector embeddings using OpenAI\nVector Storage: Stores embeddings in Pinecone with metadata\nIndex Update: Updates search index for immediate availability\n\n",
  "featuredImage": "/data/workflows/7561/7561.webp",
  "author": {
    "id": 101,
    "slug": "ilyass",
    "name": "Ilyass Kanissi",
    "avatar": ""
  },
  "categories": [
    "AI RAG",
    "Multimodal AI"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 1610,
  "downloads": 161,
  "createdAt": "2025-08-19T01:13:48.911Z",
  "updatedAt": "2026-01-16T08:51:07.222Z",
  "publishedAt": "2025-08-19T01:13:48.911Z",
  "nodes": 15,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/7561",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Customer support chatbot with RAG using OpenAI and Pinecone",
    "workflowName": "Customer support chatbot with RAG using OpenAI and Pinecone",
    "description": "# ðŸ¤– Simple RAG Customer Support Chatbot\n\n## ðŸ“‹ Overview\n\nThis intelligent customer support chatbot leverages Retrieval-Augmented Generation (RAG) to provide accurate, contextual responses by combining your knowledge base with AI capabilities. The system automatically retrieves relevant documents from your Pinecone vector store and uses them to generate informed responses through OpenAI's language models.\n\n## âš¡ Quick Setup\n\n1. Import Workflow\nImport this workflow template into your n8n instance\n2. Configure Credentials\nAdd the following API credentials:\n\n- OpenAI API Key: For chat completions and embeddings\n- Pinecone API Key: For vector database operations\n- Google Drive: For document auto ingestion\n\n3. Initialize Vector Store\nUse the \"Insert documents into Pinecone\" workflow to populate your knowledge base\n5. Activate Workflow\nEnable the main chat workflow to start receiving requests\n\n## ðŸ”§ How it Works\n\n**Main Chat Flow (Agent Workflow)**\n\nUser Message â†’ Memory Retrieval â†’ Vector Search â†’ Context Assembly â†’ AI Response â†’ Memory Update â†’ Response\n\n**Process Flow:**\n\nMessage Reception: Webhook receives user chat messages with session management\nMemory Retrieval: Loads conversation history for context continuity\nSemantic Search: Queries Pinecone vector store for relevant documents\nContext Assembly: Combines retrieved documents with conversation history\nAI Generation: OpenAI generates contextual response using assembled context\nMemory Storage: Updates conversation memory for future interactions\nResponse Delivery: Returns formatted response to user interface\n\n**Document Ingestion Flow**\n\nDocument Source â†’ Text Extraction â†’ Chunking â†’ Embedding â†’ Vector Storage\n\n**Process Flow:**\n\nDocument Trigger: Google Drive or manual file upload detection\nContent Extraction: Extracts text from various file formats (PDF, DOC, TXT)\nText Chunking: Splits documents into optimal chunks for embedding\nEmbedding Generation: Creates vector embeddings using OpenAI\nVector Storage: Stores embeddings in Pinecone with metadata\nIndex Update: Updates search index for immediate availability",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Google Drive Trigger",
      "type": "n8n-nodes-base.googleDriveTrigger",
      "role": "googleDriveTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "Download file",
      "type": "n8n-nodes-base.googleDrive",
      "role": "googleDrive",
      "configDescription": "Version 3"
    },
    {
      "name": "Pinecone Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStorePinecone",
      "role": "vectorStorePinecone",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Embeddings OpenAI",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Recursive Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
      "role": "textSplitterRecursiveCharacterTextSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 2.2"
    },
    {
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Simple Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Embeddings OpenAI1",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Reranker Cohere",
      "type": "@n8n/n8n-nodes-langchain.rerankerCohere",
      "role": "rerankerCohere",
      "configDescription": "Version 1"
    },
    {
      "name": "Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStorePinecone",
      "role": "vectorStorePinecone",
      "configDescription": "Version 1.3"
    }
  ]
}