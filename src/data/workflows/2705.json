{
  "id": 2705,
  "slug": "2705",
  "title": "Chat with GitHub API documentation: RAG-powered chatbot with Pinecone & OpenAI",
  "description": "This workflow demonstrates a Retrieval Augmented Generation (RAG) chatbot that lets you chat with the GitHub API Specification (documentation) using natural language. Built with n8n, OpenAI's LLMs and the Pinecone vector database, it provides accurate and context-aware responses to your questions about how to use the GitHub API.\nYou could adapt this to any OpenAPI specification for any public or private API, thus creating a documentation chatbout that anyone in your company can use.\n\n## How it works:\n\n* Data Ingestion: The workflow fetches the complete GitHub API OpenAPI 3 specification directly from the GitHub repository.\nChunking and Embeddings: It splits the large API spec into smaller, manageable chunks. OpenAI's embedding models then generate vector embeddings for each chunk, capturing their semantic meaning.\n* Vector Database Storage: These embeddings, along with the corresponding text chunks, are stored in a Pinecone vector database.\n* Chat Interface and Query Processing: The workflow provides a simple chat interface. When you ask a question, it generates an embedding for your query using the same OpenAI model.\n* Semantic Search and Retrieval: Pinecone is queried to find the most relevant text chunks from the API spec based on the query embedding.\n* Response Generation: The retrieved chunks and your original question are fed to OpenAI's `gpt-4o-mini` LLM, which generates a concise, informative, and contextually relevant answer, including code snippets when applicable.\n\n## Set up steps:\n\n* Create accounts: You'll need accounts with OpenAI and Pinecone.\n* API keys: Obtain API keys for both services.\nConfigure credentials: In your n8n environment, configure credentials for OpenAI and Pinecone using your API keys.\n* Import the workflow: Import this workflow into your n8n instance.\n* Pinecone Index: Ensure you have a Pinecone index named \"n8n-demo\" or adjust the workflow accordingly. The workflow is set up to work with this index out of the box.\n\n### Setup Time: Approximately 15-20 minutes.\n\n## Why use this workflow?\n\n* Learn RAG in Action: This is a practical, hands-on example of how to build a RAG-powered chatbot.\n* Adaptable Template: Easily modify this workflow to create chatbots for other APIs or knowledge bases.\n* n8n Made Easy: See how n8n simplifies complex integrations between data sources, vector databases, and LLMs.",
  "featuredImage": "/data/workflows/2705/2705.webp",
  "author": {
    "id": 101,
    "slug": "mihailtd",
    "name": "Mihai Farcas",
    "avatar": ""
  },
  "categories": [
    "Internal Wiki",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 29512,
  "downloads": 2951,
  "createdAt": "2025-01-07T20:05:27.294Z",
  "updatedAt": "2026-01-16T08:26:06.699Z",
  "publishedAt": "2025-01-07T20:05:27.294Z",
  "nodes": 17,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/2705",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Chat with GitHub API documentation: RAG-powered chatbot with Pinecone & OpenAI",
    "workflowName": "Chat with GitHub API documentation: RAG-powered chatbot with Pinecone & OpenAI",
    "description": "This workflow demonstrates a Retrieval Augmented Generation (RAG) chatbot that lets you chat with the GitHub API Specification (documentation) using natural language. Built with n8n, OpenAI's LLMs and the Pinecone vector database, it provides accurate and context-aware responses to your questions about how to use the GitHub API.\nYou could adapt this to any OpenAPI specification for any public or private API, thus creating a documentation chatbout that anyone in your company can use.\n\n## How it works:\n\n* Data Ingestion: The workflow fetches the complete GitHub API OpenAPI 3 specification directly from the GitHub repository.\nChunking and Embeddings: It splits the large API spec into smaller, manageable chunks. OpenAI's embedding models then generate vector embeddings for each chunk, capturing their semantic meaning.\n* Vector Database Storage: These embeddings, along with the corresponding text chunks, are stored in a Pinecone vector database.\n* Chat Interface and Query Processing: The workflow provides a simple chat interface. When you ask a question, it generates an embedding for your query using the same OpenAI model.\n* Semantic Search and Retrieval: Pinecone is queried to find the most relevant text chunks from the API spec based on the query embedding.\n* Response Generation: The retrieved chunks and your original question are fed to OpenAI's `gpt-4o-mini` LLM, which generates a concise, informative, and contextually relevant answer, including code snippets when applicable.\n\n## Set up steps:\n\n* Create accounts: You'll need accounts with OpenAI and Pinecone.\n* API keys: Obtain API keys for both services.\nConfigure credentials: In your n8n environment, configure credentials for OpenAI and Pinecone using your API keys.\n* Import the workflow: Import this workflow into your n8n instance.\n* Pinecone Index: Ensure you have a Pinecone index named \"n8n-demo\" or adjust the workflow accordingly. The workflow is set up to work with this index out of the box.\n\n### Setup Time: Approximately 15-20 minutes.\n\n## Why use this workflow?\n\n* Learn RAG in Action: This is a practical, hands-on example of how to build a RAG-powered chatbot.\n* Adaptable Template: Easily modify this workflow to create chatbots for other APIs or knowledge bases.\n* n8n Made Easy: See how n8n simplifies complex integrations between data sources, vector databases, and LLMs.",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "When clicking ‘Test workflow’",
      "type": "n8n-nodes-base.manualTrigger",
      "role": "manualTrigger",
      "configDescription": "Version 1"
    },
    {
      "name": "HTTP Request",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Pinecone Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStorePinecone",
      "role": "vectorStorePinecone",
      "configDescription": "Version 1"
    },
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1"
    },
    {
      "name": "Recursive Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
      "role": "textSplitterRecursiveCharacterTextSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "role": "chatTrigger",
      "configDescription": "Version 1.1"
    },
    {
      "name": "AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "role": "agent",
      "configDescription": "Version 1.7"
    },
    {
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Window Buffer Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "role": "memoryBufferWindow",
      "configDescription": "Version 1.3"
    },
    {
      "name": "Vector Store Tool",
      "type": "@n8n/n8n-nodes-langchain.toolVectorStore",
      "role": "toolVectorStore",
      "configDescription": "Version 1"
    },
    {
      "name": "OpenAI Chat Model1",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "role": "lmChatOpenAi",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Generate User Query Embedding",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Pinecone Vector Store (Querying)",
      "type": "@n8n/n8n-nodes-langchain.vectorStorePinecone",
      "role": "vectorStorePinecone",
      "configDescription": "Version 1"
    },
    {
      "name": "Generate Embeddings",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "role": "embeddingsOpenAi",
      "configDescription": "Version 1.2"
    },
    {
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}