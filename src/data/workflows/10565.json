{
  "id": 10565,
  "slug": "10565",
  "title": "Create RAG-ready knowledge bases from websites using Apify, Gemini & Supabase",
  "description": "Convert any website into a searchable vector database for AI chatbots. Submit a URL, choose scraping scope, and this workflow handles everything: scraping, cleaning, chunking, embedding, and storing in Supabase.\n\n   ## What it does\n  - Scrapes websites using Apify (3 modes: full site unlimited, full site limited, single URL)\n  - Cleans content (removes navigation, footer, ads, cookie banners, etc)\n  - Chunks text (800 chars, markdown-aware)\n  - Generates embeddings (Google Gemini, 768 dimensions)\n  - Stores in Supabase vector database\n\n  ## Requirements\n  - Apify account + API token\n  - Supabase database with pgvector extension\n  - Google Gemini API key\n\n  ## Setup\n  1. Create Supabase `documents` table with embedding column (vector 768). *[Run this SQL query](https://docs.langchain.com/oss/javascript/integrations/vectorstores/supabase) in your Supabase project to enable the vector store setup* \n  2. Add your Apify API token to all three \"Run Apify Scraper\" nodes\n  3. Add Supabase and Gemini credentials\n  4. Test with small site (5-10 pages) or single page/URL first\n\n  ## Next steps\n  Connect your vector store to an AI chatbot for RAG-powered Q&A, or build semantic search features into your apps.\n\n  **Tip:** Start with page limits to test content quality before full-site scraping. Review chunks in Supabase and adjust Apify filters if needed for better vector embeddings.\n\n---\n\n\n## Sample Outputs\n\n**Apify actor \"runs\" in Apify Dashboard from this workflow**\n![](https://i.postimg.cc/NMBqXSWs/Screenshot-2025-11-06-190813.png)\n\n\n**Supabase `docuemnts` table with scraped website content ingested in chunks with vector embeddings**\n![](https://i.postimg.cc/W4m0x8MG/Screenshot-2025-11-06-154326.png)\n",
  "featuredImage": "/data/workflows/10565/10565.webp",
  "author": {
    "id": 101,
    "slug": "deanjp",
    "name": "Dean Pike",
    "avatar": ""
  },
  "categories": [
    "Document Extraction",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 220,
  "downloads": 22,
  "createdAt": "2025-11-06T11:33:27.328Z",
  "updatedAt": "2026-01-16T09:05:32.016Z",
  "publishedAt": "2025-11-06T11:33:27.328Z",
  "nodes": 19,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/10565",
  "disclaimer": "This workflow is provided as-is. Please review and test before using in production.",
  "overview": {
    "title": "Create RAG-ready knowledge bases from websites using Apify, Gemini & Supabase",
    "workflowName": "Create RAG-ready knowledge bases from websites using Apify, Gemini & Supabase",
    "description": "Convert any website into a searchable vector database for AI chatbots. Submit a URL, choose scraping scope, and this workflow handles everything: scraping, cleaning, chunking, embedding, and storing in Supabase.\n\n   ## What it does\n  - Scrapes websites using Apify (3 modes: full site unlimited, full site limited, single URL)\n  - Cleans content (removes navigation, footer, ads, cookie banners, etc)\n  - Chunks text (800 chars, markdown-aware)\n  - Generates embeddings (Google Gemini, 768 dimensions)\n  - Stores in Supabase vector database\n\n  ## Requirements\n  - Apify account + API token\n  - Supabase database with pgvector extension\n  - Google Gemini API key\n\n  ## Setup\n  1. Create Supabase `documents` table with embedding column (vector 768). *[Run this SQL query](https://docs.langchain.com/oss/javascript/integrations/vectorstores/supabase) in your Supabase project to enable the vector store setup* \n  2. Add your Apify API token to all three \"Run Apify Scraper\" nodes\n  3. Add Supabase and Gemini credentials\n  4. Test with small site (5-10 pages) or single page/URL first\n\n  ## Next steps\n  Connect your vector store to an AI chatbot for RAG-powered Q&A, or build semantic search features into your apps.\n\n  **Tip:** Start with page limits to test content quality before full-site scraping. Review chunks in Supabase and adjust Apify filters if needed for better vector embeddings.\n\n---\n\n\n## Sample Outputs\n\n**Apify actor \"runs\" in Apify Dashboard from this workflow**\n![](https://i.postimg.cc/NMBqXSWs/Screenshot-2025-11-06-190813.png)\n\n\n**Supabase `docuemnts` table with scraped website content ingested in chunks with vector embeddings**\n![](https://i.postimg.cc/W4m0x8MG/Screenshot-2025-11-06-154326.png)",
    "features": [],
    "useCases": []
  },
  "logicalBlocks": [],
  "nodeDetails": [
    {
      "name": "Supabase Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreSupabase",
      "role": "vectorStoreSupabase",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Embeddings Google Gemini",
      "type": "@n8n/n8n-nodes-langchain.embeddingsGoogleGemini",
      "role": "embeddingsGoogleGemini",
      "configDescription": "Version 1"
    },
    {
      "name": "Improve Content Structure Quality",
      "type": "n8n-nodes-base.code",
      "role": "code",
      "configDescription": "Version 2"
    },
    {
      "name": "Recursive Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
      "role": "textSplitterRecursiveCharacterTextSplitter",
      "configDescription": "Version 1"
    },
    {
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "role": "documentDefaultDataLoader",
      "configDescription": "Version 1.1"
    },
    {
      "name": "Clean Data",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Enter Website URL and Settings",
      "type": "n8n-nodes-base.formTrigger",
      "role": "formTrigger",
      "configDescription": "Version 2.3"
    },
    {
      "name": "Prepare Settings for Apify Web Scraper",
      "type": "n8n-nodes-base.switch",
      "role": "switch",
      "configDescription": "Version 3.3"
    },
    {
      "name": "Set Data in Correct Request Format",
      "type": "n8n-nodes-base.set",
      "role": "set",
      "configDescription": "Version 3.4"
    },
    {
      "name": "Run Apify Scraper: Scrape All - w/Limit",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Run Apify Scraper: Scrape 1 URL Only",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "No Limit to Number of Scraped Pages?",
      "type": "n8n-nodes-base.if",
      "role": "if",
      "configDescription": "Version 2.2"
    },
    {
      "name": "Run Apify Scraper: Scrape All - No Limit",
      "type": "n8n-nodes-base.httpRequest",
      "role": "httpRequest",
      "configDescription": "Version 4.2"
    },
    {
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note7",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    },
    {
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "role": "stickyNote",
      "configDescription": "Version 1"
    }
  ]
}