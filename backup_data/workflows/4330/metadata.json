{
  "id": 4330,
  "slug": "4330",
  "title": "Automated resume job matching engine with Bright Data MCP & OpenAI 4o mini",
  "description": "![Automated Resume Job Matching Engine.png](fileId:1368)\n\n### Notice\nCommunity nodes can only be installed on self-hosted instances of n8n.\n\n### Who this is for\nThe Automated Resume Job Matching Engine is an intelligent workflow designed for career platforms, HR tech startups, recruiting firms, and AI developers who want to streamline job-resume matching using real-time data from LinkedIn and job boards.\n\nThis workflow is tailored for:\n\n- **HR Tech Founders** - Building next-gen recruiting products\n\n- **Recruiters & Talent Sourcers** - Seeking automated candidate-job fit evaluation\n\n- **Job Boards & Portals** - Enriching user experience with AI-driven job recommendations\n\n- **Career Coaches & Resume Writers** - Offering personalized job fit analysis\n\n- **AI Developers** - Automating large-scale matching tasks using LinkedIn and job data\n\n### What problem is this workflow solving?\nManually matching a resume to job description is time-consuming, biased, and inefficient. Additionally, accessing live job postings and candidate profiles requires overcoming web scraping limitations.\n\nThis workflow solves:\n\n- Automated LinkedIn profile and job post data extraction using Bright Data MCP infrastructure\n\n- Semantic matching between job requirements and candidate resume using OpenAI 4o mini\n\n- Pagination handling for high-volume job data\n\n- End-to-end automation from scraping to delivery via webhook and persisting the job matched response to disk\n\n### What this workflow does\n\n**Bright Data MCP for Job Data Extraction**\n- Uses Bright Data MCP Clients to extract multiple job listings (supports pagination)\n\n- Pulls job data from LinkedIn with the pre-defined filtering criteria's\n\n**OpenAI 4o mini LLM Matching Engine**\n- Extracts paginated job data from the Bright Data MCP extracted info via the MCP scrape_as_html tool.\n\n- Extracts textual job description information via the scraped job information by leveraging the Bright Data MCP scrape_as_html tool.\n\n- AI Job Matching node handles the job description and the candidate resume compare to generate match scores with insights\n\n**Data Delivery**\n- Sends final match report to a Webhook Notification endpoint\n\n- Persistence of AI matched job response to disk \n\n### Pre-conditions\n\n1. Knowledge of Model Context Protocol (MCP) is highly essential. Please read this blog post - [model-context-protocol](https://www.anthropic.com/news/model-context-protocol)\n2. You need to have the [Bright Data](https://brightdata.com/) account and do the necessary setup as mentioned in the **Setup** section below.\n3. You need to have the Google Gemini API Key. Visit [Google AI Studio](https://aistudio.google.com/)\n3. You need to install the Bright Data MCP Server [@brightdata/mcp](https://www.npmjs.com/package/@brightdata/mcp)\n4. You need to install the [n8n-nodes-mcp](https://github.com/nerding-io/n8n-nodes-mcp)\n\n### Setup\n\n1. Please make sure to setup n8n locally with MCP Servers by navigating to [n8n-nodes-mcp](https://github.com/nerding-io/n8n-nodes-mcp)\n2. Please make sure to install the Bright Data MCP Server [@brightdata/mcp](https://www.npmjs.com/package/@brightdata/mcp)  on your local machine.\n2. Sign up at [Bright Data](https://brightdata.com/).\n3. Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.\n4. Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.\n5. In n8n, configure the OpenAi account credentials.\n6. In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.\n![MCPClientAccount.png](fileId:1367)\nMake sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=&lt;your-token&gt;.\n7. Update the Set input fields for candidate resume, keywords and other filtering criteria's.\n8. Update the Webhook HTTP Request node with the Webhook endpoint of your choice.\n9. Update the file name and path to persist on disk.\n\n### How to customize this workflow to your needs\n\n**Target Different Job Boards**\n\n- Set input fields with the sites like Indeed, ZipRecruiter, or Monster\n\n**Customize Matching Criteria**\n- Adjust the prompt inside the AI Job Match node\n\n- Include scoring metrics like skills match %, experience relevance, or cultural fit\n\n**Automate Scheduling**\n- Use a Cron Node to periodically check for new jobs matching a profile\n\n- Set triggers based on webhook or input form submissions\n\n**Output Customization**\n- Add Markdown/PDF formatting for report summaries\n\n- Extend with Google Sheets export for internal analytics\n\n**Enhance Data Security**\n- Mask personal info before sending to external endpoints\n\n",
  "featuredImage": "/data/workflows/4330/4330.webp",
  "author": {
    "id": 101,
    "slug": "ranjancse",
    "name": "Ranjan Dailata",
    "avatar": ""
  },
  "categories": [
    "HR",
    "AI Summarization"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 2852,
  "downloads": 285,
  "createdAt": "2025-05-23T00:01:02.829Z",
  "updatedAt": "2026-01-16T08:33:50.682Z",
  "publishedAt": "2025-05-23T00:01:02.829Z",
  "nodes": 22,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/4330"
}