{
  "workflow": {
    "id": 9971,
    "name": "Track AI model executions with LangFuse observability for better performance insights",
    "views": 60,
    "recentViews": 0,
    "totalViews": 60,
    "createdAt": "2025-10-21T07:48:53.582Z",
    "description": "## About this template\nThis template is to demonstrate how to  trace the observations per execution ID in Langfuse via ingestion API.\n\n## Good to know\n* Endpoint: `https://cloud.langfuse.com/api/public/ingestion`\n* Auth is a `Generic Credential Type` with a `Basic Auth`: `username` = `you_public_key`, `password` = `your_secret_key`.\n\n## How it works\n* **Trigger**: the workflow is executed by another workflow after an AI run finishes (input parameter `execution_id`).\n\n* **Remove duplicates**\nEnsures we only process each `execution_id` once (optional but recommended).\n\n* **Wait to get execution data**\nDelay (60-80 secs) so totals and per-step metrics are available.\n\n* **Get execution**\nFetches workflow metadata and token totals.\n\n* **Code: structure execution data**\nNormalizes your run into an array of `perModelRuns` with model, tokens, latency, and text previews.\n\n* **Split Out** ‚Üí **Loop Over Items**\nIterates each run step.\n\n* **Code: prepare JSON for Langfuse**\n  Builds a batch with:\n    * trace-create (stable id trace-&lt;executionId&gt;, grouped into session-&lt;workflowId&gt;) \n    * generation-create (model, input/output, usage, timings from latency)\n\n\n* **HTTP Request to Langfuse**\nPosts the batch. Optional short Wait between sends.\n\n## Requirements\n1. Langfuse Cloud project and API keys\n2. n8n instance with the HTTP node\n\n## Customizing\n1. Add span-create and set `parentObservationId` on the generation to nest under spans.\n2. Add scores or feedback later via score-create.\n3. Replace `sessionId` strategy (per workflow, per user, etc.). If some steps don‚Äôt produce tokens, compute and set usage yourself before sending.",
    "workflow": {
      "id": "cFEhvBNcaSFxATXN",
      "meta": {
        "instanceId": "64e2e6219c21561109cfc6c128be1b2db6e516fa79f92287604734757bdc0881",
        "templateCredsSetupCompleted": true
      },
      "name": "Langfuse-tracing",
      "tags": [
        {
          "id": "IIhB5fqgApTNy0rp",
          "name": "tracing",
          "createdAt": "2025-10-12T18:54:49.152Z",
          "updatedAt": "2025-10-12T18:54:49.152Z"
        }
      ],
      "nodes": [
        {
          "id": "d792a10f-7746-4c53-9451-6f0472bae2be",
          "name": "When Executed by Another Workflow",
          "type": "n8n-nodes-base.executeWorkflowTrigger",
          "position": [
            0,
            0
          ],
          "parameters": {
            "workflowInputs": {
              "values": [
                {
                  "name": "execution_id"
                }
              ]
            }
          },
          "typeVersion": 1.1
        },
        {
          "id": "b1d16fb4-fce6-46a1-a612-70b5559d9af3",
          "name": "n8n",
          "type": "n8n-nodes-base.n8n",
          "position": [
            660,
            0
          ],
          "parameters": {
            "options": {
              "activeWorkflows": true
            },
            "resource": "execution",
            "operation": "get",
            "executionId": "={{ $('When Executed by Another Workflow').item.json.execution_id }}",
            "requestOptions": {}
          },
          "credentials": {
            "n8nApi": {
              "id": "credential-id",
              "name": "n8nApi Credential"
            }
          },
          "typeVersion": 1
        },
        {
          "id": "b6bedd99-921a-4160-887c-303c9956b850",
          "name": "Split Out",
          "type": "n8n-nodes-base.splitOut",
          "position": [
            1120,
            0
          ],
          "parameters": {
            "include": "selectedOtherFields",
            "options": {},
            "fieldToSplitOut": "perModelRuns",
            "fieldsToInclude": "workflowId, , workflowName, executionId, startedAt, stoppedAt, executionMs, executionSec, totals_promptTokens, totals_completionTokens, totals_totalTokens"
          },
          "typeVersion": 1
        },
        {
          "id": "d2524dd7-554f-425e-879b-53b070b793e7",
          "name": "Loop Over Items",
          "type": "n8n-nodes-base.splitInBatches",
          "position": [
            1340,
            0
          ],
          "parameters": {
            "options": {}
          },
          "typeVersion": 3
        },
        {
          "id": "9a7e20bc-607f-4e17-b4f4-7dac76212551",
          "name": "HTTP Request",
          "type": "n8n-nodes-base.httpRequest",
          "position": [
            2100,
            60
          ],
          "parameters": {
            "url": "https://cloud.langfuse.com/api/public/ingestion",
            "method": "POST",
            "options": {},
            "jsonBody": "={{ { batch: $json.batch } }}",
            "sendBody": true,
            "sendHeaders": true,
            "specifyBody": "json",
            "authentication": "genericCredentialType",
            "genericAuthType": "httpBasicAuth",
            "headerParameters": {
              "parameters": [
                {
                  "name": "Content-Type",
                  "value": "application/json"
                }
              ]
            }
          },
          "credentials": {
            "httpBasicAuth": {
              "id": "credential-id",
              "name": "httpBasicAuth Credential"
            }
          },
          "typeVersion": 4.2
        },
        {
          "id": "7108e1e5-975d-43ed-a877-990544994b50",
          "name": "Wait1",
          "type": "n8n-nodes-base.wait",
          "position": [
            1820,
            60
          ],
          "webhookId": "4fdb5c55-03c9-4b96-b021-e91b2d1c09aa",
          "parameters": {
            "amount": 3
          },
          "typeVersion": 1.1
        },
        {
          "id": "8665bccf-5646-4544-ac39-fcefceec9e28",
          "name": "Remove Duplicates",
          "type": "n8n-nodes-base.removeDuplicates",
          "position": [
            220,
            0
          ],
          "parameters": {
            "options": {}
          },
          "typeVersion": 2
        },
        {
          "id": "3bdd9fdb-2508-46b4-b167-62dcfedcd71b",
          "name": "Wait to get an execution data",
          "type": "n8n-nodes-base.wait",
          "position": [
            440,
            0
          ],
          "webhookId": "339e09b8-4206-46c9-9f30-dca7fd6ab0fb",
          "parameters": {
            "amount": 80
          },
          "typeVersion": 1.1
        },
        {
          "id": "93852674-c6a5-415f-b532-f70c14f24151",
          "name": "Code: structure execution data",
          "type": "n8n-nodes-base.code",
          "position": [
            880,
            0
          ],
          "parameters": {
            "jsCode": "function toArray(x){return Array.isArray(x)?x:(x==null?[]:[x]);}\nfunction safeGet(obj, path, fallback=undefined){\n  try { return path.split(\".\").reduce((o,k)=>o==null?undefined:o[k], obj) ?? fallback; }\n  catch { return fallback; }\n}\nfunction flattenMessages(msgs){\n  const arr = Array.isArray(msgs)?msgs:(msgs==null?[]:[msgs]);\n  return { array: arr, joined: arr.join(\"\\n\") };\n}\n\nconst raw = $input.first().json;\nconst exec = Array.isArray(raw) ? raw[0] : raw;\n\nconst runData = safeGet(exec, \"data.resultData.runData\", {});\nconst workflow = safeGet(exec, \"workflowData\", {});\nconst startedAt = new Date(exec.startedAt || exec.createdAt || Date.now());\nconst stoppedAt = new Date(exec.stoppedAt || Date.now());\nconst executionMs = Math.max(0, stoppedAt - startedAt);\n\n// Build lookups by node and executionIndex\nconst nodeRuns = {};\nfor (const [nodeName, runs] of Object.entries(runData)) {\n  const arr = toArray(runs);\n  const byIndex = new Map();\n  const indices = [];\n  for (const r of arr) {\n    const idx = Number(r?.executionIndex);\n    if (Number.isFinite(idx)) {\n      byIndex.set(idx, r);\n      indices.push(idx);\n    }\n  }\n  indices.sort((a,b)=>a-b);\n  nodeRuns[nodeName] = { all: arr, byIndex, indices };\n}\n\n// Precompute candidate prompt nodes: names starting with \"Get a prompt\"\nconst promptNodeNames = Object.keys(nodeRuns).filter(n => n.startsWith(\"Get a prompt\"));\n\n// Aggregated latency per node\nconst perNodeLatency = Object.entries(runData).reduce((acc, [nodeName, runs]) => {\n  const arr = toArray(runs);\n  const times = [];\n  for (const r of arr) {\n    const t = Number(r?.executionTime ?? 0);\n    if (!Number.isNaN(t) && t >= 0) times.push(t);\n  }\n  const sum = times.reduce((s,v)=>s+v,0);\n  const count = times.length;\n  const avg = count ? sum / count : 0;\n  acc[nodeName] = { count, sumMs: sum, avgMs: avg, samplesMs: times };\n  return acc;\n}, {});\n\n// nearest index ‚â§ target\nfunction nearestLE(indices, target){\n  if (!indices || indices.length === 0) return undefined;\n  let lo = 0, hi = indices.length - 1, best;\n  while (lo <= hi) {\n    const mid = (lo + hi) >> 1;\n    const v = indices[mid];\n    if (v === target) return v;\n    if (v < target) { best = v; lo = mid + 1; }\n    else { hi = mid - 1; }\n  }\n  return best;\n}\n\n// Choose the best \"Get a prompt*\" run for a given LLM run index\nfunction choosePromptRunForIndex(idx, prevNodeName) {\n  if (!Number.isFinite(idx) || promptNodeNames.length === 0) return { nodeName: \"\", run: undefined };\n\n  // 1) If prevNode is a \"Get a prompt*\" node, try to use it first\n  if (prevNodeName && prevNodeName.startsWith(\"Get a prompt\") && nodeRuns[prevNodeName]) {\n    const group = nodeRuns[prevNodeName];\n    let run = group.byIndex.get(idx - 1);\n    if (!run) {\n      const near = nearestLE(group.indices, idx);\n      if (near != null) run = group.byIndex.get(near);\n    }\n    if (!run && group.indices.length) {\n      run = group.byIndex.get(group.indices[group.indices.length - 1]);\n    }\n    if (run) return { nodeName: prevNodeName, run };\n  }\n\n  // 2) Otherwise, scan all prompt nodes and pick the closest to (idx - 1)\n  let best = { nodeName: \"\", run: undefined, score: Infinity, diff: Infinity };\n  const target = idx - 1;\n\n  for (const name of promptNodeNames) {\n    const group = nodeRuns[name];\n    if (!group || group.indices.length === 0) continue;\n\n    // Try exact (idx-1)\n    let candidate = group.byIndex.get(target);\n    let candIdx = candidate ? target : undefined;\n\n    if (!candidate) {\n      // nearest ‚â§ idx\n      const near = nearestLE(group.indices, idx);\n      if (near != null) {\n        candidate = group.byIndex.get(near);\n        candIdx = near;\n      }\n    }\n    if (!candidate) {\n      // fallback: last run\n      const lastIdx = group.indices[group.indices.length - 1];\n      candidate = group.byIndex.get(lastIdx);\n      candIdx = lastIdx;\n    }\n    if (!candidate || candIdx == null) continue;\n\n    const diff = Math.abs(candIdx - target);\n    const penalty = candIdx <= idx ? 0 : 0.5;\n    const score = diff + penalty;\n\n    if (score < best.score) {\n      best = { nodeName: name, run: candidate, score, diff };\n    }\n  }\n\n  return { nodeName: best.nodeName, run: best.run };\n}\n\n// Resolve prompt from any \"Get a prompt*\" node (metadata only)\nfunction resolvePromptForRun(run) {\n  const idx = Number(run?.executionIndex);\n  const prevNode = safeGet(run, \"source.0.previousNode\", \"\");\n\n  const { nodeName: chosenNode, run: promptRun } = choosePromptRunForIndex(idx, prevNode);\n  if (!promptRun) return { promptName: \"\", promptVersion: undefined, promptText: \"\", promptNode: \"\" };\n\n  const promptJson = safeGet(promptRun, \"data.main.0.0.json\", {});\n  return {\n    promptName: String(promptJson.name ?? \"\"),\n    promptVersion: (promptJson.version != null) ? Number(promptJson.version) : undefined,\n    promptText: String(promptJson.prompt ?? promptJson.text ?? \"\"),\n    promptNode: chosenNode\n  };\n}\n\n// Focus OpenAI Chat Model* runs (OpenAI Chat Model, OpenAI Chat Model1..N)\nconst openAiNodeNames = Object.keys(nodeRuns).filter(n =>\n  n === \"OpenAI Chat Model\" || n.startsWith(\"OpenAI Chat Model\")\n);\nconst openAiRuns = [];\nfor (const name of openAiNodeNames) {\n  const group = nodeRuns[name];\n  if (!group) continue;\n  for (const r of group.all) {\n    openAiRuns.push({ run: r, nodeName: name });\n  }\n}\n\n// Helper: extract only the part after the last \"Human:\" (or \"User:\") tag,\n// and strip leading punctuation like \":\" or \"-\" and whitespace/newlines.\nfunction extractHumanPortion(text){\n  if (!text) return \"\";\n  const tags = [\"Human:\", \"User:\"];\n  let lastPos = -1;\n  let lastTag = \"\";\n  for (const tag of tags) {\n    const pos1 = text.lastIndexOf(\"\\n\" + tag);\n    const pos2 = text.lastIndexOf(tag);\n    const pos = Math.max(pos1, pos2);\n    if (pos > lastPos) { lastPos = pos; lastTag = tag; }\n  }\n  if (lastPos === -1) return text;\n  let start = lastPos;\n  if (text.slice(start, start + 1) === \"\\n\") start += 1;\n  start += lastTag.length;\n  let out = text.slice(start);\n  out = out.replace(/^[:\\-‚Äì‚Äî\\s]+/, \"\");\n  return out;\n}\n\n// Robust completion text extractor across variants\nfunction getCompletionText(r){\n  // Common n8n LangChain variants\n  const paths = [\n    \"data.ai_languageModel.0.0.json.response.generations.0.0.text\",\n    \"data.ai_languageModel.0.0.json.response.generations.0.text\",\n    \"data.ai_languageModel.0.0.json.response.generations.0.message.content\",\n    \"data.ai_languageModel.0.0.json.response.generations.0.0.message.content\",\n    // Sometimes under data.main\n    \"data.main.0.0.json.response.generations.0.0.text\",\n    \"data.main.0.0.json.response.generations.0.text\",\n    \"data.main.0.0.json.response.generations.0.message.content\",\n    \"data.main.0.0.json.response.generations.0.0.message.content\",\n    // OpenAI-style choices if ever passed through\n    \"data.ai_languageModel.0.0.json.response.choices.0.message.content\",\n    \"data.main.0.0.json.response.choices.0.message.content\",\n    // Fallback simple text\n    \"data.ai_languageModel.0.0.json.response.text\",\n    \"data.main.0.0.json.response.text\"\n  ];\n  for (const p of paths) {\n    const v = safeGet(r, p);\n    if (typeof v === \"string\" && v.length) return v;\n  }\n  return \"\";\n}\n\n// Collect all LLM runs with input/output fields\nconst perRun = [];\nfor (const { run: r, nodeName: llmNodeName } of openAiRuns) {\n  const model =\n    safeGet(r, \"inputOverride.ai_languageModel.0.0.json.options.model\") ||\n    safeGet(r, \"inputOverride.ai_languageModel.0.0.json.model\") ||\n    \"unknown-model\";\n\n  const promptTokens = Number(safeGet(r, \"data.ai_languageModel.0.0.json.tokenUsage.promptTokens\", 0));\n  const completionTokens = Number(safeGet(r, \"data.ai_languageModel.0.0.json.tokenUsage.completionTokens\", 0));\n  const totalTokensRaw = Number(safeGet(r, \"data.ai_languageModel.0.0.json.tokenUsage.totalTokens\", 0));\n  const totalTokens = totalTokensRaw || (promptTokens + completionTokens);\n\n  // Input to AI node: messages array under inputOverride\n  const messages = safeGet(r, \"inputOverride.ai_languageModel.0.0.json.messages\", []);\n  const promptFromMessages = flattenMessages(messages);\n  const inputTextAll = promptFromMessages.joined || \"\";\n  const inputTextHumanOnly = extractHumanPortion(inputTextAll);\n\n  // Output text (robust)\n  const completionText = getCompletionText(r);\n\n  const nodeName =\n    safeGet(r, \"source.0.previousNode\") ||\n    safeGet(r, \"metadata.subRun.0.node\") ||\n    llmNodeName ||\n    \"Unknown Node\";\n\n  // Keep prompt metadata from \"Get a prompt*\" for reference (name/version/node)\n  const { promptName, promptVersion, promptText, promptNode } = resolvePromptForRun(r);\n\n  const latencyMs = Number(r?.executionTime ?? 0);\n\n  // Previews\n  const promptPreview = (inputTextHumanOnly || \"\").slice(0, 2000);\n  const completionPreview = (completionText || \"\").slice(0, 2000);\n\n  perRun.push({\n    model,\n    nodeName,\n    latencyMs,\n    promptTokens,\n    completionTokens,\n    totalTokens,\n    promptName,                 // metadata only\n    promptVersion,              // metadata only\n    promptText: inputTextHumanOnly, // canonical input (Human-only)\n    promptJoined: inputTextHumanOnly,\n    completionText,\n    promptPreview,\n    completionPreview,\n    promptNode: promptNode || \"\"\n  });\n}\n\n// Aggregate per model\nconst perModelMap = new Map();\nfor (const r of perRun) {\n  if (!perModelMap.has(r.model)) {\n    perModelMap.set(r.model, {\n      model: r.model,\n      runs: 0,\n      promptTokens: 0,\n      completionTokens: 0,\n      totalTokens: 0,\n      nodeNamesSet: new Set(),\n      nodeNameCounts: new Map(),\n      examples: [],\n      latencyByNode: new Map(),\n    });\n  }\n  const m = perModelMap.get(r.model);\n  m.runs += 1;\n  m.promptTokens += r.promptTokens;\n  m.completionTokens += r.completionTokens;\n  m.totalTokens += r.totalTokens;\n  m.nodeNamesSet.add(r.nodeName);\n  m.nodeNameCounts.set(r.nodeName, (m.nodeNameCounts.get(r.nodeName) || 0) + 1);\n\n  if (!m.latencyByNode.has(r.nodeName)) m.latencyByNode.set(r.nodeName, { sumMs: 0, count: 0 });\n  const agg = m.latencyByNode.get(r.nodeName);\n  if (Number.isFinite(r.latencyMs) && r.latencyMs >= 0) {\n    agg.sumMs += r.latencyMs;\n    agg.count += 1;\n  }\n\n  if (m.examples.length < 2) {\n    m.examples.push({\n      promptPreview: (r.promptJoined || \"\").slice(0, 200),\n      completionPreview: (r.completionText || \"\").slice(0, 200),\n    });\n  }\n}\n\nconst perModel = Array.from(perModelMap.values()).map(m => {\n  const latencyMsByNode = Array.from(m.latencyByNode.entries()).map(([nodeName, a]) => ({\n    nodeName,\n    sumMs: a.sumMs,\n    count: a.count,\n    avgMs: a.count ? a.sumMs / a.count : 0,\n  })).sort((a,b)=>a.nodeName.localeCompare(b.nodeName));\n\n  return {\n    model: m.model,\n    runs: m.runs,\n    promptTokens: m.promptTokens,\n    completionTokens: m.completionTokens,\n    totalTokens: m.totalTokens,\n    nodeNames: Array.from(m.nodeNamesSet).sort(),\n    nodeNameCounts: Array.from(m.nodeNameCounts.entries())\n      .map(([nodeName, runs]) => ({ nodeName, runs }))\n      .sort((a,b)=>a.nodeName.localeCompare(b.nodeName)),\n    latencyMsByNode,\n    examples: m.examples,\n  };\n});\n\nconst totals = perModel.reduce((acc,m)=>{\n  acc.models += 1;\n  acc.runs += m.runs;\n  acc.promptTokens += m.promptTokens;\n  acc.completionTokens += m.completionTokens;\n  acc.totalTokens += m.totalTokens;\n  return acc;\n}, { models:0, runs:0, promptTokens:0, completionTokens:0, totalTokens:0 });\n\nconst summary = {\n  workflowId: workflow.id || exec.workflowId || \"\",\n  workflowName: workflow.name || \"n8n-workflow\",\n  executionId: String(exec.id || \"\"),\n  startedAt: startedAt.toISOString(),\n  stoppedAt: stoppedAt.toISOString(),\n  executionMs,\n  executionSec: executionMs / 1000,\n  perModel,\n  totals,\n  firstPrompt: perRun[0]?.promptJoined || \"\",\n  firstCompletion: perRun[0]?.completionText || \"\",\n  lastPrompt: perRun[perRun.length - 1]?.promptJoined || \"\",\n  lastCompletion: perRun[perRun.length - 1]?.completionText || \"\",\n};\n\n// Flat runs for downstream nodes\nconst perModelRuns = perRun.map(r => ({\n  model: r.model,\n  nodeName: r.nodeName,\n  latencyMs: r.latencyMs,\n  promptTokens: r.promptTokens,\n  completionTokens: r.completionTokens,\n  totalTokens: r.totalTokens,\n  promptName: r.promptName || \"\",\n  promptVersion: r.promptVersion,\n  promptText: r.promptText || \"\",      // Human-only input\n  promptPreview: r.promptPreview,      // Human-only preview\n  completionPreview: r.completionPreview,\n  promptNode: r.promptNode || \"\"\n}));\n\nreturn [{\n  json: {\n    workflowId: summary.workflowId,\n    workflowName: summary.workflowName,\n    executionId: summary.executionId,\n    startedAt: summary.startedAt,\n    stoppedAt: summary.stoppedAt,\n    executionMs: summary.executionMs,\n    executionSec: summary.executionSec,\n    totals_promptTokens: totals.promptTokens,\n    totals_completionTokens: totals.completionTokens,\n    totals_totalTokens: totals.totalTokens,\n    perNodeLatency,\n    perModel,\n    perModelRuns,\n    summary\n  }\n}];"
          },
          "typeVersion": 2
        },
        {
          "id": "4f571c0d-a993-469c-b4cb-aead0a9f9d47",
          "name": "Code: prepare JSON for LF",
          "type": "n8n-nodes-base.code",
          "position": [
            1600,
            60
          ],
          "parameters": {
            "jsCode": "const batch = [];\n\n// Stable identifiers for the whole execution\nconst executionId = String($json.executionId || $execution.id);\nconst traceId = `trace-${executionId}`;\n\n// Grouping: one session per workflow (adjust)\nconst sessionId = `session-${$json.workflowId || 'unknown'}`;\n\n// Pull per-run fields\nconst run = $json.perModelRuns || {};\nconst model = run.model === \"gpt-4.1-mini\" ? \"gpt-4o-mini\" : (run.model || \"unknown\");\nconst inputText = run.promptText || run.promptPreview || \"\";\nconst outputText = run.completionPreview || \"\";\nconst latencyMs = Number(run.latencyMs ?? 1);\nconst safeLatency = Number.isFinite(latencyMs) && latencyMs > 0 ? latencyMs : 1;\n\n// Timing\nconst end = Date.now();\nconst start = end - safeLatency;\n\n// Prompt naming\nconst promptName = run.promptName || $json.promptName || $json.nodeName || \"Prompt\";\nconst promptVersion = ($json.promptVersion != null) ? Number($json.promptVersion) : null;\n\n// Prompt link\nconst promptBaseUrl = \"https://cloud.langfuse.com/project/cmdpyxcw40047ad072og9tzqg/prompts/\";\nconst promptLink = promptBaseUrl + encodeURIComponent(String(promptName || \"\").trim());\n\n// Build events\nconst traceEventId = `evt-trace-${traceId}`;\nconst genEventId = `evt-gen-${traceId}-${$itemIndex != null ? $itemIndex : 0}-${end}`;\n\n// TRACE (idempotent)\nbatch.push({\n  id: traceEventId,\n  timestamp: new Date().toISOString(),\n  type: \"trace-create\",\n  body: {\n    id: traceId,\n    timestamp: new Date().toISOString(),\n    name: $json.workflowName || promptName,\n    userId: `workflow-${$json.workflowId || 'unknown'}`,\n    sessionId,\n    tags: [\"n8n\", \"execution\", $json.nodeName || \"node\"],\n    input: inputText,\n    output: outputText,\n    metadata: {\n      source: \"n8n\",\n      workflowId: $json.workflowId,\n      workflowName: $json.workflowName,\n      executionId,\n      promptName,\n      promptVersion,\n      promptLink,\n      workflowTotalTokens: Number($json.totals_totalTokens) || 0,\n      workflowPromptTokens: Number($json.totals_promptTokens) || 0,\n      workflowCompletionTokens: Number($json.totals_completionTokens) || 0\n    }\n  }\n});\n\n// GENERATION\nbatch.push({\n  id: genEventId,\n  timestamp: new Date().toISOString(),\n  type: \"generation-create\",\n  body: {\n    id: `gen-${traceId}-${end}`,\n    traceId,\n    timestamp: new Date().toISOString(),\n    name: promptName,\n    model,\n    input: inputText,\n    output: outputText,\n    startTime: new Date(start).toISOString(),\n    endTime: new Date(end).toISOString(),\n    usage: {\n      promptTokens: Number(run.promptTokens) || 0,\n      completionTokens: Number(run.completionTokens) || 0,\n      totalTokens: Number(run.totalTokens) || 0\n    },\n    metadata: {\n      workflowId: $json.workflowId,\n      workflowName: $json.workflowName,\n      executionId,\n      promptName,\n      promptVersion,\n      promptLink,\n      latencyMs: safeLatency\n    }\n  }\n});\n\nreturn { json: { batch } };"
          },
          "typeVersion": 2
        },
        {
          "id": "d18ab1bc-c40b-4777-af2d-46c9ac0f34fa",
          "name": "Sticky Note",
          "type": "n8n-nodes-base.stickyNote",
          "position": [
            -20,
            200
          ],
          "parameters": {
            "width": 680,
            "height": 1120,
            "content": "## About this template\nThis template is to demonstrate how to  trace the observations per execution ID in Langfuse via ingestion API.\n\n## Good to know\n* Endpoint: https://cloud.langfuse.com/api/public/ingestion\n* Auth is a `Generic Credential Type` with a `Basic Auth`: `username` = `you_public_key`, `password` = `your_secret_key`.\n\n## How it works\n* **Trigger**: the workflow is executed by another workflow after an AI run finishes (input parameter `execution_id`).\n\n* **Remove duplicates**\nEnsures we only process each `execution_id` once (optional but recommended).\n\n* **Wait to get execution data**\nDelay (60-80 secs) so totals and per-step metrics are available.\n\n* **Get execution**\nFetches workflow metadata and token totals.\n\n* **Code: structure execution data**\nNormalizes your run into an array of `perModelRuns` with model, tokens, latency, and text previews.\n\n* **Split Out** ‚Üí **Loop Over Items**\nIterates each run step.\n\n* **Code: prepare JSON for Langfuse**\n  Builds a batch with:\n    * trace-create (stable id trace-<executionId>, grouped into session-<workflowId>) \n    * generation-create (model, input/output, usage, timings from latency)\n\n\n* **HTTP Request to Langfuse**\nPosts the batch. Optional short Wait between sends.\n\n## Requirements\n1. Langfuse Cloud project and API keys\n2. n8n instance with the HTTP node\n\n## Customizing\n1. Add span-create and set parentObservationId on the generation to nest under spans.\n2. Add scores or feedback later via score-create.\n3. Replace `sessionId` strategy (per workflow, per user, etc.). If some steps don‚Äôt produce tokens, compute and set usage yourself before sending."
          },
          "typeVersion": 1
        },
        {
          "id": "f7c974c4-4fe1-4359-8d8d-954f062f245c",
          "name": "Sticky Note1",
          "type": "n8n-nodes-base.stickyNote",
          "position": [
            800,
            -460
          ],
          "parameters": {
            "width": 460,
            "height": 400,
            "content": "## Code: structure execution data\n\n### What the node does\n\n1. Reads a full n8n execution (from ‚ÄúGet execution‚Äù).\n2. Finds all OpenAI Chat Model runs, extracts model, latency, token usage, input and output text.\n3. Heuristically links each LLM run to the nearest ‚ÄúGet a prompt*‚Äù node to capture prompt name/version/text.\n4. Aggregates usage by model and per node, and returns:\n* perModelRuns: flat per‚Äërun records for downstream nodes (used by the Langfuse step).\n* perModel: usage/latency aggregated by model.\n* totals: overall token totals.\n* perNodeLatency: latency stats per node.\n* summary: execution‚Äëlevel metadata and first/last prompts."
          },
          "typeVersion": 1
        }
      ],
      "active": false,
      "pinData": {},
      "settings": {
        "executionOrder": "v1"
      },
      "versionId": "418b6a95-03df-428c-825f-387362823aad",
      "connections": {
        "n8n": {
          "main": [
            [
              {
                "node": "Code: structure execution data",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "Wait1": {
          "main": [
            [
              {
                "node": "HTTP Request",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "Split Out": {
          "main": [
            [
              {
                "node": "Loop Over Items",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "HTTP Request": {
          "main": [
            [
              {
                "node": "Loop Over Items",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "Loop Over Items": {
          "main": [
            [],
            [
              {
                "node": "Code: prepare JSON for LF",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "Remove Duplicates": {
          "main": [
            [
              {
                "node": "Wait to get an execution data",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "Code: prepare JSON for LF": {
          "main": [
            [
              {
                "node": "Wait1",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "Wait to get an execution data": {
          "main": [
            [
              {
                "node": "n8n",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "Code: structure execution data": {
          "main": [
            [
              {
                "node": "Split Out",
                "type": "main",
                "index": 0
              }
            ]
          ]
        },
        "When Executed by Another Workflow": {
          "main": [
            [
              {
                "node": "Remove Duplicates",
                "type": "main",
                "index": 0
              }
            ]
          ]
        }
      }
    },
    "lastUpdatedBy": 1,
    "workflowInfo": {
      "nodeCount": 12,
      "nodeTypes": {
        "n8n-nodes-base.n8n": {
          "count": 1
        },
        "n8n-nodes-base.code": {
          "count": 2
        },
        "n8n-nodes-base.wait": {
          "count": 2
        },
        "n8n-nodes-base.splitOut": {
          "count": 1
        },
        "n8n-nodes-base.stickyNote": {
          "count": 2
        },
        "n8n-nodes-base.httpRequest": {
          "count": 1
        },
        "n8n-nodes-base.splitInBatches": {
          "count": 1
        },
        "n8n-nodes-base.removeDuplicates": {
          "count": 1
        },
        "n8n-nodes-base.executeWorkflowTrigger": {
          "count": 1
        }
      }
    },
    "status": "published",
    "user": {
      "name": "Artem Makarov",
      "username": "makarovartyom",
      "bio": "AI/ML engineer with 6+ years of experience. Having a deep expertise in AI tools, I select the right stack for your task to rapidly build end-to-end PoC and MVP.",
      "verified": false,
      "links": [
        ""
      ],
      "avatar": "https://gravatar.com/avatar/4fc05afb8185baa548cb73c0cf2787148864f125c93d3b7b0d8e70a2fd8f1c46?r=pg&d=retro&size=200"
    },
    "nodes": [
      {
        "id": 19,
        "icon": "file:httprequest.svg",
        "name": "n8n-nodes-base.httpRequest",
        "codex": {
          "data": {
            "alias": [
              "API",
              "Request",
              "URL",
              "Build",
              "cURL"
            ],
            "resources": {
              "generic": [
                {
                  "url": "https://n8n.io/blog/2021-the-year-to-automate-the-new-you-with-n8n/",
                  "icon": "‚òÄÔ∏è",
                  "label": "2021: The Year to Automate the New You with n8n"
                },
                {
                  "url": "https://n8n.io/blog/why-business-process-automation-with-n8n-can-change-your-daily-life/",
                  "icon": "üß¨",
                  "label": "Why business process automation with n8n can change your daily life"
                },
                {
                  "url": "https://n8n.io/blog/automatically-pulling-and-visualizing-data-with-n8n/",
                  "icon": "üìà",
                  "label": "Automatically pulling and visualizing data with n8n"
                },
                {
                  "url": "https://n8n.io/blog/learn-how-to-automatically-cross-post-your-content-with-n8n/",
                  "icon": "‚úçÔ∏è",
                  "label": "Learn how to automatically cross-post your content with n8n"
                },
                {
                  "url": "https://n8n.io/blog/automatically-adding-expense-receipts-to-google-sheets-with-telegram-mindee-twilio-and-n8n/",
                  "icon": "üßæ",
                  "label": "Automatically Adding Expense Receipts to Google Sheets with Telegram, Mindee, Twilio, and n8n"
                },
                {
                  "url": "https://n8n.io/blog/running-n8n-on-ships-an-interview-with-maranics/",
                  "icon": "üõ≥",
                  "label": "Running n8n on ships: An interview with Maranics"
                },
                {
                  "url": "https://n8n.io/blog/what-are-apis-how-to-use-them-with-no-code/",
                  "icon": " ü™¢",
                  "label": "What are APIs and how to use them with no code"
                },
                {
                  "url": "https://n8n.io/blog/5-tasks-you-can-automate-with-notion-api/",
                  "icon": "‚ö°Ô∏è",
                  "label": "5 tasks you can automate with the new Notion API "
                },
                {
                  "url": "https://n8n.io/blog/world-poetry-day-workflow/",
                  "icon": "üìú",
                  "label": "Celebrating World Poetry Day with a daily poem in Telegram"
                },
                {
                  "url": "https://n8n.io/blog/automate-google-apps-for-productivity/",
                  "icon": "üí°",
                  "label": "15 Google apps you can combine and automate to increase productivity"
                },
                {
                  "url": "https://n8n.io/blog/automate-designs-with-bannerbear-and-n8n/",
                  "icon": "üé®",
                  "label": "Automate Designs with Bannerbear and n8n"
                },
                {
                  "url": "https://n8n.io/blog/how-uproc-scraped-a-multi-page-website-with-a-low-code-workflow/",
                  "icon": " üï∏Ô∏è",
                  "label": "How uProc scraped a multi-page website with a low-code workflow"
                },
                {
                  "url": "https://n8n.io/blog/building-an-expense-tracking-app-in-10-minutes/",
                  "icon": "üì±",
                  "label": "Building an expense tracking app in 10 minutes"
                },
                {
                  "url": "https://n8n.io/blog/5-workflow-automations-for-mattermost-that-we-love-at-n8n/",
                  "icon": "ü§ñ",
                  "label": "5 workflow automations for Mattermost that we love at n8n"
                },
                {
                  "url": "https://n8n.io/blog/how-to-use-the-http-request-node-the-swiss-army-knife-for-workflow-automation/",
                  "icon": "üß∞",
                  "label": "How to use the HTTP Request Node - The Swiss Army Knife for Workflow Automation"
                },
                {
                  "url": "https://n8n.io/blog/learn-how-to-use-webhooks-with-mattermost-slash-commands/",
                  "icon": "ü¶Ñ",
                  "label": "Learn how to use webhooks with Mattermost slash commands"
                },
                {
                  "url": "https://n8n.io/blog/how-a-membership-development-manager-automates-his-work-and-investments/",
                  "icon": "üìà",
                  "label": "How a Membership Development Manager automates his work and investments"
                },
                {
                  "url": "https://n8n.io/blog/a-low-code-bitcoin-ticker-built-with-questdb-and-n8n-io/",
                  "icon": "üìà",
                  "label": "A low-code bitcoin ticker built with QuestDB and n8n.io"
                },
                {
                  "url": "https://n8n.io/blog/how-to-set-up-a-ci-cd-pipeline-with-no-code/",
                  "icon": "üé°",
                  "label": "How to set up a no-code CI/CD pipeline with GitHub and TravisCI"
                },
                {
                  "url": "https://n8n.io/blog/automations-for-activists/",
                  "icon": "‚ú®",
                  "label": "How Common Knowledge use workflow automation for activism"
                },
                {
                  "url": "https://n8n.io/blog/creating-scheduled-text-affirmations-with-n8n/",
                  "icon": "ü§ü",
                  "label": "Creating scheduled text affirmations with n8n"
                },
                {
                  "url": "https://n8n.io/blog/how-goomer-automated-their-operations-with-over-200-n8n-workflows/",
                  "icon": "üõµ",
                  "label": "How Goomer automated their operations with over 200 n8n workflows"
                },
                {
                  "url": "https://n8n.io/blog/aws-workflow-automation/",
                  "label": "7 no-code workflow automations for Amazon Web Services"
                }
              ],
              "primaryDocumentation": [
                {
                  "url": "https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.httprequest/"
                }
              ]
            },
            "categories": [
              "Development",
              "Core Nodes"
            ],
            "nodeVersion": "1.0",
            "codexVersion": "1.0",
            "subcategories": {
              "Core Nodes": [
                "Helpers"
              ]
            }
          }
        },
        "group": "[\"output\"]",
        "defaults": {
          "name": "HTTP Request",
          "color": "#0004F5"
        },
        "iconData": {
          "type": "file",
          "fileBuffer": "data:image/svg+xml;base64,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"
        },
        "displayName": "HTTP Request",
        "typeVersion": 4,
        "nodeCategories": [
          {
            "id": 5,
            "name": "Development"
          },
          {
            "id": 9,
            "name": "Core Nodes"
          }
        ]
      },
      {
        "id": 39,
        "icon": "fa:sync",
        "name": "n8n-nodes-base.splitInBatches",
        "codex": {
          "data": {
            "alias": [
              "Loop",
              "Concatenate",
              "Batch",
              "Split",
              "Split In Batches"
            ],
            "resources": {
              "generic": [
                {
                  "url": "https://n8n.io/blog/how-uproc-scraped-a-multi-page-website-with-a-low-code-workflow/",
                  "icon": " üï∏Ô∏è",
                  "label": "How uProc scraped a multi-page website with a low-code workflow"
                },
                {
                  "url": "https://n8n.io/blog/benefits-of-automation-and-n8n-an-interview-with-hubspots-hugh-durkin/",
                  "icon": "üéñ",
                  "label": "Benefits of automation and n8n: An interview with HubSpot's Hugh Durkin"
                }
              ],
              "primaryDocumentation": [
                {
                  "url": "https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.splitinbatches/"
                }
              ]
            },
            "categories": [
              "Core Nodes"
            ],
            "nodeVersion": "1.0",
            "codexVersion": "1.0",
            "subcategories": {
              "Core Nodes": [
                "Flow"
              ]
            }
          }
        },
        "group": "[\"organization\"]",
        "defaults": {
          "name": "Loop Over Items",
          "color": "#007755"
        },
        "iconData": {
          "icon": "sync",
          "type": "icon"
        },
        "displayName": "Loop Over Items (Split in Batches)",
        "typeVersion": 3,
        "nodeCategories": [
          {
            "id": 9,
            "name": "Core Nodes"
          }
        ]
      },
      {
        "id": 514,
        "icon": "fa:pause-circle",
        "name": "n8n-nodes-base.wait",
        "codex": {
          "data": {
            "alias": [
              "pause",
              "sleep",
              "delay",
              "timeout"
            ],
            "resources": {
              "generic": [
                {
                  "url": "https://n8n.io/blog/how-to-get-started-with-crm-automation-and-no-code-workflow-ideas/",
                  "icon": "üë•",
                  "label": "How to get started with CRM automation (with 3 no-code workflow ideas"
                },
                {
                  "url": "https://n8n.io/blog/aws-workflow-automation/",
                  "label": "7 no-code workflow automations for Amazon Web Services"
                }
              ],
              "primaryDocumentation": [
                {
                  "url": "https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.wait/"
                }
              ]
            },
            "categories": [
              "Core Nodes"
            ],
            "nodeVersion": "1.0",
            "codexVersion": "1.0",
            "subcategories": {
              "Core Nodes": [
                "Helpers",
                "Flow"
              ]
            }
          }
        },
        "group": "[\"organization\"]",
        "defaults": {
          "name": "Wait",
          "color": "#804050"
        },
        "iconData": {
          "icon": "pause-circle",
          "type": "icon"
        },
        "displayName": "Wait",
        "typeVersion": 1,
        "nodeCategories": [
          {
            "id": 9,
            "name": "Core Nodes"
          }
        ]
      },
      {
        "id": 565,
        "icon": "fa:sticky-note",
        "name": "n8n-nodes-base.stickyNote",
        "codex": {
          "data": {
            "alias": [
              "Comments",
              "Notes",
              "Sticky"
            ],
            "categories": [
              "Core Nodes"
            ],
            "nodeVersion": "1.0",
            "codexVersion": "1.0",
            "subcategories": {
              "Core Nodes": [
                "Helpers"
              ]
            }
          }
        },
        "group": "[\"input\"]",
        "defaults": {
          "name": "Sticky Note",
          "color": "#FFD233"
        },
        "iconData": {
          "icon": "sticky-note",
          "type": "icon"
        },
        "displayName": "Sticky Note",
        "typeVersion": 1,
        "nodeCategories": [
          {
            "id": 9,
            "name": "Core Nodes"
          }
        ]
      },
      {
        "id": 826,
        "icon": "file:n8n.svg",
        "name": "n8n-nodes-base.n8n",
        "codex": {
          "data": {
            "alias": [
              "Workflow",
              "Execution"
            ],
            "resources": {
              "primaryDocumentation": [
                {
                  "url": "https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.n8n/"
                }
              ],
              "credentialDocumentation": [
                {
                  "url": "https://docs.n8n.io/api/authentication/"
                }
              ]
            },
            "categories": [
              "Development",
              "Core Nodes"
            ],
            "nodeVersion": "1.0",
            "codexVersion": "1.0",
            "subcategories": {
              "Core Nodes": [
                "Helpers",
                "Other Trigger Nodes"
              ]
            }
          }
        },
        "group": "[\"transform\"]",
        "defaults": {
          "name": "n8n"
        },
        "iconData": {
          "type": "file",
          "fileBuffer": "data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGZpbGw9Im5vbmUiIHZpZXdCb3g9IjAgMCAyMzAgMTIwIj48cGF0aCBmaWxsPSIjRUE0QjcxIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik0yMDQgNDhjLTExLjE4MyAwLTIwLjU4LTcuNjQ5LTIzLjI0NC0xOGgtMjcuNTA4YTEyIDEyIDAgMCAwLTExLjgzNiAxMC4wMjdsLS45ODcgNS45MTlBMjMuOTQgMjMuOTQgMCAwIDEgMTMyLjYyNiA2MGEyMy45NCAyMy45NCAwIDAgMSA3Ljc5OSAxNC4wNTRsLjk4NyA1LjkxOUExMiAxMiAwIDAgMCAxNTMuMjQ4IDkwaDMuNTA4QzE1OS40MiA3OS42NDkgMTY4LjgxNyA3MiAxODAgNzJjMTMuMjU1IDAgMjQgMTAuNzQ1IDI0IDI0cy0xMC43NDUgMjQtMjQgMjRjLTExLjE4MyAwLTIwLjU4LTcuNjQ5LTIzLjI0NC0xOGgtMy41MDhjLTExLjczMiAwLTIxLjc0NC04LjQ4Mi0yMy42NzMtMjAuMDU0bC0uOTg3LTUuOTE5QTEyIDEyIDAgMCAwIDExNi43NTIgNjZoLTkuNTA4QzEwNC41OCA3Ni4zNTEgOTUuMTgzIDg0IDg0IDg0cy0yMC41OC03LjY0OS0yMy4yNDQtMThINDcuMjQ0QzQ0LjU4IDc2LjM1MSAzNS4xODMgODQgMjQgODQgMTAuNzQ1IDg0IDAgNzMuMjU1IDAgNjBzMTAuNzQ1LTI0IDI0LTI0YzExLjE4MyAwIDIwLjU4IDcuNjQ5IDIzLjI0NCAxOGgxMy41MTJDNjMuNDIgNDMuNjQ5IDcyLjgxNyAzNiA4NCAzNnMyMC41OCA3LjY0OSAyMy4yNDQgMThoOS41MDhhMTIgMTIgMCAwIDAgMTEuODM2LTEwLjAyN2wuOTg3LTUuOTE5QzEzMS41MDQgMjYuNDgyIDE0MS41MTYgMTggMTUzLjI0OCAxOGgyNy41MDhDMTgzLjQyIDcuNjQ5IDE5Mi44MTcgMCAyMDQgMGMxMy4yNTUgMCAyNCAxMC43NDUgMjQgMjRzLTEwLjc0NSAyNC0yNCAyNG0wLTEyYzYuNjI3IDAgMTItNS4zNzMgMTItMTJzLTUuMzczLTEyLTEyLTEyLTEyIDUuMzczLTEyIDEyIDUuMzczIDEyIDEyIDEyTTI0IDcyYzYuNjI3IDAgMTItNS4zNzMgMTItMTJzLTUuMzczLTEyLTEyLTEyLTEyIDUuMzczLTEyIDEyIDUuMzczIDEyIDEyIDEybTcyLTEyYzAgNi42MjctNS4zNzMgMTItMTIgMTJzLTEyLTUuMzczLTEyLTEyIDUuMzczLTEyIDEyLTEyIDEyIDUuMzczIDEyIDEybTk2IDM2YzAgNi42MjctNS4zNzMgMTItMTIgMTJzLTEyLTUuMzczLTEyLTEyIDUuMzczLTEyIDEyLTEyIDEyIDUuMzczIDEyIDEyIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiLz48L3N2Zz4="
        },
        "displayName": "n8n",
        "typeVersion": 1,
        "nodeCategories": [
          {
            "id": 5,
            "name": "Development"
          },
          {
            "id": 9,
            "name": "Core Nodes"
          }
        ]
      },
      {
        "id": 834,
        "icon": "file:code.svg",
        "name": "n8n-nodes-base.code",
        "codex": {
          "data": {
            "alias": [
              "cpde",
              "Javascript",
              "JS",
              "Python",
              "Script",
              "Custom Code",
              "Function"
            ],
            "details": "The Code node allows you to execute JavaScript in your workflow.",
            "resources": {
              "primaryDocumentation": [
                {
                  "url": "https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.code/"
                }
              ]
            },
            "categories": [
              "Development",
              "Core Nodes"
            ],
            "nodeVersion": "1.0",
            "codexVersion": "1.0",
            "subcategories": {
              "Core Nodes": [
                "Helpers",
                "Data Transformation"
              ]
            }
          }
        },
        "group": "[\"transform\"]",
        "defaults": {
          "name": "Code"
        },
        "iconData": {
          "type": "file",
          "fileBuffer": "data:image/svg+xml;base64,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"
        },
        "displayName": "Code",
        "typeVersion": 2,
        "nodeCategories": [
          {
            "id": 5,
            "name": "Development"
          },
          {
            "id": 9,
            "name": "Core Nodes"
          }
        ]
      },
      {
        "id": 837,
        "icon": "fa:sign-out-alt",
        "name": "n8n-nodes-base.executeWorkflowTrigger",
        "codex": {
          "data": {
            "resources": {
              "generic": [],
              "primaryDocumentation": [
                {
                  "url": "https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.executeworkflowtrigger/"
                }
              ]
            },
            "categories": [
              "Core Nodes"
            ],
            "nodeVersion": "1.0",
            "codexVersion": "1.0",
            "subcategories": {
              "Core Nodes": [
                "Helpers"
              ]
            }
          }
        },
        "group": "[\"trigger\"]",
        "defaults": {
          "name": "When Executed by Another Workflow",
          "color": "#ff6d5a"
        },
        "iconData": {
          "icon": "sign-out-alt",
          "type": "icon"
        },
        "displayName": "Execute Workflow Trigger",
        "typeVersion": 1,
        "nodeCategories": [
          {
            "id": 9,
            "name": "Core Nodes"
          }
        ]
      },
      {
        "id": 1238,
        "icon": "file:removeDuplicates.svg",
        "name": "n8n-nodes-base.removeDuplicates",
        "codex": {
          "data": {
            "alias": [
              "Dedupe",
              "Deduplicate",
              "Duplicates",
              "Remove",
              "Unique",
              "Transform",
              "Array",
              "List",
              "Item"
            ],
            "details": "",
            "resources": {
              "generic": [],
              "primaryDocumentation": [
                {
                  "url": "https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.removeduplicates/"
                }
              ]
            },
            "categories": [
              "Core Nodes"
            ],
            "nodeVersion": "1.0",
            "codexVersion": "1.0",
            "subcategories": {
              "Core Nodes": [
                "Data Transformation"
              ]
            }
          }
        },
        "group": "[\"transform\"]",
        "defaults": {
          "name": "Remove Duplicates"
        },
        "iconData": {
          "type": "file",
          "fileBuffer": "data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI1MTIiIGhlaWdodD0iNTEyIiBmaWxsPSJub25lIj48ZyBmaWxsPSIjNTRCOEM5IiBjbGlwLXBhdGg9InVybCgjYSkiPjxwYXRoIGQ9Ik0xMzQuMDk3IDExMWgzOC44Mjl2MzIuNTA4SDEzOC4xNnYzNC42MzVoLTMyLjUwOHYtMzguNjk5YzAtMTUuNzA5IDEyLjczNS0yOC40NDQgMjguNDQ1LTI4LjQ0NG03Ny42NTggMzIuNTA4VjExMWg3Ny42NTd2MzIuNTA4em0xMTYuNDg2IDBWMTExaDc3LjY1OHYzMi41MDh6bTExNi40ODcgMFYxMTFoMzguODI5YzE1LjcxIDAgMjguNDQ1IDEyLjczNSAyOC40NDUgMjguNDQ0djM4LjY5OWgtMzIuNTA4di0zNC42MzV6bTM0Ljc2NiA3My4yMzhoMzIuNTA4djM4LjY5OGMwIDE1LjcxLTEyLjczNSAyOC40NDUtMjguNDQ1IDI4LjQ0NWgtMzguODI5di0zMi41MDhoMzQuNzY2ek0wIDI0NC41MzdDMCAyMjkuMzI5IDEyLjczNSAyMTcgMjguNDQ0IDIxN2gzNDkuNDYxYzE1LjcwOSAwIDI4LjQ0NCAxMi4zMjkgMjguNDQ0IDI3LjUzN3YxMjkuODE1YzAgMTUuMjA4LTEyLjczNSAyNy41MzctMjguNDQ0IDI3LjUzN0gyOC40NDVDMTIuNzM0IDQwMS44ODkgMCAzODkuNTYgMCAzNzQuMzUyeiIvPjwvZz48ZGVmcz48Y2xpcFBhdGggaWQ9ImEiPjxwYXRoIGZpbGw9IiNmZmYiIGQ9Ik0wIDBoNTEydjUxMkgweiIvPjwvY2xpcFBhdGg+PC9kZWZzPjwvc3ZnPg=="
        },
        "displayName": "Remove Duplicates",
        "typeVersion": 2,
        "nodeCategories": [
          {
            "id": 9,
            "name": "Core Nodes"
          }
        ]
      },
      {
        "id": 1239,
        "icon": "file:splitOut.svg",
        "name": "n8n-nodes-base.splitOut",
        "codex": {
          "data": {
            "alias": [
              "Split",
              "Nested",
              "Transform",
              "Array",
              "List",
              "Item"
            ],
            "details": "",
            "resources": {
              "generic": [],
              "primaryDocumentation": [
                {
                  "url": "https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.splitout/"
                }
              ]
            },
            "categories": [
              "Core Nodes"
            ],
            "nodeVersion": "1.0",
            "codexVersion": "1.0",
            "subcategories": {
              "Core Nodes": [
                "Data Transformation"
              ]
            }
          }
        },
        "group": "[\"transform\"]",
        "defaults": {
          "name": "Split Out"
        },
        "iconData": {
          "type": "file",
          "fileBuffer": "data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI1MTIiIGhlaWdodD0iNTEyIiBmaWxsPSJub25lIj48ZyBmaWxsPSIjOUI2REQ1IiBjbGlwLXBhdGg9InVybCgjYSkiPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgZD0iTTQ4MCAxNDhjMC02LjYyNy01LjM3My0xMi0xMi0xMkgzMjJjLTYuNjI3IDAtMTIgNS4zNzMtMTIgMTJ2MjRjMCA2LjYyNyA1LjM3MyAxMiAxMiAxMmgxNDZjNi42MjcgMCAxMi01LjM3MyAxMi0xMnptMCA5NmMwLTYuNjI3LTUuMzczLTEyLTEyLTEySDMyMmMtNi42MjcgMC0xMiA1LjM3My0xMiAxMnYyNGMwIDYuNjI3IDUuMzczIDEyIDEyIDEyaDE0NmM2LjYyNyAwIDEyLTUuMzczIDEyLTEyem0wIDk2YzAtNi42MjctNS4zNzMtMTItMTItMTJIMzIyYy02LjYyNyAwLTEyIDUuMzczLTEyIDEydjI0YzAgNi42MjcgNS4zNzMgMTIgMTIgMTJoMTQ2YzYuNjI3IDAgMTItNS4zNzMgMTItMTJ6IiBjbGlwLXJ1bGU9ImV2ZW5vZGQiLz48cGF0aCBkPSJNNDM4IDc2YzAgNi42MjctNS4zNzMgMTItMTIgMTJIMzA5Ljc4M2MtMTcuNjczIDAtMzIgMTQuMzI3LTMyIDMydjU2YzAgMjYuOTc4LTEwLjI3MiA1MS41NTctMjcuMTE5IDcwLjAzOS01LjA1NSA1LjU0NS01LjA1NSAxNC4zNzcgMCAxOS45MjIgMTYuODQ3IDE4LjQ4MiAyNy4xMTkgNDMuMDYxIDI3LjExOSA3MC4wMzl2NTZjMCAxNy42NzMgMTQuMzI3IDMyIDMyIDMySDQyNmM2LjYyNyAwIDEyIDUuMzczIDEyIDEydjI0YzAgNi42MjctNS4zNzMgMTItMTIgMTJIMzA5Ljc4M2MtNDQuMTgzIDAtODAtMzUuODE3LTgwLTgwdi01NmMwLTMwLjkyOC0yNS4wNzItNTYtNTYtNTZhNS43ODMgNS43ODMgMCAwIDEtNS43ODMtNS43ODN2LTM2LjQzNGE1Ljc4MyA1Ljc4MyAwIDAgMSA1Ljc4My01Ljc4M2MzMC45MjggMCA1Ni0yNS4wNzIgNTYtNTZ2LTU2YzAtNDQuMTgzIDM1LjgxNy04MCA4MC04MEg0MjZjNi42MjcgMCAxMiA1LjM3MyAxMiAxMnoiLz48cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik0xMzYgMjQ0YzAtNi42MjctNS4zNzMtMTItMTItMTJIMTJjLTYuNjI3IDAtMTIgNS4zNzMtMTIgMTJ2MjRjMCA2LjYyNyA1LjM3MyAxMiAxMiAxMmgxMTJjNi42MjcgMCAxMi01LjM3MyAxMi0xMnoiIGNsaXAtcnVsZT0iZXZlbm9kZCIvPjwvZz48ZGVmcz48Y2xpcFBhdGggaWQ9ImEiPjxwYXRoIGZpbGw9IiNmZmYiIGQ9Ik01MTIgMEgwdjUxMmg1MTJ6Ii8+PC9jbGlwUGF0aD48L2RlZnM+PC9zdmc+"
        },
        "displayName": "Split Out",
        "typeVersion": 1,
        "nodeCategories": [
          {
            "id": 9,
            "name": "Core Nodes"
          }
        ]
      }
    ],
    "categories": [
      {
        "id": 5,
        "name": "Engineering"
      },
      {
        "id": 48,
        "name": "AI RAG"
      }
    ],
    "image": []
  }
}