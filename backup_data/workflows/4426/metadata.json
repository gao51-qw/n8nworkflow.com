{
  "id": 4426,
  "slug": "4426",
  "title": "Evaluate RAG response accuracy with OpenAI: document groundedness metric",
  "description": "### This n8n template demonstrates how to calculate the evaluation metric \"RAG document groundedness\" which in this scenario, measures the ability to provide or reference information included only in retrieved vector store documents.\n\nThe scoring approach is adapted from [https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates#pointwise_groundedness](https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates#pointwise_groundedness)\n\n### How it works\n* This evaluation works best for an agent that requires document retrieval from a vector store or similar source.\n* For our scoring, we need to collect the agent's response and the documents retrieved and use an LLM to assess if the former is based off the latter.\n* A key factor is to look out information in the response which is not mentioned in the documents.\n* A high score indicates LLM adherence and alignment whereas a low score could signal inadequate prompt or model hallucination.\n\n### Requirements\n* n8n version 1.94+\n* Check out this Google Sheet for a sample data [https://docs.google.com/spreadsheets/d/1YOnu2JJjlxd787AuYcg-wKbkjyjyZFgASYVV0jsij5Y/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1YOnu2JJjlxd787AuYcg-wKbkjyjyZFgASYVV0jsij5Y/edit?usp=sharing)\n",
  "featuredImage": "/data/workflows/4426/4426.webp",
  "author": {
    "id": 101,
    "slug": "jimleuk",
    "name": "Jimleuk",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "AI Summarization"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 1484,
  "downloads": 148,
  "createdAt": "2025-05-27T07:21:41.587Z",
  "updatedAt": "2026-01-16T08:34:19.788Z",
  "publishedAt": "2025-05-27T07:21:41.587Z",
  "nodes": 25,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/4426"
}