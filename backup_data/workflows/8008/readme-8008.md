# Smarter RAG agents with enriched retrieval and modular workflows

> An extendable RAG template to build powerful, explainable AI assistants â€” with query understanding, semantic metadata, and support for free-tier tools like Gemini, Gemma and Supabase.

## Description
This workflow helps you build smart, production-ready RAG agents that go far beyond basic document Q&A.

It includes:

âœ… File ingestion and chunking

âœ… Asynchronous LLM-powered enrichment

âœ… Filterable metadata-based search

âœ… Gemma-based query understanding and generation

âœ… Cohere re-ranking

âœ… Memory persistence via Postgres



Everything is modular, low-cost, and designed to run even with **free-tier LLMs and vector databases**.

Whether you want to build a chatbot, internal knowledge assistant, documentation search engine, or a filtered content explorer â€” this is your foundation.


## âš™ï¸ How It Works
This workflow is divided into 3 pipelines:

### ğŸ“¥ Ingestion
- Upload a PDF via form
- Extract text and chunk it for embedding
- Store in Supabase vector store using Google Gemini embeddings


### ğŸ§  Enrichment (Async)
- Scheduled task fetches new chunks
- Each chunk is enriched with LLM metadata (topics, use_case, risks, audience level, summary, etc.)
- Metadata is added to the vector DB for improved retrieval and filtering


### ğŸ¤– Agent Chat
- A user question triggers the RAG agent
- Query Builder transforms it into keywords and filters
- Vector DB is queried and reranked
- The final answer is generated using only retrieved evidence, with references
- Chat memory is managed via Postgres

## ğŸŒŸ Key Features
- **Asynchronous enrichment** â†’ Save tokens, batch process with free-tier LLMs like Gemma
- **Metadata-aware** â†’ Improved filtering and reranking
- **Explainable answers** â†’ Agent cites sources and sections
- **Chat memory** â†’ Persistent context with Postgres
- **Modular design** â†’ Swap LLMs, rerankers, vector DBs, and even enrichment schema
- **Free to run** â†’ Built with Gemini, Gemma, Cohere, Supabase (free tier-compatible)



## ğŸ” Required Credentials

|Tool|Use|
|-|-|-|
|Supabase w/ PostreSQL|Vector DB + storage|
|Google Gemini/Gemma|Embeddings & LLM|
|Cohere API|Re-ranking|
|PostgreSQL|Chat memory|



## ğŸ§° Customization Tips
- Swap extractFromFile with Notion/Google Drive integrations

- Extend Metadata Obtention prompt to fit your domain (e.g., financial, legal)

- Replace LLMs with OpenAI, Mistral, or Ollama 

- Replace Postgre Chat Memory with Simple Memory or any other

- Use a webhook instead of a form to automate ingestion

- Connect to Telegram/Slack UI with a few extra nodes


## ğŸ’¡ Use Cases
- Company knowledge base bot (internal docs, SOPs)

- Educational assistant with smart filtering (by topic or level)
- Legal or policy assistant that cites source sections
- Product documentation Q&A with multi-language support
- Training material assistant that highlights risks/examples
- Content Generation


## ğŸ§  Who Itâ€™s For
- Indie developers building smart chatbots
- AI consultants prototyping Q&A assistants
- Teams looking for an internal knowledge agent
- Anyone building affordable, explainable AI tools

## ğŸš€ Try It Out!

Deploy a modular RAG assistant using n8n, Supabase, and Gemini â€” fully customizable and almost free to run.

#### 1. ğŸ“ Prepare Your PDFs

- Use any internal documents, manuals, or reports in **PDF **format.

- Optional: Add Google Drive integration to automate ingestion.

#### 2. ğŸ§© Set Up Supabase

- Create a free Supabase project

- Use the table creation queries included in the workflow to set up your schema.

- Add your *supabaseUrl *and *supabaseKey *in your n8n credentials.

&gt; ğŸ’¡ Pro Tip:
Make sure you match the embedding dimensions to your model.
This workflow uses *Gemini text-embedding-04* (768-dim) â€” if switching to OpenAI, change your table vector size to 1536.

#### 3. ğŸ§  Connect Gemini & Gemma

- Use Gemini/Gemma for embeddings and optional metadata enrichment.

- Or deploy locally for lightweight async LLM processing (via Ollama/HuggingFace).

#### 4. âš™ï¸ Import the Workflow in n8n

- Open n8n (self-hosted or cloud).

- Import the workflow file and paste your credentials.


Youâ€™re ready to ingest, enrich, and query your document base.


## ğŸ’¬ Have Feedback or Ideas? Iâ€™d Love to Hear

This project is open, modular, and evolving â€” just like great workflows should be :).

If youâ€™ve tried it, built on top of it, or have suggestions for improvement, Iâ€™d genuinely love to hear from you. Letâ€™s share ideas, collaborate, or just connect as part of the n8n builder community.

ğŸ“§ [ascuncia.es@gmail.com](mailto:ascuncia.es+n8ncreator@gmail.com)

ğŸ”— [Linkedin](https://www.linkedin.com/in/alejandro-scuncia-60a62348/)

## ğŸ“Š Basic Information

- **Workflow ID:** 8008
- **Complexity:** advanced
- **Node Count:** 32
- **Views:** 2006
- **Downloads:** 200
- **Created:** 2025/8/29
- **Last Updated:** 2026/1/16
- **Source:** [View on n8n.io](https://n8n.io/workflows/8008)

## ğŸ‘¤ Author

- **Name:** Alejandro Scuncia
- **Username:** @ascuncia

## ğŸ·ï¸ Categories

- Internal Wiki
- AI RAG

## ğŸ”— Nodes Used

- **@n8n/n8n-nodes-langchain.documentDefaultDataLoader** 
- **stickyNote** (Ã—8)
- **@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter** 
- **supabase** (Ã—3)
- **@n8n/n8n-nodes-langchain.chatTrigger** 
- **@n8n/n8n-nodes-langchain.vectorStoreSupabase** (Ã—2)
- **@n8n/n8n-nodes-langchain.embeddingsGoogleGemini** (Ã—2)
- **formTrigger** 
- **extractFromFile** 
- **splitInBatches** 
- **merge** 
- **wait** 
- **set** (Ã—2)
- **@n8n/n8n-nodes-langchain.googleGemini** (Ã—2)
- **scheduleTrigger** 
- **@n8n/n8n-nodes-langchain.agent** 
- **@n8n/n8n-nodes-langchain.rerankerCohere** 
- **@n8n/n8n-nodes-langchain.lmChatGoogleGemini** 
- **@n8n/n8n-nodes-langchain.memoryPostgresChat** 

## ğŸš€ How to Use

1. Download the workflow JSON file
2. Import it into your n8n instance
3. Configure the credentials for the nodes
4. Activate and test the workflow

## ğŸ”€ Workflow Structure

This workflow contains 32 nodes with 23 node connections.

---

*This workflow was sourced from [n8n.io](https://n8n.io) community templates.*
