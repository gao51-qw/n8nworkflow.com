{
  "id": 6076,
  "slug": "6076",
  "title": "Indeed job scraper with AI filtering & company research using Apify and Tavily",
  "description": "*This workflow contains community nodes that are only compatible with the self-hosted version of n8n.*\n\nThis workflow **scrapes job listings** on indeed via Apify, **automatically gets that dataset**, extracts information about the listing **filters jobs off relevance**, finds a decision maker at the company and updates a database (google sheets) with that info for outreach. **All you need to do is run Apify actor** then the database will update with the processed data.\n\n#### Benefits:\n\nComplete Job search Automation - A webhook monitors the Apify actor which sends a integration and starts the process\nAI-Powered Filter - Uses ChatGPT to analyze content/context, identify company goals, and filters based on job description\nSmart Duplicate Prevention - Automatically tracks processed job listings in a database to avoid redundancy\nMulti-Platform Intelligence - Combines Indeed scraping, web research via Tavily, and enriches each listing\nNiche Focus - Process content from multiple niches 6 currently (hardcoded) but can be changed to fit other niches (just prompt the \"job filter\" node)\n\n#### How It Works:\n\n1. Indeed Job Discovery:\n\n- Search and apply filter for relevant job listings, copy and use URL in Apify\n- Uses Apify's Indeed job scraper to scrape job listings from the URL of interest\n- Automatically scrapes the information, stores it in a dataset and initiates a integration\n\n2. Oncoming Data Processing:\n\n- Loops over 500 items (can be changed) with a batch size of 55 items (can be changed) to avoid running into API timeouts.\n- Multiple filters to ensure all fields are scrapped with our required metrics (website must exist and number of employees &lt; 250)\n- Duplicate job listings are removed from oncoming batch to be processed\n\n\n3. Job Analysis & Filter:\n\n- An additional filter to remove any job listing from the oncoming batch if it already exists in the google sheets database\n- Then all new job listings gets pasted to chatGPT which uses information about the job post/description to determine if it is relevant to us\n- All relevant jobs get a new field \"verdict\" which is either true or false and we keep the ones where verdict is true\n\n\n4. Enrich & Update Database:\n\n- Uses Tavily to search for a decision maker (doesn't always finds one) and populate a row in google sheet with information about the job listing, the company and a decision maker at that company.\n- Waits for 1 minute and 30 seconds to avoid google sheets and chatGPT API timeouts then loops back to the next batch to start filtering again until all job listings are processed\n\n### Required Google Sheets Database Setup:\n\nBefore running this workflow, create a Google Sheets database with these exact column headers:\nEssential Columns:\n\njobUrl - Unique identifier for job listings\ntitle - Position Title\ndescriptionText - Description of job listing\nhiringDemand/isHighVolumeHiring - Are they hiring at high volume?\nhiringDemand/isUrgentHire - Are they hiring at high urgency?\nisRemote - Is this job remote?\njobType/0 - Job type: In person, Remote, Part-time, etc.\ncompanyCeo/name - CEO name collected from Tavily's search\nicebreaker - Column for holding custom icebreakers for each job listing (Not completed in the workflow. I will upload another that does this called \"Personalized IJSFE\")\nscrapedCeo - CEO name collected from Apify Scraper\nemail - Email listed on for job listing\ncompanyName - Name of company that posted the job\t\ncompanyDescription - Description of the company that posted the job\ncompanyLinks/corporateWebsite - Website of the company that posted the job\ncompanyNumEmployees - Number of employees the company listed that they have\nlocation/country - Location of where the job is to take place\nsalary/salaryText - Salary on job listing\n\nSetup Instructions:\n\nCreate a new Google Sheet with these column headers in the first row\nName the sheet whatever you please\nConnect your Google Sheets OAuth credentials in n8n\nUpdate the document ID in the workflow nodes\n\nThe merge logic relies on the id column to prevent duplicate processing, so this structure is essential for the workflow to function correctly.\nFeel free to reach out for additional help or clarification at my gmail: terflix45@gmail.com and I'll get back to you as soon as I can.\n\n\n### Set Up Steps:\n\n1. Configure Apify Integration:\n\n- Sign up for an Apify account and obtain API key\n- Get indeed job scraper actor and use Apify's integration to send a HTTP request to your n8n webhook (if test URL doesn't work use production URL)\n- Use Apify node with Resource: Dataset, Operation: Get items and use your Api key as your credentials\n\n2. Set Up AI Services:\n\n- Add OpenAI API credentials for job filtering\n- Add Tavily API credentials for company research \n- Set up appropriate rate limiting for cost control\n\n3. Database Configuration:\n\n- Create Google Sheets database with provided column structure\n- Connect Google Sheets OAuth credentials\n- Configure the merge logic for duplicate detection\n\n4. Content Filtering Setup:\n\n- Customize the AI prompts for your specific niche, requirements or interest\n- Adjust the filtering criteria to fit your needs\n",
  "featuredImage": "/data/workflows/6076/6076.webp",
  "author": {
    "id": 101,
    "slug": "adrian-bent",
    "name": "Adrian Bent",
    "avatar": ""
  },
  "categories": [
    "Lead Generation",
    "AI Summarization"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 2291,
  "downloads": 229,
  "createdAt": "2025-07-17T04:57:04.652Z",
  "updatedAt": "2026-01-16T08:43:00.434Z",
  "publishedAt": "2025-07-17T04:57:04.652Z",
  "nodes": 23,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/6076"
}