{
  "id": 4423,
  "slug": "4423",
  "title": "Evaluations metric: answer similarity",
  "description": "### This n8n template demonstrates how to calculate the evaluation metric \"Similarity\" which in this scenario, measures the consistency of the agent.\n\nThe scoring approach is adapted from the open-source evaluations project [RAGAS](https://docs.ragas.io/) and you can see the source here [https://github.com/explodinggradients/ragas/blob/main/ragas/src/ragas/metrics/_answer_similarity.py](https://github.com/explodinggradients/ragas/blob/main/ragas/src/ragas/metrics/_answer_similarity.py)\n\n### How it works\n* This evaluation works best where questions are close-ended or about facts where the answer can have little to no deviation.\n* For our scoring, we generate embeddings for both the AI's response and ground truth and calculate the cosine similarity between them.\n* A high score indicates LLM consistency with expected results whereas a low score could signal model hallucination.\n\n### Requirements\n* n8n version 1.94+\n* Check out this Google Sheet for a sample data [https://docs.google.com/spreadsheets/d/1YOnu2JJjlxd787AuYcg-wKbkjyjyZFgASYVV0jsij5Y/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1YOnu2JJjlxd787AuYcg-wKbkjyjyZFgASYVV0jsij5Y/edit?usp=sharing)\n",
  "featuredImage": "/data/workflows/4423/4423.webp",
  "author": {
    "id": 101,
    "slug": "jimleuk",
    "name": "Jimleuk",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "AI Summarization"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 1011,
  "downloads": 101,
  "createdAt": "2025-05-27T07:15:17.213Z",
  "updatedAt": "2026-01-16T08:34:18.534Z",
  "publishedAt": "2025-05-27T07:15:17.213Z",
  "nodes": 21,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/4423"
}