{
  "id": 8008,
  "slug": "8008",
  "title": "Smarter RAG agents with enriched retrieval and modular workflows",
  "description": "An extendable RAG template to build powerful, explainable AI assistants â€” with query understanding, semantic metadata, and support for free-tier tools like Gemini, Gemma and Supabase.\n\n## Description\nThis workflow helps you build smart, production-ready RAG agents that go far beyond basic document Q&A.\n\nIt includes:\n\nâœ… File ingestion and chunking\n\nâœ… Asynchronous LLM-powered enrichment\n\nâœ… Filterable metadata-based search\n\nâœ… Gemma-based query understanding and generation\n\nâœ… Cohere re-ranking\n\nâœ… Memory persistence via Postgres\n\n\n\nEverything is modular, low-cost, and designed to run even with **free-tier LLMs and vector databases**.\n\nWhether you want to build a chatbot, internal knowledge assistant, documentation search engine, or a filtered content explorer â€” this is your foundation.\n\n\n## âš™ï¸ How It Works\nThis workflow is divided into 3 pipelines:\n\n### ğŸ“¥ Ingestion\n- Upload a PDF via form\n- Extract text and chunk it for embedding\n- Store in Supabase vector store using Google Gemini embeddings\n\n\n### ğŸ§  Enrichment (Async)\n- Scheduled task fetches new chunks\n- Each chunk is enriched with LLM metadata (topics, use_case, risks, audience level, summary, etc.)\n- Metadata is added to the vector DB for improved retrieval and filtering\n\n\n### ğŸ¤– Agent Chat\n- A user question triggers the RAG agent\n- Query Builder transforms it into keywords and filters\n- Vector DB is queried and reranked\n- The final answer is generated using only retrieved evidence, with references\n- Chat memory is managed via Postgres\n\n## ğŸŒŸ Key Features\n- **Asynchronous enrichment** â†’ Save tokens, batch process with free-tier LLMs like Gemma\n- **Metadata-aware** â†’ Improved filtering and reranking\n- **Explainable answers** â†’ Agent cites sources and sections\n- **Chat memory** â†’ Persistent context with Postgres\n- **Modular design** â†’ Swap LLMs, rerankers, vector DBs, and even enrichment schema\n- **Free to run** â†’ Built with Gemini, Gemma, Cohere, Supabase (free tier-compatible)\n\n\n\n## ğŸ” Required Credentials\n\n|Tool|Use|\n|-|-|-|\n|Supabase w/ PostreSQL|Vector DB + storage|\n|Google Gemini/Gemma|Embeddings & LLM|\n|Cohere API|Re-ranking|\n|PostgreSQL|Chat memory|\n\n\n\n## ğŸ§° Customization Tips\n- Swap extractFromFile with Notion/Google Drive integrations\n\n- Extend Metadata Obtention prompt to fit your domain (e.g., financial, legal)\n\n- Replace LLMs with OpenAI, Mistral, or Ollama \n\n- Replace Postgre Chat Memory with Simple Memory or any other\n\n- Use a webhook instead of a form to automate ingestion\n\n- Connect to Telegram/Slack UI with a few extra nodes\n\n\n## ğŸ’¡ Use Cases\n- Company knowledge base bot (internal docs, SOPs)\n\n- Educational assistant with smart filtering (by topic or level)\n- Legal or policy assistant that cites source sections\n- Product documentation Q&A with multi-language support\n- Training material assistant that highlights risks/examples\n- Content Generation\n\n\n## ğŸ§  Who Itâ€™s For\n- Indie developers building smart chatbots\n- AI consultants prototyping Q&A assistants\n- Teams looking for an internal knowledge agent\n- Anyone building affordable, explainable AI tools\n\n## ğŸš€ Try It Out!\n\nDeploy a modular RAG assistant using n8n, Supabase, and Gemini â€” fully customizable and almost free to run.\n\n#### 1. ğŸ“ Prepare Your PDFs\n\n- Use any internal documents, manuals, or reports in **PDF **format.\n\n- Optional: Add Google Drive integration to automate ingestion.\n\n#### 2. ğŸ§© Set Up Supabase\n\n- Create a free Supabase project\n\n- Use the table creation queries included in the workflow to set up your schema.\n\n- Add your *supabaseUrl *and *supabaseKey *in your n8n credentials.\n\n&gt; ğŸ’¡ Pro Tip:\nMake sure you match the embedding dimensions to your model.\nThis workflow uses *Gemini text-embedding-04* (768-dim) â€” if switching to OpenAI, change your table vector size to 1536.\n\n#### 3. ğŸ§  Connect Gemini & Gemma\n\n- Use Gemini/Gemma for embeddings and optional metadata enrichment.\n\n- Or deploy locally for lightweight async LLM processing (via Ollama/HuggingFace).\n\n#### 4. âš™ï¸ Import the Workflow in n8n\n\n- Open n8n (self-hosted or cloud).\n\n- Import the workflow file and paste your credentials.\n\n\nYouâ€™re ready to ingest, enrich, and query your document base.\n\n\n## ğŸ’¬ Have Feedback or Ideas? Iâ€™d Love to Hear\n\nThis project is open, modular, and evolving â€” just like great workflows should be :).\n\nIf youâ€™ve tried it, built on top of it, or have suggestions for improvement, Iâ€™d genuinely love to hear from you. Letâ€™s share ideas, collaborate, or just connect as part of the n8n builder community.\n\nğŸ“§ [ascuncia.es@gmail.com](mailto:ascuncia.es+n8ncreator@gmail.com)\n\nğŸ”— [Linkedin](https://www.linkedin.com/in/alejandro-scuncia-60a62348/)",
  "featuredImage": "/data/workflows/8008/8008.webp",
  "author": {
    "id": 101,
    "slug": "ascuncia",
    "name": "Alejandro Scuncia",
    "avatar": ""
  },
  "categories": [
    "Internal Wiki",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 2006,
  "downloads": 200,
  "createdAt": "2025-08-28T22:18:25.522Z",
  "updatedAt": "2026-01-16T08:53:32.431Z",
  "publishedAt": "2025-08-28T22:18:25.522Z",
  "nodes": 32,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/8008"
}