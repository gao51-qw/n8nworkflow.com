{
  "id": 10566,
  "slug": "10566",
  "title": "Process large documents with OCR using SubworkflowAI and Gemini",
  "description": "## Working with Large Documents In Your VLM OCR Workflow\n\nDocument workflows are popular ways to use AI but what happens when your document is too large for your app or your AI to handle? Whether its context window or application memory that's grinding to a halt, [Subworkflow.ai](https://subworkflow.ai?utm=n8n) is one approach to keep you going.\n\n&gt; Subworkflow.ai is a third party API service to help AI developers work with documents too large for context windows and runtime memory.\n\n### Prequisites\n1. You'll need a Subworkflow.ai API key to use the Subworkflow.ai service.\n2. Add the API key as a header auth credential. More details in the official docs [https://docs.subworkflow.ai/category/api-reference](https://docs.subworkflow.ai/category/api-reference)\n\n### How it Works\n1. Import your document into your n8n workflow\n2. Upload it to the Subworkflow.ai service via the **Extract API** using the HTTP node. This endpoint takes files up to 100mb.\n3. Once uploaded, this will trigger an `Extract` job on the service's side and the response is a \"job\" record to track progress.\n4. Poll Subworkflow.ai's `Jobs` endpoint and keep polling until the job is finished. You can use the \"IF\" node looping back unto itself to achieve this in n8n.\n5. Once the job is done, the `Dataset` of the uploaded document is ready for retrieval. Use the `Datasets` and `DatasetItems` API to retrieve whatever you need to complete your AI task.\n6. In this example, all pages are retrieved and run through a multimodal LLM to parse into markdown. A well-known process when parsing data tables or graphics are required.\n\n### How to use\n* Integrate Subworkflow's Extract API seemlessly into your existing document workflows to support larger documents from 100mb+ to up to 5000 pages.\n\n### Customising the workflow\n* Sometimes you don't want the entire document back especially if the document is quite large (think 500+ pages!), instead, use query parameters on the `DatasetItems` API to pick individual pages or a range of pages to reduce the load.\n\n### Need Help?\n* **Official API documentation**: [https://docs.subworkflow.ai/category/api-reference](https://docs.subworkflow.ai/category/api-reference)\n* **Join the discord**: [https://discord.gg/RCHeCPJnYw](RCHeCPJnYw)",
  "featuredImage": "/data/workflows/10566/10566.webp",
  "author": {
    "id": 101,
    "slug": "jimleuk",
    "name": "Jimleuk",
    "avatar": ""
  },
  "categories": [
    "Document Extraction",
    "Multimodal AI"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 7793,
  "downloads": 779,
  "createdAt": "2025-11-06T11:49:13.463Z",
  "updatedAt": "2026-01-16T09:05:32.419Z",
  "publishedAt": "2025-11-06T11:49:13.463Z",
  "nodes": 16,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/10566"
}