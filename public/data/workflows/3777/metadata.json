{
  "id": 3777,
  "slug": "3777",
  "title": "Extract, Transform LinkedIn Data with Bright Data MCP Server & Google Gemini",
  "description": "### Disclaimer\nThis template is only available on n8n self-hosted as it's making use of the community node for MCP Client.\n\n![Extract, Transform LinkedIn Data with Bright Data.png](fileId:1221)\n\n### Who this is for?\n\nThe Extract, Transform LinkedIn Data with Bright Data MCP Server & Google Gemini workflow is an automated solution that scrapes LinkedIn content via Bright Data MCP Server then transforms the response using a Gemini LLM. The final output is sent via webhook notification and also persisted on disk.\n\nThis workflow is tailored for:â€‹\n1. **Data Analysts** : Who require structured LinkedIn datasets for analytics and reporting.\n\n2. **Marketing and Sales Teams** : Looking to enrich lead databases, track company updates, and identify market trends.\n\n3. **Recruiters and Talent Acquisition Specialists** : Who want to automate candidate sourcing and company research.\n\n4. **AI Developers** : Integrating real-time professional data into intelligent applications.\n\n5. **Business Intelligence Teams** : Needing current and comprehensive LinkedIn data to drive strategic decisions.\n\n### What problem is this workflow solving?\n\nGathering structured and meaningful information from the web is traditionally slow, manual, and error-prone.\n\nThis workflow solves:\n\n1. Reliable web scraping using Bright Data MCP Server LinkedIn tools.\n\n2. LinkedIn person and company web scrapping with AI Agents setup with the Bright Data MCP Server tools.\n\n3. Data extraction and transformation with Google Gemini LLM.\n\n4. Persists the LinkedIn person and company info to disk.\n\n5. Performs a Webhook notification with the LinkedIn person and company info.\n\n### What this workflow does?\n\nThis n8n workflow performs the following steps:\n\n1. **Trigger**: Start manually.\n\n2. **Input URL(s)**: Specify the LinkedIn person and company URL.\n\n3. **Web Scraping (Bright Data)**: Use Bright Data's MCP Server, LinkedIn tools for the person and company data extract.\n\n4. **Data Transformation & Aggregation**: Uses the Google LLM for handling the data transformation.\n\n5. **Store / Output**: Save results into disk and also performs a Webhook notification.\n\n### Pre-conditions\n\n1. Knowledge of Model Context Protocol (MCP) is highly essential. Please read this blog post - [model-context-protocol](https://www.anthropic.com/news/model-context-protocol)\n2. You need to have the [Bright Data](https://brightdata.com/) account and do the necessary setup as mentioned in the **Setup** section below.\n3. You need to have the Google Gemini API Key. Visit [Google AI Studio](https://aistudio.google.com/)\n3. You need to install the Bright Data MCP Server [@brightdata/mcp](https://www.npmjs.com/package/@brightdata/mcp)\n4. You need to install the [n8n-nodes-mcp](https://github.com/nerding-io/n8n-nodes-mcp)\n\n### Setup\n\n1. Please make sure to setup n8n locally with MCP Servers by navigating to [n8n-nodes-mcp](https://github.com/nerding-io/n8n-nodes-mcp)\n2. Please make sure to install the Bright Data MCP Server [@brightdata/mcp](https://www.npmjs.com/package/@brightdata/mcp)  on your local machine.\n2. Sign up at [Bright Data](https://brightdata.com/).\n3. Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.\n4. Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.\n5. In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).\n6. In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.\n![MCPClientAccount.png](fileId:1220)\nMake sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=&lt;your-token&gt;.\n7. Update the LinkedIn URL person and company workflow.\n8. Update the Webhook HTTP Request node with the Webhook endpoint of your choice.\n9. Update the file name and path to persist on disk.\n\n### How to customize this workflow to your needs\n\n1. **Different Inputs**: Instead of static URLs, accept URLs dynamically via webhook or form submissions.\n\n2. **Data Extraction**: Modify the **LinkedIn Data Extractor** node with the suitable prompt to format the data as you wish.\n\n3. **Outputs**: Update the Webhook endpoints to send the response to Slack channels, Airtable, Notion, CRM systems, etc.\n",
  "featuredImage": "/data/workflows/3777/3777.webp",
  "author": {
    "id": 101,
    "slug": "ranjancse",
    "name": "Ranjan Dailata",
    "avatar": ""
  },
  "categories": [
    "Lead Generation",
    "AI Summarization"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 818,
  "downloads": 81,
  "createdAt": "2025-04-28T22:28:54.106Z",
  "updatedAt": "2026-01-16T08:31:25.022Z",
  "publishedAt": "2025-04-28T22:28:54.106Z",
  "nodes": 20,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/3777"
}