{
  "id": 7563,
  "slug": "7563",
  "title": "Create a company policy chatbot with RAG, Pinecone vector database, and OpenAI",
  "description": "# A RAG Chatbot with n8n and Pinecone Vector Database  \n\nRetrieval-Augmented Generation (RAG) allows Large Language Models (LLMs) to provide **context-aware answers** by retrieving information from an external vector database. In this post, we‚Äôll walk through a complete **n8n workflow** that builds a chatbot capable of answering company policy questions using **Pinecone Vector Database** and **OpenAI models**.  \n\nOur setup has two main parts:  \n\n1. **Data Loading to RAG** ‚Äì documents (company policies) are ingested from Google Drive, processed, embedded, and stored in Pinecone.  \n2. **Data Retrieval using RAG** ‚Äì user queries are routed through an AI Agent that uses Pinecone to retrieve relevant information and generate precise answers.  \n\n---\n\n## 1. Data Loading to RAG  \n\nThis workflow section handles **document ingestion**. Whenever a new policy file is uploaded to Google Drive, it is automatically processed and indexed in Pinecone.  \n\n**Nodes involved:**  \n\n- **Google Drive Trigger**  \n  Watches a specific folder in Google Drive. Any new or updated file triggers the workflow.  \n\n- **Google Drive (Download)**  \n  Fetches the file (e.g., a PDF policy document) from Google Drive for processing.  \n\n- **Recursive Character Text Splitter**  \n  Splits long documents into smaller chunks (with a defined overlap). This ensures embeddings remain context-rich and retrieval works effectively.  \n\n- **Default Data Loader**  \n  Reads the binary document (PDF in this setup) and extracts the text.  \n\n- **OpenAI Embeddings**  \n  Generates high-dimensional vector representations of each text chunk using OpenAI‚Äôs embedding models.  \n\n- **Pinecone Vector Store (Insert Mode)**  \n  Stores the embeddings into a Pinecone index (`n8ntest`), under a chosen namespace. This step makes the policy data searchable by semantic similarity.  \n\nüëâ **Example flow**: When HR uploads a new *Work From Home Policy* PDF to Google Drive, it is automatically split, embedded, and indexed in Pinecone.  \n\n---\n\n## 2. Data Retrieval using RAG  \n\nOnce documents are loaded into Pinecone, the chatbot is ready to handle user queries. This section of the workflow connects the **chat interface**, **AI Agent**, and **retrieval pipeline**.  \n\n**Nodes involved:**  \n\n- **When Chat Message Received**  \n  Acts as the webhook entry point when a user sends a question to the chatbot.  \n\n- **AI Agent**  \n  The core reasoning engine. It is configured with a **system message** instructing it to only use Pinecone-backed knowledge when answering.  \n\n- **Simple Memory**  \n  Keeps track of the conversation context, so the bot can handle multi-turn queries.  \n\n- **Vector Store QnA Tool**  \n  Queries Pinecone for the most relevant chunks related to the user‚Äôs question. In this workflow, it is configured to fetch company policy documents.  \n\n- **Pinecone Vector Store (Query Mode)**  \n  Acts as the connection to Pinecone, fetching embeddings that best match the query.  \n\n- **OpenAI Chat Model**  \n  Refines the retrieved chunks into a natural and concise answer. The model ensures answers remain grounded in the source material.  \n\n- **Calculator Tool**  \n  Optional helper if the query involves numerical reasoning (e.g., leave calculations or benefit amounts).  \n\nüëâ **Example flow**: A user asks *‚ÄúHow many work-from-home days are allowed per month?‚Äù*. The AI Agent queries Pinecone through the Vector Store QnA tool, retrieves the relevant section of the HR policy, and returns a concise answer grounded in the actual document.  \n\n---\n\n## Wrapping Up  \n\nBy combining **n8n automation**, **Pinecone for vector storage**, and **OpenAI for embeddings + LLM reasoning**, we‚Äôve created a **self-updating RAG chatbot**.  \n\n- **Data Loading pipeline** ensures that every new company policy document uploaded to Google Drive is immediately available for semantic search.  \n- **Data Retrieval pipeline** allows employees to ask natural language questions and get document-backed answers.  \n\nThis setup can easily be adapted for other domains ‚Äî compliance manuals, tax regulations, legal contracts, or even product documentation.  \n\n\n![Screenshot 20250819 083747.png](fileId:2140)",
  "featuredImage": "/data/workflows/7563/7563.webp",
  "author": {
    "id": 101,
    "slug": "prathoure",
    "name": "Pramod Rathoure",
    "avatar": ""
  },
  "categories": [
    "AI RAG",
    "Multimodal AI"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 1509,
  "downloads": 150,
  "createdAt": "2025-08-19T03:10:36.422Z",
  "updatedAt": "2026-01-16T08:51:07.556Z",
  "publishedAt": "2025-08-19T03:10:36.422Z",
  "nodes": 17,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/7563"
}