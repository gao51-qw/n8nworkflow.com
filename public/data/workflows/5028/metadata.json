{
  "id": 5028,
  "slug": "5028",
  "title": "Basic RAG chat",
  "description": "This workflow demonstrates a simple Retrieval-Augmented Generation (RAG) pipeline in n8n, split into two main sections:\n\nğŸ”¹ Part 1: Load Data into Vector Store\nReads files from disk (or Google Drive).\n\nSplits content into manageable chunks using a recursive text splitter.\n\nGenerates embeddings using the Cohere Embedding API.\n\nStores the vectors into an In-Memory Vector Store (for simplicity; can be replaced with Pinecone, Qdrant, etc.).\n\nğŸ”¹ Part 2: Chat with the Vector Store\nTakes user input from a chat UI or trigger node.\n\nEmbeds the query using the same Cohere embedding model.\n\nRetrieves similar chunks from the vector store via similarity search.\n\nUses Groq-hosted LLM to generate a final answer based on the context.\n\nğŸ› ï¸ Technologies Used:\nğŸ“¦ Cohere Embedding API\n\nâš¡ Groq LLM for fast inference\n\nğŸ§  n8n for orchestrating and visualizing the flow\n\nğŸ§² In-Memory Vector Store (for prototyping)\n\nğŸ§ª Usage:\nUpload or point to your source documents.\n\nEmbed them and populate the vector store.\n\nAsk questions through the chat trigger node.\n\nReceive context-aware responses based on retrieved content.\n\n",
  "featuredImage": "/data/workflows/5028/5028.webp",
  "author": {
    "id": 101,
    "slug": "justin",
    "name": "JustinLee",
    "avatar": ""
  },
  "categories": [
    "Internal Wiki",
    "AI RAG"
  ],
  "complexityLevel": "intermediate",
  "price": 0,
  "visitors": 4861,
  "downloads": 486,
  "createdAt": "2025-06-18T16:55:03.909Z",
  "updatedAt": "2026-01-16T08:37:16.425Z",
  "publishedAt": "2025-06-18T16:55:03.909Z",
  "nodes": 14,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/5028"
}