{
  "id": 8195,
  "slug": "8195",
  "title": "Crawl website blog content and save to Google Sheets with Dumpling AI",
  "description": "### Who is this for?  \nThis workflow is perfect for content strategists, SEO specialists, marketing agencies, and virtual assistants who need to quickly audit and collect blog content from client websites into a structured Google Sheet without doing manual crawling and copy-pasting.  \n\n---  \n\n### What problem is this workflow solving?  \nManually visiting a website, finding blog posts, and copying content into a spreadsheet is time-consuming and prone to errors. This workflow automates the process: it crawls a website, filters only blog-related pages, scrapes the article content, and stores everything neatly in Google Sheets for easy analysis and content strategy planning.  \n\n---  \n\n### What this workflow does  \nThe workflow starts when a client submits their website URL through a form. A Google Sheet is automatically created and headers are added for organizing the audit. Dumpling AI then crawls the website to discover all available pages, while the automation filters out only blog-related URLs. Each blog page is scraped for content, and the structured results (URL, crawled page, and website content) are appended row by row into the Google Sheet.  \n\n---  \n\n### Nodes Overview  \n\n1. Form Trigger ‚Äì `Form Submission (Client URL)`  \n   Captures the client‚Äôs website URL to start the workflow.  \n\n2. Google Sheets ‚Äì `Create Blog Audit Sheet`  \n   Creates a new Google Sheet with a title based on the submitted URL.  \n\n3. Set ‚Äì `Set Sheet Headers`  \n   Defines the headers: `Url`, `Crawled_pages`, `website_content`.  \n\n4. Code ‚Äì `Format Header Row`  \n   Formats the headers properly before sending them to the sheet.  \n\n5. HTTP Request ‚Äì `Insert Headers into Sheet`  \n   Updates the Google Sheet with the prepared header row.  \n\n6. HTTP Request ‚Äì `Dumpling AI: Crawl Website`  \n   Crawls the submitted URL to discover internal pages.  \n\n7. Code ‚Äì `Extract Blog URLs`  \n   Filters the crawl results and keeps only URLs that match common blog patterns (e.g., `/blog/`, `/articles/`, `/posts/`).  \n\n8. HTTP Request ‚Äì `Dumpling AI: Scrape Blog Pages`  \n   Scrapes the text content from each filtered blog page.  \n\n9. Set ‚Äì `Prepare Row Data`  \n   Maps the URL, blog page link, and scraped content into structured fields.  \n\n10. Google Sheets ‚Äì `Save Blog Data to Google Sheets`  \n    Appends the structured data into the audit sheet row by row.  \n\n---  \n\n### üìù Notes  \n\n- Set up Dumpling AI and generate your API key from: [Dumpling AI](https://www.dumplingai.com/)  \n- Google Sheets must be connected with write permissions enabled.  \n- You can change the crawl depth or limit (currently set to 10 pages) in the `Dumpling AI: Crawl Website` node.  \n- The `Extract Blog URLs` node uses regex patterns to detect blog content. You can customize these patterns to match your website‚Äôs URL structure.  \n",
  "featuredImage": "/data/workflows/8195/8195.webp",
  "author": {
    "id": 101,
    "slug": "yang",
    "name": "Yang",
    "avatar": ""
  },
  "categories": [
    "Market Research"
  ],
  "complexityLevel": "intermediate",
  "price": 0,
  "visitors": 664,
  "downloads": 66,
  "createdAt": "2025-09-02T21:11:07.174Z",
  "updatedAt": "2026-01-16T08:54:35.795Z",
  "publishedAt": "2025-09-02T21:11:07.174Z",
  "nodes": 11,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/8195"
}