{
  "id": 11618,
  "slug": "11618",
  "title": "Compare GPT-4, Claude & Gemini Responses with Contextual AI's LMUnit Evaluation",
  "description": "## PROBLEM  \nEvaluating and comparing responses from multiple LLMs (OpenAI, Claude, Gemini) can be challenging when done manually.  \n- Each model produces outputs that differ in clarity, tone, and reasoning structure.  \n- Traditional evaluation metrics like ROUGE or BLEU fail to capture nuanced quality differences.  \n- Human evaluations are inconsistent, slow, and difficult to scale.  \n\n### This workflow automates **LLM response quality evaluation** using **Contextual AI’s LMUnit**, a natural language unit testing framework that provides systematic, fine-grained feedback on response clarity and conciseness.  \n&gt; **Note:** LMUnit offers natural language-based evaluation with a 1–5 scoring scale, enabling consistent and interpretable results across different model outputs.\n\n## How it works  \n- A **chat trigger node** collects responses from multiple LLMs such as **OpenAI GPT-4.1, **Claude 4.5 Sonnet**, and **Gemini 2.5 Flash**.  \n- Each model receives the same input prompt to ensure fair comparison, which is then aggregated and associated with each test cases\n- We use Contextual AI's LMUnit node to evaluate each response using predefined quality criteria:  \n  - “Is the response clear and easy to understand?” - Clarity \n  - “Is the response concise and free from redundancy?”  - Conciseness \n- **LMUnit** then produces evaluation scores (1–5) for each test \n- Results are aggregated and formatted into a structured summary showing model-wise performance and overall averages.\n\n## How to set up  \n- Create a free [Contextual AI account](https://app.contextual.ai/) and obtain your `CONTEXTUALAI_API_KEY`.  \n- In your **n8n** instance, add this key as a credential under “Contextual AI.”  \n- Obtain and add credentials for each model provider you wish to test:  \n  - **OpenAI API Key:** [platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)  \n  - **Anthropic API Key:** [console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys)  \n  - **Gemini API Key:** [ai.google.dev/gemini-api/docs/api-key](https://ai.google.dev/gemini-api/docs/api-key)  \n- Start sending prompts using chat interface to automatically generate model outputs and evaluations.\n\n## How to customize the workflow  \n- Add more **evaluation criteria** (e.g., factual accuracy, tone, completeness) in the LMUnit test configuration.  \n- Include additional **LLM providers** by duplicating the response generation nodes.  \n- Adjust **thresholds and aggregation logic** to suit your evaluation goals.  \n- Enhance the final summary formatting for dashboards, tables, or JSON exports.  \n- For detailed API parameters, refer to the [LMUnit API reference](https://docs.contextual.ai/api-reference/lmunit/lmunit).  \n- If you have feedback or need support, please email **feedback@contextual.ai**.",
  "featuredImage": "/data/workflows/11618/11618.webp",
  "author": {
    "id": 101,
    "slug": "jinash",
    "name": "Jinash Rouniyar",
    "avatar": ""
  },
  "categories": [
    "Engineering",
    "AI Summarization"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 868,
  "downloads": 86,
  "createdAt": "2025-12-09T02:58:37.978Z",
  "updatedAt": "2026-01-16T09:09:22.017Z",
  "publishedAt": "2025-12-09T02:58:37.978Z",
  "nodes": 20,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/11618"
}