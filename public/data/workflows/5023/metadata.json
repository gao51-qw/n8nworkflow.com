{
  "id": 5023,
  "slug": "5023",
  "title": "Build a RAG system with automatic citations using Qdrant, Gemini & OpenAI",
  "description": "\nThis workflow implements a **Retrieval-Augmented Generation (RAG)** system that:\n\n* Stores vectorized documents in **Qdrant**,\n* Retrieves relevant content based on user input,\n* Generates AI answers using **Google Gemini**,\n* Automatically **cites the document sources** (from Google Drive).\n\n---\n\n### **Workflow Steps**\n\n1. **Create Qdrant Collection**\n   A REST API node creates a new collection in Qdrant with specified vector size (1536) and cosine similarity.\n\n2. **Load Files from Google Drive**\n   The workflow lists all files in a Google Drive folder, downloads them as plain text, and loops through each.\n\n3. **Text Preprocessing & Embedding**\n\n   * Documents are split into chunks (500 characters, with 50-character overlap).\n   * Embeddings are created using **OpenAI embeddings** (`text-embedding-3-small` assumed).\n   * Metadata (file name and ID) is attached to each chunk.\n\n4. **Store in Qdrant**\n   All vectors, along with metadata, are inserted into the Qdrant collection.\n\n5. **Chat Input & Retrieval**\n\n   * When a chat message is received, the question is embedded and matched against Qdrant.\n   * Top 5 relevant document chunks are retrieved.\n   * A Gemini model is used to generate the answer based on those sources.\n\n6. **Source Aggregation & Response**\n\n   * File IDs and names are deduplicated.\n   * The AI response is combined with a list of cited documents (filenames).\n   * Final output:\n\n     ```\n     AI Response\n\n     Sources: [\"Document1\", \"Document2\"]\n     ```\n\n---\n\n### **Main Advantages**\n\n* **End-to-end Automation**: From document ingestion to chat response generation, fully automated with no manual steps.\n* **Scalable Knowledge Base**: Easy to expand by simply adding files to the Google Drive folder.\n* **Traceable Responses**: Each answer includes its source files, increasing transparency and trustworthiness.\n* **Modular Design**: Each step (embedding, storage, retrieval, response) is isolated and reusable.\n* **Multi-provider AI**: Combines OpenAI (for embeddings) and Google Gemini (for chat), optimizing performance and flexibility.\n* **Secure & Customizable**: Uses API credentials and configurable chunk size, collection name, etc.\n\n---\n\n\n### **How It Works**  \n1. **Document Processing & Vectorization**  \n   - The workflow retrieves documents from a specified Google Drive folder.  \n   - Each file is downloaded, split into chunks (using a recursive text splitter), and converted into embeddings via OpenAI.  \n   - The embeddings, along with metadata (file ID and name), are stored in a Qdrant vector database under the collection `negozio-emporio-verde`.  \n\n2. **Query Handling & Response Generation**  \n   - When a user submits a chat message, the workflow:  \n     - Embeds the query using OpenAI.  \n     - Retrieves the top 5 relevant document chunks from Qdrant.  \n     - Uses Google Gemini to generate a response based on the retrieved context.  \n     - Aggregates and deduplicates the source file names from the retrieved chunks.  \n   - The final output includes both the AI-generated response and a list of source documents (e.g., `Sources: [\"FAQ.pdf\", \"Policy.txt\"]`).  \n\n---  \n\n### **Set Up Steps**  \n1. **Configure Qdrant Collection**  \n   - Replace `QDRANTURL` and `COLLECTION` in the **\"Create collection\"** HTTP node to initialize the Qdrant collection with:  \n     - Vector size: `1536` (OpenAI embedding dimension).  \n     - Distance metric: `Cosine`.  \n   - Ensure the **\"Clear collection\"** node is configured to reset the collection if needed.  \n\n2. **Google Drive & OpenAI Integration**  \n   - Link the Google Drive node to the target folder (`Test Negozio` in this example).  \n   - Verify OpenAI and Google Gemini API credentials are correctly set in their respective nodes.  \n\n3. **Metadata & Output Customization**  \n   - Adjust the **\"Aggregate\"** and **\"Response\"** nodes if additional metadata fields are needed.  \n   - Modify the **\"Output\"** node to format the response (e.g., changing `Sources: {{...}}` to match your preferred style).  \n\n4. **Testing**  \n   - Trigger the workflow manually to test document ingestion.  \n   - Use the chat interface to verify responses include accurate source attribution.  \n\n**Note**: Replace placeholder values (e.g., `QDRANTURL`) with actual endpoints before deployment.\n\n---\n### **Need help customizing?**  \n[Contact me](mailto:info@n3w.it) for consulting and support or add me on [Linkedin](https://www.linkedin.com/in/davideboizza/). ",
  "featuredImage": "/data/workflows/5023/5023.webp",
  "author": {
    "id": 101,
    "slug": "n3witalia",
    "name": "Davide",
    "avatar": ""
  },
  "categories": [
    "Internal Wiki",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 2588,
  "downloads": 258,
  "createdAt": "2025-06-18T14:37:45.594Z",
  "updatedAt": "2026-01-16T08:37:15.114Z",
  "publishedAt": "2025-06-18T14:37:45.594Z",
  "nodes": 29,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/5023"
}