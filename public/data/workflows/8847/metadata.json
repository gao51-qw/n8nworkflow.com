{
  "id": 8847,
  "slug": "8847",
  "title": "Daily RAG research paper hub with arXiv, Gemini AI, and Notion",
  "description": "Fetch user-specific research papers from **arXiv** on a daily schedule, process and structure the data, and create or update entries in a Notion database, with support for data delivery\n\n- **Paper Topic**: single query keyword\n- **Update Frequency**: Daily updates, with fewer than 20 entries expected per day\n- **Tools**:\n    - **Platform**: n8n, for end-to-end workflow configuration\n    - **AI Model**: Gemini-2.5-Flash, for daily paper summarization and data processing\n    - **Database**: Notion, with two tables — Daily Paper Summary and Paper Details\n    - **Message**: Feishu (IM bot notifications), Gmail (email notifications)\n\n \n## 1. Data Retrieval\n### arXiv API\n\nThe arXiv provides a public API that allows users to query research papers by topic or by predefined categories.\n\n[arXiv API User Manual](https://info.arxiv.org/help/api/user-manual.html#arxiv-api-users-manual)\n\n**Key Notes:**\n\n1. **Response Format**: The API returns data as a typical *Atom Response*.\n2. **Timezone & Update Frequency**:  \n   - The arXiv submission process operates on a 24-hour cycle.  \n   - Newly submitted articles become available in the API only at midnight *after* they have been processed.  \n   - Feeds are updated daily at midnight Eastern Standard Time (EST).  \n   - Therefore, a single request per day is sufficient.  \n3. **Request Limits**:  \n   - The maximum number of results per call (`max_results`) is **30,000**,  \n   - Results must be retrieved in slices of at most **2,000** at a time, using the `max_results` and `start` query parameters.  \n4. **Time Format**:  \n   - The expected format is `[YYYYMMDDTTTT+TO+YYYYMMDDTTTT]`,  \n   - `TTTT` is provided in 24-hour time to the minute, in GMT.\n\n### Scheduled Task\n\n- **Execution Frequency**: Daily  \n- **Execution Time**: 6:00 AM  \n- **Time Parameter Handling (JS)**:  \n  According to arXiv’s update rules, the scheduled task should query the **previous day’s (T-1)** `submittedDate` data.\n\n\n \n## 2. **Data Extraction**\n\n### Data Cleaning Rules (Convert to Standard JSON)\n\n1. **Remove Header**  \n   - Keep only the 【entry】【/entry】 blocks representing paper items.\n\n2. **Single Item**  \n   - Each 【entry】【/entry】 represents a single item.\n\n3. **Field Processing Rules**  \n   - 【id】【/id】 ➡️ id  \n     Extract content.  \n     Example:  \n     【id】http://arxiv.org/abs/2409.06062v1【/id】 → http://arxiv.org/abs/2409.06062v1  \n\n   - 【updated】【/updated】 ➡️ updated  \n     Convert timestamp to `yyyy-mm-dd hh:mm:ss`  \n\n   - 【published】【/published】 ➡️ published  \n     Convert timestamp to `yyyy-mm-dd hh:mm:ss`  \n\n   - 【title】【/title】 ➡️ title  \n     Extract text content  \n\n   - 【summary】【/summary】 ➡️ summary  \n     Keep text, remove line breaks  \n\n   - 【author】【/author】 ➡️ author  \n     Combine all authors into an array  \n     Example: `[ \"Ernest Pusateri\", \"Anmol Walia\" ]` (for Notion multi-select field)  \n\n   - 【arxiv:comment】【/arxiv:comment】 ➡️ Ignore / discard  \n\n   - 【link type=\"text/html\"】 ➡️ html_url  \n     Extract URL  \n\n   - 【link type=\"application/pdf\"】 ➡️ pdf_url  \n     Extract URL  \n\n   - 【arxiv:primary_category term=\"cs.CL\"】 ➡️ primary_category  \n     Extract `term` value  \n\n   - 【category】 ➡️ category  \n     Merge all 【category】 values into an array  \n     Example: `[ \"eess.AS\", \"cs.SD\" ]` (for Notion multi-select field)  \n\n4. **Add Empty Fields**  \n   - `github`  \n   - `huggingface`\n\n\n\n\n\n\n\n## 3. Data Processing\n\nAnalyze and summarize paper data using AI, then standardize output as JSON.\n\n- Single Paper Basic Information Analysis and Enhancement\n- Daily Paper Summary and Multilingual Translation\n\n\n## 4. Data Storage: Notion Database\n\n- Create a corresponding database in Notion with the same predefined field names.  \n- In Notion, create an integration under **Integrations** and grant access to the database. Obtain the corresponding **Secret Key**.  \n- Use the Notion **\"Create a database page\"** node to configure the field mapping and store the data.  \n\n**Notes**  \n- **\"Create a database page\"** only adds new entries; data will not be updated.  \n- The `updated` and `published` timestamps of arXiv papers are in **UTC**.  \n- Notion **single-select** and **multi-select** fields only accept arrays. They do not automatically parse comma-separated strings. You need to format them as proper arrays.  \n- Notion does not accept `null` values, which causes a **400 error**.  \n\n\n## 5. Data Delivery\n\nSet up two channels for message delivery: **EMAIL** and **IM**, and define the message format and content.\n\n### Email: Gmail\n\n**GMAIL OAuth 2.0 – Official Documentation**  \n[Configure your OAuth consent screen](https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/?utm_source=n8n_app&utm_medium=credential_settings&utm_campaign=create_new_credentials_modal#configure-your-oauth-consent-screen)\n\n**Steps:**\n- Enable Gmail API  \n- Create OAuth consent screen  \n- Create OAuth client credentials  \n- Audience: Add **Test users** under Testing status  \n\n**Message format**: HTML  \n(Model: OpenAI GPT — used to design an HTML email template)\n\n### IM: Feishu (LARK)\n\n**Bots in groups**  \n[Use bots in groups](https://www.larksuite.com/hc/en-US/articles/360048487736-use-bots-in-groups)\n",
  "featuredImage": "/data/workflows/8847/8847.webp",
  "author": {
    "id": 101,
    "slug": "dongou",
    "name": "dongou",
    "avatar": ""
  },
  "categories": [
    "Market Research",
    "AI RAG"
  ],
  "complexityLevel": "advanced",
  "price": 0,
  "visitors": 644,
  "downloads": 64,
  "createdAt": "2025-09-23T05:24:27.840Z",
  "updatedAt": "2026-01-16T08:57:55.636Z",
  "publishedAt": "2025-09-23T05:24:27.840Z",
  "nodes": 22,
  "version": "1.0.0",
  "sourceUrl": "https://n8n.io/workflows/8847"
}