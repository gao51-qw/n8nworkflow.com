# Monitor regulatory updates with ScrapeGraphAI and send alerts via Telegram

> # Breaking News Aggregator with Telegram and Redis

**‚ö†Ô∏è COMMUNITY TEMPLATE DISCLAIMER: This is a community-contributed template that uses ScrapeGraphAI (a community node). Please ensure you have the ScrapeGraphAI community node installed in your n8n instance before using this template.**

This workflow monitors selected government websites, regulatory bodies, and legal-news portals for new or amended regulations relevant to specific industries. It scrapes the latest headlines, compares them against previously recorded items in Redis, and pushes real-time compliance alerts to a Telegram channel or chat.

## Pre-conditions/Requirements

### Prerequisites
- n8n instance (self-hosted or cloud)
- ScrapeGraphAI community node installed
- Redis server accessible from n8n
- Telegram Bot created via BotFather
- (Optional) Cron node if you want fully automated scheduling instead of manual trigger

### Required Credentials
- **ScrapeGraphAI API Key** ‚Äì Enables ScrapeGraphAI scraping functionality  
- **Telegram Bot Token** ‚Äì Allows n8n to send messages via your bot  
- **Redis Credentials** ‚Äì Host, port, and (if set) password for your Redis instance  

### Redis Setup Requirements
| Key Name | Description | Example |
|----------|-------------|---------|
| `latestRegIds` | Redis Set used to store hashes/IDs of the most recent regulatory articles processed | `latestRegIds` |

&gt; Hint: Use a dedicated Redis DB (e.g., DB 1) to keep workflow data isolated from other applications.

## How it works

This workflow monitors selected government websites, regulatory bodies, and legal-news portals for new or amended regulations relevant to specific industries. It scrapes the latest headlines, compares them against previously recorded items in Redis, and pushes real-time compliance alerts to a Telegram channel or chat.

## Key Steps:
- **Manual Trigger / Cron**: Starts the workflow manually or on a set schedule (e.g., daily at 06:00 UTC).  
- **Code (Define Sources)**: Returns an array of URL objects pointing to regulatory pages to monitor.  
- **SplitInBatches**: Iterates through each source URL in manageable chunks.  
- **ScrapeGraphAI**: Extracts article titles, publication dates, and article URLs from each page.  
- **Merge (Combine Results)**: Consolidates scraped items into a single stream.  
- **If (Deduplication Check)**: Verifies whether each article ID already exists in Redis.  
- **Set (Format Message)**: Creates a human-readable Telegram message string.  
- **Telegram**: Sends the formatted compliance alert to your chosen chat/channel.  
- **Redis (Add New IDs)**: Stores the article ID so it is not sent again in the future.  
- **Sticky Note**: Provides inline documentation inside the workflow canvas.

## Set up steps

**Setup Time: 10-15 minutes**

1. **Install community nodes**: In n8n, go to *Settings ‚Üí Community Nodes* and install `n8n-nodes-scrapegraphai`.  
2. **Create credentials**:  
   a. Telegram ‚Üí *Credentials ‚Üí Telegram API* ‚Üí paste your bot token.  
   b. Redis ‚Üí *Credentials ‚Üí Redis* ‚Üí fill host, port, password, DB.  
   c. ScrapeGraphAI ‚Üí *Credentials ‚Üí ScrapeGraphAI API* ‚Üí enter your key.  
3. **Configure the ‚ÄúDefine Sources‚Äù Code node**: Replace the placeholder URLs with the regulatory pages you need to monitor.  
4. **Update Telegram chat ID**: Open any chat with your bot and use `https://api.telegram.org/bot&lt;token&gt;/getUpdates` to find the `chat.id`. Insert this value in the Telegram node.  
5. **Adjust frequency**: Replace the Manual Trigger with a Cron node (e.g., daily 06:00 UTC).  
6. **Test the workflow**: Execute once manually; confirm messages appear in Telegram and that Redis keys are created.  
7. **Activate**: Enable the workflow so it runs automatically according to your schedule.

## Node Descriptions

### Core Workflow Nodes:
- **Manual Trigger** ‚Äì Allows on-demand execution during development/testing.  
- **Code (Define Sources)** ‚Äì Returns an array of page URLs and meta info to the workflow.  
- **SplitInBatches** ‚Äì Prevents overloading websites by scraping in controlled groups.  
- **ScrapeGraphAI** ‚Äì Performs the actual web scraping using an AI-assisted parser.  
- **Merge** ‚Äì Merges data streams from multiple batches into one.  
- **If (Check Redis)** ‚Äì Filters out already-processed articles using Redis SET membership.  
- **Set** ‚Äì Shapes output into a user-friendly Telegram message.  
- **Telegram** ‚Äì Delivers compliance alerts to stakeholders in real time.  
- **Redis** ‚Äì Persists article IDs to avoid duplicate notifications.  
- **Sticky Note** ‚Äì Contains usage tips directly on the canvas.

### Data Flow:
1. **Manual Trigger** ‚Üí **Code (Define Sources)** ‚Üí **SplitInBatches** ‚Üí **ScrapeGraphAI**  
2. **ScrapeGraphAI** ‚Üí **Merge** ‚Üí **If (Check Redis)**  
3. **If (true)** ‚Üí **Set** ‚Üí **Telegram** ‚Üí **Redis**

## Customization Examples

### Change industries or keywords
```javascript
// Code node snippet
return [
  {
    url: "https://regulator.gov/energy-updates",
    industry: "Energy",
    keywords: ["renewable", "grid", "tariff"]
  },
  {
    url: "https://financewatch.gov/financial-rules",
    industry: "Finance",
    keywords: ["AML", "KYC", "cryptocurrency"]
  }
];
```

### Modify Telegram message formatting
```javascript
// Set node ‚ÄúParameters ‚Üí Value‚Äù
items[0].json.message = `üõ°Ô∏è *${$json.industry} Regulation Update*\n\n*${$json.title}*\n${$json.date}\n${$json.url}`;
return items;
```

## Data Output Format

The workflow outputs structured JSON data:

```json
{
  "title": "EU Proposes New ESG Disclosure Rules",
  "date": "2024-04-18",
  "url": "https://europa.eu/legal/eu-proposes-esg-disclosure",
  "industry": "Finance"
}
```

## Troubleshooting

### Common Issues
1. **Empty scraped data** ‚Äì Verify CSS selectors/XPath in the ScrapeGraphAI node; website structure may have changed.  
2. **Duplicate alerts** ‚Äì Ensure Redis credentials point to the same DB across nodes; otherwise IDs are not shared.  

### Performance Tips
- Limit `SplitInBatches` to 2-3 URLs at a time if sites implement rate limiting.  
- Use environment variables for credentials to simplify migration between stages.

**Pro Tips:**
- Combine this workflow with n8n‚Äôs **Error Trigger** to log failures to Slack or email.  
- Maintain a CSV of source URLs in Google Sheets and fetch it dynamically via the Google Sheets node.  
- Pair with the **Webhook** node to let team members add new sources on the fly.

## üìä Basic Information

- **Workflow ID:** 12072
- **Complexity:** advanced
- **Node Count:** 16
- **Views:** 7
- **Downloads:** 0
- **Created:** 2025/12/23
- **Last Updated:** 2026/1/16
- **Source:** [View on n8n.io](https://n8n.io/workflows/12072)

## üë§ Author

- **Name:** vinci-king-01
- **Username:** @vinci-king-01

## üè∑Ô∏è Categories

- Market Research
- AI Summarization

## üîó Nodes Used

- **manualTrigger** 
- **code** (√ó3)
- **splitInBatches** 
- **n8n-nodes-scrapegraphai.scrapegraphAi** 
- **merge** 
- **if** 
- **set** 
- **telegram** 
- **redis** 
- **stickyNote** (√ó5)

## üöÄ How to Use

1. Download the workflow JSON file
2. Import it into your n8n instance
3. Configure the credentials for the nodes
4. Activate and test the workflow

## üîÄ Workflow Structure

This workflow contains 16 nodes with 8 node connections.

---

*This workflow was sourced from [n8n.io](https://n8n.io) community templates.*
